1
00:00:00,000 --> 00:00:10,480
Hello and welcome to In Control, the first podcast on control theory.

2
00:00:10,480 --> 00:00:21,920
Here we discuss the science of feedback, decision-making, artificial intelligence, and much more.

3
00:00:21,920 --> 00:00:27,280
I'm your host Alberto Padoan, live from our recording studio at ETH Zurich.

4
00:00:27,280 --> 00:00:31,840
Big thanks to our sponsor, the National Center of Competence in Research on Dependable Ubiquitous

5
00:00:31,840 --> 00:00:36,520
Automation, which you can check following the link in the description.

6
00:00:36,520 --> 00:00:38,360
Our guest today is Ben Recht.

7
00:00:38,360 --> 00:00:43,240
Ben is a full professor in the Department of Electrical Engineering and Computer Sciences

8
00:00:43,240 --> 00:00:46,160
at the University of California, Berkeley.

9
00:00:46,160 --> 00:00:48,200
Ben has received a number of awards.

10
00:00:48,200 --> 00:00:54,840
I'm just going to mention that Ben received the NeurIPS Test of Time Award twice.

11
00:00:54,840 --> 00:00:55,840
Welcome to the show, Ben.

12
00:00:56,200 --> 00:00:57,520
Thanks for having me.

13
00:00:57,520 --> 00:01:04,640
So perhaps one nice way to start this podcast is to say that you predicted the existence

14
00:01:04,640 --> 00:01:06,040
of this podcast.

15
00:01:06,040 --> 00:01:11,520
In an IFAC plenary about two years ago, if I'm not wrong, you said this.

16
00:01:11,520 --> 00:01:16,280
And I'm just kind of considering this like an experiment in podcasting because the world

17
00:01:16,280 --> 00:01:20,320
really needs a control theory podcast.

18
00:01:20,320 --> 00:01:21,320
Maybe there is one.

19
00:01:21,320 --> 00:01:23,880
If there is one, I'm happy to participate.

20
00:01:23,880 --> 00:01:24,880
Send me an email.

21
00:01:24,920 --> 00:01:28,440
I'd love to jump on your control theory podcast.

22
00:01:28,440 --> 00:01:31,200
So Ben, you did envision this happening.

23
00:01:31,200 --> 00:01:33,760
So where should I invest my money?

24
00:01:33,760 --> 00:01:34,760
What else can you predict?

25
00:01:34,760 --> 00:01:38,840
Did I predict or did I cause?

26
00:01:38,840 --> 00:01:42,600
I think that's always the tricky question with all of these things, right?

27
00:01:42,600 --> 00:01:44,240
Well, there is a funny anecdote here.

28
00:01:44,240 --> 00:01:47,880
I was absolutely not aware of you mentioning this thing.

29
00:01:47,880 --> 00:01:53,840
I did not participate to that conference, nor did I watch your lecture before somebody

30
00:01:53,840 --> 00:01:57,080
told me in our department.

31
00:01:57,080 --> 00:02:04,040
And then I went on the internet, I found your plenary on YouTube, and I was so pleased to

32
00:02:04,040 --> 00:02:08,040
see that somehow you managed to predict this thing.

33
00:02:08,040 --> 00:02:10,040
Yeah, man.

34
00:02:10,040 --> 00:02:14,520
So Alberto, why did you decide that you, I mean, independently decide that you wanted

35
00:02:14,520 --> 00:02:16,800
to do a control-centric podcast?

36
00:02:16,800 --> 00:02:22,360
Well, you know, there's so many other podcasts out there about artificial intelligence, machine

37
00:02:22,360 --> 00:02:24,880
learning, even soft robotics.

38
00:02:24,880 --> 00:02:31,920
So I guess it's time for us to step up or step out of our academic rooms and maybe talk

39
00:02:31,920 --> 00:02:34,560
about our cool stuff.

40
00:02:34,560 --> 00:02:35,560
Absolutely.

41
00:02:35,560 --> 00:02:44,140
Perhaps a good starting point for our chat today will be about your personal trajectory.

42
00:02:44,140 --> 00:02:46,520
How did it all start?

43
00:02:46,520 --> 00:02:47,520
How early should I go?

44
00:02:47,520 --> 00:02:48,520
As much as you want.

45
00:02:48,520 --> 00:02:49,520
It's a little bit hard, right?

46
00:02:49,520 --> 00:02:50,520
Six months in, if you want.

47
00:02:50,520 --> 00:03:00,080
I mean, I'm having a hard time remembering how I got to my office today.

48
00:03:00,080 --> 00:03:01,080
So it's a lot.

49
00:03:01,080 --> 00:03:04,000
It's trying to dig back into some ancient times.

50
00:03:04,000 --> 00:03:05,000
I don't know.

51
00:03:05,000 --> 00:03:06,000
It's interesting, right?

52
00:03:06,000 --> 00:03:13,960
I think we're in this funny period of recruiting graduate students, and I always like to ask

53
00:03:13,960 --> 00:03:17,920
graduate students why they want to go to graduate school.

54
00:03:18,920 --> 00:03:20,920
I actually stole this one from my wife.

55
00:03:20,920 --> 00:03:27,240
One of the questions I ask them or tell them is, you know, if you could envision yourself

56
00:03:27,240 --> 00:03:33,680
doing anything else other than going to graduate school, you should probably just do that.

57
00:03:33,680 --> 00:03:35,080
I firmly believe that.

58
00:03:35,080 --> 00:03:39,360
I really feel like if you can figure out some other thing that would make you happy and

59
00:03:39,360 --> 00:03:42,480
fulfilled and that, you know, maybe it's risky, maybe it's not.

60
00:03:42,480 --> 00:03:45,760
I still suggest that people do that.

61
00:03:46,600 --> 00:03:50,320
For whatever reason, I wasn't one of those people.

62
00:03:50,320 --> 00:03:53,280
I kind of fell into the going to graduate school thing after college.

63
00:03:53,280 --> 00:03:54,280
I wasn't sure.

64
00:03:54,280 --> 00:03:59,440
I mean, I was always, you know, good in school, but I always had this.

65
00:03:59,440 --> 00:04:00,440
I had a lot of other passions.

66
00:04:00,440 --> 00:04:06,560
I was really into music as a high school kid and college kid, and by the time I got to

67
00:04:06,560 --> 00:04:09,480
my senior year of college, I thought to myself, either I'm going to do one of two things.

68
00:04:09,480 --> 00:04:17,560
I'm either going to go and try to volunteer at all of these recording studios in Chicago

69
00:04:17,560 --> 00:04:21,080
where I was going to undergraduate or I'll go to graduate school.

70
00:04:21,080 --> 00:04:23,160
And then I looked at the graduate schools.

71
00:04:23,160 --> 00:04:27,360
I found one that looked kind of interesting at MIT where they were doing a bunch of weird

72
00:04:27,360 --> 00:04:28,360
stuff.

73
00:04:28,360 --> 00:04:29,560
And it was like, OK, this looks different.

74
00:04:29,560 --> 00:04:31,200
It would be a big change.

75
00:04:31,200 --> 00:04:32,800
They're kind of doing stuff related to music.

76
00:04:32,800 --> 00:04:33,800
I'll just apply there.

77
00:04:33,800 --> 00:04:35,320
And it was very dumb.

78
00:04:35,320 --> 00:04:37,880
Me as a 22 year old, just I had no, I mean, I don't know.

79
00:04:38,280 --> 00:04:42,520
It was not very well thought out process to how I ended up where I am.

80
00:04:42,520 --> 00:04:48,600
But I think it was, it's interesting to go from, you know, I did pure math as an undergraduate.

81
00:04:48,600 --> 00:04:54,800
I decided pretty early on, well, not pretty early on, actually, I remember that when I

82
00:04:54,800 --> 00:05:00,960
decided I couldn't be a mathematician, it was in a class on commutative algebra.

83
00:05:00,960 --> 00:05:06,280
We had done some kind of proof in this class that required six chalkboards and it was just

84
00:05:06,320 --> 00:05:08,480
a diagram drawing all the other chalkboards.

85
00:05:08,480 --> 00:05:11,440
And then I was, I just thought that was one of those wake up and be like, I don't know

86
00:05:11,440 --> 00:05:13,160
what I'm doing with my life days.

87
00:05:13,160 --> 00:05:16,000
And I was like, I can't do this for the rest of my life.

88
00:05:16,000 --> 00:05:17,680
So I kind of just wandered around a lot.

89
00:05:17,680 --> 00:05:20,840
But it was one of those things where if I had, if I had a better idea, I probably would

90
00:05:20,840 --> 00:05:22,840
not be where I am today.

91
00:05:22,840 --> 00:05:27,960
So I ended up at MIT and that kind of changed a lot of my perspectives, going from someplace

92
00:05:27,960 --> 00:05:30,480
very theoretical to someplace very applied.

93
00:05:30,840 --> 00:05:37,880
And so having those, having that big juxtaposition, that big pendulum shift has been something

94
00:05:37,880 --> 00:05:44,160
that has kind of gone on throughout my career, swinging back and forth between just doing

95
00:05:44,160 --> 00:05:51,520
very pure stuff and then trying to get myself into more applied areas, sometimes successful

96
00:05:51,520 --> 00:05:52,640
and sometimes not.

97
00:05:52,640 --> 00:05:59,960
And then bouncing back into theory and also just having a lot of, I don't know what the

98
00:05:59,960 --> 00:06:04,480
right way to put this is, but like the attention span of a gnat, just like not, not being able

99
00:06:04,480 --> 00:06:08,520
to really necessarily focus too long on one thing.

100
00:06:08,520 --> 00:06:11,560
I think this is one of the fun things about being a professor.

101
00:06:11,560 --> 00:06:13,360
Everybody could be a professor in a different way.

102
00:06:13,360 --> 00:06:18,200
There are some people who are going to spend their entire careers, like improving one inequality

103
00:06:18,200 --> 00:06:21,240
because that's the, that's what they care about.

104
00:06:21,240 --> 00:06:23,040
And then I don't know what I'm doing.

105
00:06:23,040 --> 00:06:24,800
I'm just doing the opposite.

106
00:06:24,800 --> 00:06:28,960
I change what I want to work on every, every couple of weeks, maybe.

107
00:06:28,960 --> 00:06:34,520
No, I think that's selling myself a little short, but I do have a hard time spending

108
00:06:34,520 --> 00:06:38,600
more than a few years on a given topic.

109
00:06:38,600 --> 00:06:43,680
And I guess we'll get to this, but I will say right now, not super deep into control

110
00:06:43,680 --> 00:06:44,680
at the moment.

111
00:06:44,680 --> 00:06:50,200
So that was something that, that phase currently is over, but control is something I've gone

112
00:06:50,200 --> 00:06:53,080
back and forth with for my entire career.

113
00:06:53,080 --> 00:06:54,560
So I'm sure I'll come back.

114
00:06:54,560 --> 00:06:55,560
Yeah.

115
00:06:55,560 --> 00:07:00,720
I'm just curious about how did you stumble on control theory?

116
00:07:00,720 --> 00:07:02,360
Yeah.

117
00:07:02,360 --> 00:07:07,800
That was just one of those weird life coincidences, I think I got very lucky.

118
00:07:07,800 --> 00:07:12,480
So I, I mean, I think with a lot of these things, one of the things we underappreciate

119
00:07:12,480 --> 00:07:17,200
about academics in general is just these network connections that kind of take you to where

120
00:07:17,200 --> 00:07:18,200
you go, right?

121
00:07:18,200 --> 00:07:19,200
It's all about people.

122
00:07:19,200 --> 00:07:24,520
It's so much more about people than it is necessarily about particularities of research

123
00:07:24,520 --> 00:07:27,120
problems as to why you end up in a place.

124
00:07:27,120 --> 00:07:33,360
And so when I was pretty early on in graduate school, I went to, I went to a talk by John

125
00:07:33,360 --> 00:07:40,280
Doyle and I just, I, I appreciated his style, which hopefully a lot of people here have

126
00:07:40,280 --> 00:07:43,320
seen at one point or another.

127
00:07:43,320 --> 00:07:48,280
And it ended up talking to him a bunch after the talk with some graduate students.

128
00:07:48,280 --> 00:07:52,080
I always remember he said to me, I like you, you have no tact.

129
00:07:52,080 --> 00:07:53,080
I'll remember that.

130
00:07:53,080 --> 00:07:54,560
I, that's, will stick with me forever.

131
00:07:54,560 --> 00:08:01,320
But yeah, so once I met John, John actually introduced me to Raf D'Andrea, who had been

132
00:08:01,320 --> 00:08:03,400
doing a sabbatical in Boston.

133
00:08:03,400 --> 00:08:06,320
His wife was working at a company in Boston.

134
00:08:06,320 --> 00:08:07,640
Raf was currently at Cornell.

135
00:08:07,640 --> 00:08:10,000
This was before he was at ETH.

136
00:08:10,000 --> 00:08:13,680
And so, and then Raf and I hung out for a bit and yeah, that was kind of how it all

137
00:08:13,680 --> 00:08:14,680
started.

138
00:08:14,680 --> 00:08:18,920
So it was John, then Raf, and then I met a bunch of other people and it was, yeah, that

139
00:08:18,920 --> 00:08:21,640
was my, so again, random connection.

140
00:08:22,640 --> 00:08:23,840
Randomly, John liked me.

141
00:08:23,840 --> 00:08:27,400
I think that was kind of the, that was the beginning.

142
00:08:27,400 --> 00:08:28,400
This is super cool.

143
00:08:28,400 --> 00:08:34,200
I guess also given that you mentioned the influence of John, this is probably also one

144
00:08:34,200 --> 00:08:39,160
of the reasons a posteriori of your interest on robust control and perhaps bridging robust

145
00:08:39,160 --> 00:08:44,440
control to the world of machine learning.

146
00:08:44,440 --> 00:08:51,760
I guess my last question before we pause and move on, let's say to more technical terrain,

147
00:08:51,760 --> 00:08:55,040
is about the people that influenced your research.

148
00:08:55,040 --> 00:09:00,160
So if you had to name three of them, I guess one would be probably John Doyle, as you mentioned.

149
00:09:00,160 --> 00:09:01,160
Definitely John.

150
00:09:01,160 --> 00:09:02,160
Yeah.

151
00:09:02,160 --> 00:09:03,160
But what about the other two then?

152
00:09:03,160 --> 00:09:07,760
It's a great, wow.

153
00:09:07,760 --> 00:09:09,000
Other people who've been influenced by research.

154
00:09:09,000 --> 00:09:14,000
I mean, I think, I think again, the way you want, the way I think about it is not necessarily,

155
00:09:14,560 --> 00:09:17,120
and I probably could dig into this more and think about like, who are the people who I

156
00:09:17,120 --> 00:09:20,320
read who like really changed the way I thought.

157
00:09:20,320 --> 00:09:24,440
But I think a lot of the times it's just who are the people who were there to mentor me

158
00:09:24,440 --> 00:09:28,080
and to kind of lead me in these different directions.

159
00:09:28,080 --> 00:09:34,960
I would say, you know, Steve Wright had a huge impact on my, both on my professional

160
00:09:34,960 --> 00:09:39,240
career and the way I think about optimization.

161
00:09:40,240 --> 00:09:45,520
Steve was really instrumental in recruiting me to Wisconsin, where I was before Berkeley.

162
00:09:45,520 --> 00:09:52,440
And he was just really both a personal mentor and also just taught me so much about the

163
00:09:52,440 --> 00:09:54,640
ins and outs of optimization research.

164
00:09:54,640 --> 00:10:00,320
So I think Steve is another person who really stands out in my head as a real, a huge influence.

165
00:10:00,320 --> 00:10:07,240
Let me think about who else, because this is always tricky.

166
00:10:07,240 --> 00:10:10,240
Because there have been a lot of other people who I've just really enjoyed working with,

167
00:10:10,240 --> 00:10:15,960
like, you know, like Raph for bringing me into control and taking me in those directions.

168
00:10:15,960 --> 00:10:21,400
Pretty soon after I met Raph and John, Pablo Pirillo moved from ETH to MIT.

169
00:10:21,400 --> 00:10:23,680
Look at all these ETH connections.

170
00:10:23,680 --> 00:10:25,680
He moved from ETH to MIT.

171
00:10:25,680 --> 00:10:30,520
And then, you know, I learned so much from Pablo.

172
00:10:30,520 --> 00:10:35,640
Dimitri Burtsekas also at MIT is someone I've talked with a lot about so many different

173
00:10:35,640 --> 00:10:36,920
things at different points.

174
00:10:36,920 --> 00:10:41,680
I think what Dimitri is interesting because I learned convex analysis from him.

175
00:10:41,680 --> 00:10:44,080
That was my first real interaction with him.

176
00:10:44,080 --> 00:10:51,200
And I learned a lot about optimization algorithms, but it wasn't until many years later that

177
00:10:51,200 --> 00:10:55,400
I got into optimal control, reinforcement learning, and Dimitri had done so much work

178
00:10:55,400 --> 00:10:56,400
there.

179
00:10:56,400 --> 00:10:59,520
So it's kind of fun to reconnect with him in that space, because we had done so much

180
00:10:59,520 --> 00:11:03,840
of our chatting had been about optimization, then optimization and machine learning.

181
00:11:03,840 --> 00:11:07,440
But then it kind of looped back to the stuff that he was just the, you know, he was one

182
00:11:07,440 --> 00:11:11,040
of the most important researchers in dynamic programming.

183
00:11:11,040 --> 00:11:16,000
And it just wasn't, you know, it's just funny that we go through these trend cycles.

184
00:11:16,000 --> 00:11:20,640
Dynamic programming wasn't maybe as hot a topic when I was in graduate school, but it

185
00:11:20,640 --> 00:11:22,920
was kind of fun to kind of loop back with him.

186
00:11:22,920 --> 00:11:26,800
I certainly have more Burtsekas books than I think any other person.

187
00:11:26,800 --> 00:11:30,000
This is an interesting trivia.

188
00:11:30,000 --> 00:11:33,160
I mean, he writes so many books.

189
00:11:33,160 --> 00:11:34,160
It's not really fair.

190
00:11:34,160 --> 00:11:35,560
So many good books, I would say.

191
00:11:35,560 --> 00:11:36,840
I mean, they're all excellent.

192
00:11:36,840 --> 00:11:37,840
They're all excellent.

193
00:11:37,840 --> 00:11:38,840
Yeah, yeah.

194
00:11:38,840 --> 00:11:39,840
Oh, absolutely.

195
00:11:39,840 --> 00:11:42,360
But it's just also not, I mean, I don't know how anyone could keep up with that kind of

196
00:11:42,360 --> 00:11:43,360
prolific.

197
00:11:43,360 --> 00:11:47,560
This is so impressive.

198
00:11:47,560 --> 00:11:48,560
And you're right.

199
00:11:48,560 --> 00:11:51,000
Like, it's amazing how many of them are good, right?

200
00:11:51,000 --> 00:11:56,120
I mean, his nonlinear optimization book is just, it's just filled with amazing things.

201
00:11:56,120 --> 00:11:58,880
And he had that classic book on, I mean, it was amazing.

202
00:11:58,880 --> 00:12:00,000
I forgot about that.

203
00:12:00,000 --> 00:12:05,240
He wrote this book on distributed computing with Sitsiklis, I don't even remember when

204
00:12:05,240 --> 00:12:06,720
that book was from.

205
00:12:06,720 --> 00:12:07,720
It's old.

206
00:12:07,720 --> 00:12:14,480
And it kind of came back in the early 2010s was a very, like, everybody was into distributed

207
00:12:14,480 --> 00:12:15,480
machine learning.

208
00:12:15,480 --> 00:12:19,960
It turns out that anything you wanted was almost certainly in that book already.

209
00:12:19,960 --> 00:12:22,600
And so I remember interacting with him a lot about that.

210
00:12:22,600 --> 00:12:27,160
He was a little bit upset, honestly, that like, some people didn't realize that, but

211
00:12:27,160 --> 00:12:28,160
it worked out.

212
00:12:28,160 --> 00:12:29,160
Right.

213
00:12:29,320 --> 00:12:33,440
So it's distributed computing, he had done dynamic programming, he had done all sorts

214
00:12:33,440 --> 00:12:38,640
of stuff on stochastic gradients, just a really impressive person.

215
00:12:38,640 --> 00:12:45,440
And I think there's this, lots of people will write lots of papers, but there's really very

216
00:12:45,440 --> 00:12:52,520
few people who will, can turn a research program into courses, right?

217
00:12:52,520 --> 00:12:55,640
That's a hard jump to take something that is like, this is my research, and I'm going

218
00:12:55,640 --> 00:13:01,000
to turn this into something that I could teach to, you know, first year graduate students.

219
00:13:01,000 --> 00:13:06,480
That's taking cutting edge stuff and turning it into that kind of synthesis is, you know,

220
00:13:06,480 --> 00:13:07,480
really, really hard.

221
00:13:07,480 --> 00:13:09,640
And Dimitri is just incredibly talented at it.

222
00:13:09,640 --> 00:13:13,040
Yeah, this requires incredible mental clarity.

223
00:13:13,040 --> 00:13:19,760
I'll just take this assist that you served me to mention that all the people that you

224
00:13:19,760 --> 00:13:27,360
mentioned before are very welcome in this podcast, as well.

225
00:13:27,360 --> 00:13:31,280
You should, Dimitri would be a fun person to chat with, honestly, I think he would be,

226
00:13:31,280 --> 00:13:36,160
I mean, they're all fun, John always is fun, but I think Dimitri has a lot of very interesting

227
00:13:36,160 --> 00:13:37,160
perspectives.

228
00:13:37,160 --> 00:13:38,720
You should, I should, I'll send a pink tip.

229
00:13:38,720 --> 00:13:39,720
Let's see what we can do.

230
00:13:39,720 --> 00:13:40,720
Yeah.

231
00:13:40,720 --> 00:13:41,720
Yeah.

232
00:13:41,720 --> 00:13:42,720
Yeah.

233
00:13:42,720 --> 00:13:49,040
Okay, so maybe we can move on to, I would probably start with Argamin, your legendary

234
00:13:49,920 --> 00:13:55,760
blog, at least legendary for the underground community of machine learning and control geeks.

235
00:13:57,200 --> 00:14:01,680
You posted a wealth of super interesting blog posts there.

236
00:14:03,280 --> 00:14:09,120
Perhaps if I had to summarize the context of your own enterprise, there would be something like,

237
00:14:09,840 --> 00:14:13,760
in theory, there's no difference between theory and practice, but in practice, there is.

238
00:14:15,680 --> 00:14:16,640
What do you say about this?

239
00:14:17,280 --> 00:14:18,400
I mean, that's certainly true.

240
00:14:19,520 --> 00:14:22,800
The question is, is the gap bigger in theory or practice?

241
00:14:22,800 --> 00:14:27,120
And I actually, I think most of the time we say that the gap is bigger in practice,

242
00:14:27,120 --> 00:14:29,200
but I don't know, it just depends.

243
00:14:29,200 --> 00:14:33,840
I mean, I see some pretty loose bounds, especially in the things I've been working with.

244
00:14:35,280 --> 00:14:38,640
Actually, many times in my career, you see these things where the theory is much more

245
00:14:38,640 --> 00:14:40,400
pessimistic than it need be.

246
00:14:40,400 --> 00:14:45,600
So, you know, it's interesting to see that interplay.

247
00:14:46,240 --> 00:14:49,600
Yeah, no, the blog has been a very fun outlet for me.

248
00:14:50,720 --> 00:14:57,120
Yeah, I guess, you know, busting myths is really your cottage industry somehow.

249
00:14:57,120 --> 00:15:02,000
So I was wondering, what was your favorite myth to be busted?

250
00:15:02,000 --> 00:15:06,320
Or what would you be willing to do like in the future?

251
00:15:06,320 --> 00:15:09,040
What's the ultimate myth to bust?

252
00:15:09,600 --> 00:15:15,040
I think sometimes the ones I've found most interesting were the ones I believed.

253
00:15:16,240 --> 00:15:18,880
Right, where it just completely changes my mind.

254
00:15:20,160 --> 00:15:24,080
Usually, this is going to be something led by a graduate student or a postdoc in the

255
00:15:24,080 --> 00:15:29,280
group, and they just show you something that, or show me something that I just don't, would

256
00:15:29,280 --> 00:15:31,360
never have believed a day before.

257
00:15:32,960 --> 00:15:36,640
I think probably like the, there are probably lots of good examples here.

258
00:15:36,640 --> 00:15:42,720
But a really notable one, and one I've been writing about, again, recently, is there's

259
00:15:42,720 --> 00:15:47,520
this concept in machine learning and statistics of overfitting to data, right?

260
00:15:47,520 --> 00:15:52,240
I mean, where the idea would be that somehow you believe a little bit too much in your

261
00:15:52,240 --> 00:15:57,280
data and are unable to extrapolate out to new data, because you've really, you know,

262
00:15:57,280 --> 00:16:04,560
you've somehow caught on to aberrations and particularities of the data set you have.

263
00:16:04,560 --> 00:16:08,640
So it's a very common thing, and we teach it to our undergraduates, that, you know,

264
00:16:08,640 --> 00:16:10,000
overfitting is a huge problem.

265
00:16:11,360 --> 00:16:15,760
And in particular, the way that we do machine learning causes overfitting.

266
00:16:15,760 --> 00:16:18,480
So what, let me, let me dive into that.

267
00:16:18,480 --> 00:16:19,920
Yeah, let's jump into this.

268
00:16:19,920 --> 00:16:20,480
Absolutely.

269
00:16:20,480 --> 00:16:21,120
Okay.

270
00:16:21,120 --> 00:16:21,440
Yeah.

271
00:16:21,440 --> 00:16:26,960
So what we do in machine learning, just from the beginning of time, it turns out, and I

272
00:16:26,960 --> 00:16:31,280
could talk more about, I could talk more about the history of this, because I've also been

273
00:16:31,280 --> 00:16:34,320
absolutely involved in trying to dig into some of the history.

274
00:16:34,320 --> 00:16:37,840
Feel free to express your own necessities.

275
00:16:37,840 --> 00:16:39,680
Scratch your own itches if you want.

276
00:16:39,680 --> 00:16:44,000
Yeah, well, so from the beginning of time in machine learning, there was this notion

277
00:16:44,000 --> 00:16:50,160
that you would have a data set that you train on, a data set that you evaluate or test on,

278
00:16:50,160 --> 00:16:52,080
and then the data in the wild.

279
00:16:52,880 --> 00:16:59,840
And so it turns out that this, the earliest we found of this practice goes back to the

280
00:16:59,840 --> 00:17:00,880
late 1950s.

281
00:17:00,880 --> 00:17:02,880
It's a really old idea.

282
00:17:03,920 --> 00:17:08,480
And it's kind of, it's kind of amazing that it was, it was, I mean, a lot of people think

283
00:17:08,480 --> 00:17:10,560
of that as an idea that comes from statistics.

284
00:17:10,560 --> 00:17:13,360
And it's an idea that comes from machine learning before statistics.

285
00:17:13,360 --> 00:17:14,640
It's interesting how early that is.

286
00:17:14,640 --> 00:17:15,680
Hashtag today I learned.

287
00:17:16,560 --> 00:17:20,160
Well, I mean, I think, I think one thing that I've learned, and something I never appreciated

288
00:17:20,160 --> 00:17:27,200
before was, before recently was, you know, that machine learning and statistics are much

289
00:17:27,200 --> 00:17:29,440
more contemporary than I think we realize.

290
00:17:29,440 --> 00:17:33,520
I think we, a lot of people think statistics is very old, because probability is very old.

291
00:17:34,400 --> 00:17:39,440
But statistics is kind of the, you know, the appropriate application of probabilistic thinking

292
00:17:39,440 --> 00:17:42,400
to how we interact with the real world.

293
00:17:42,400 --> 00:17:48,480
And I think those kinds of real aspects weren't nailed down to the 30s, 1930s, which is just

294
00:17:48,480 --> 00:17:49,040
fascinating.

295
00:17:49,040 --> 00:17:55,120
I mean, there was work by, you know, notably Ronald Fisher and some of his predecessors

296
00:17:55,440 --> 00:17:59,280
and Gossett, who was the guy who invented the t-test.

297
00:17:59,280 --> 00:18:01,200
This was done in the 20s or so.

298
00:18:01,200 --> 00:18:05,600
But that wasn't really until the 30s, where there was like a real rigorous body of statistics,

299
00:18:05,600 --> 00:18:09,040
notably by jersey name and kind of nailed that down.

300
00:18:09,040 --> 00:18:11,840
But, and it's kind of crazy to think that that's the 30s.

301
00:18:11,840 --> 00:18:15,280
It's like, you would think that that would be the 1830s, but it's the 1930s.

302
00:18:15,280 --> 00:18:19,040
It's much, much later than, than I think people appreciate.

303
00:18:19,280 --> 00:18:26,480
So machine learning is roughly 1950s, and really was well established by 1960.

304
00:18:26,480 --> 00:18:31,440
So it's only a gap of, you know, what are we talking about here, like 25 years, not

305
00:18:31,440 --> 00:18:35,440
decades, not sorry, not like, you know, it's insane, centuries, it was really a gap of

306
00:18:35,440 --> 00:18:36,320
about 25 years.

307
00:18:36,320 --> 00:18:36,800
Yeah.

308
00:18:36,800 --> 00:18:41,200
To the full foundation of statistical machine learning, and the full foundation of statistics,

309
00:18:41,200 --> 00:18:42,800
they're very intertwined with each other.

310
00:18:42,800 --> 00:18:46,800
So I think that's where does machine learning start for you with a perceptron?

311
00:18:47,520 --> 00:18:51,520
Um, with the kind of like the understanding of how the perceptron works.

312
00:18:51,520 --> 00:18:52,160
Okay.

313
00:18:52,160 --> 00:18:55,360
And it's not, it's not just the perceptron, because there are other things that people

314
00:18:55,360 --> 00:19:00,480
were experimenting with at the time, lots of different even like model based methods,

315
00:19:01,280 --> 00:19:06,000
if you kind of go back to literature, but really just the idea of understanding that

316
00:19:06,000 --> 00:19:10,480
there's this, you know, in sample versus out sample performance, I could characterize in

317
00:19:10,480 --> 00:19:12,720
sample performance and out of sample performance.

318
00:19:13,520 --> 00:19:19,280
And so kind of particular way, this was really, this is late 1950s, through early 1960s was

319
00:19:19,280 --> 00:19:20,960
where these ideas really came together.

320
00:19:20,960 --> 00:19:21,200
Yeah.

321
00:19:21,200 --> 00:19:25,680
And going back to the myth that you actually want to bust here, am I understanding correctly

322
00:19:25,680 --> 00:19:29,040
that what you say is that we should overfit?

323
00:19:29,040 --> 00:19:30,720
Or we shouldn't be scared to overfit?

324
00:19:30,720 --> 00:19:31,040
Or?

325
00:19:31,040 --> 00:19:31,280
No.

326
00:19:32,240 --> 00:19:34,480
Well, I think it's that we shouldn't be scared to overfit.

327
00:19:34,480 --> 00:19:38,640
So one of the things that people kind of thought and statisticians have said this is that if

328
00:19:38,640 --> 00:19:44,400
you, you look at what we're doing, where we have, you know, some data set, and some

329
00:19:45,120 --> 00:19:49,840
testing set, and we're just always, you know, using the this one data set, and then evaluating

330
00:19:49,840 --> 00:19:54,080
on the testing set, and repeating this process, you know, you could construct really weird

331
00:19:54,080 --> 00:19:59,920
examples where you essentially overfit to this testing set, and then you get some new

332
00:19:59,920 --> 00:20:02,560
data, and you've just done you just do terribly.

333
00:20:02,560 --> 00:20:05,600
So there's, there are ways to construct examples where that happens.

334
00:20:06,240 --> 00:20:12,080
And in my group, we've done now just hundreds and hundreds of experiments, we're showing

335
00:20:12,080 --> 00:20:13,280
that that just doesn't happen.

336
00:20:14,240 --> 00:20:19,360
So we've done a lot of reproduction studies where we'll reproduce test sets, trying to

337
00:20:19,360 --> 00:20:23,360
be as close as possible to the generating process of the original test set.

338
00:20:23,360 --> 00:20:28,080
And we've done this for data sets that like the ImageNet data set, or the squad data set,

339
00:20:28,080 --> 00:20:32,560
or a variety of others where we can regenerate these kind of test sets.

340
00:20:33,280 --> 00:20:42,800
And we see that you don't there's no overfitting the models that perform the best on the original

341
00:20:42,800 --> 00:20:45,760
test set are the ones that perform the best on the new data.

342
00:20:46,640 --> 00:20:48,960
So completely goes against what you were told.

343
00:20:48,960 --> 00:20:50,720
So you can't How do you explain this?

344
00:20:53,440 --> 00:20:54,640
That is a wonderful question.

345
00:20:57,040 --> 00:20:59,840
I don't I wish I had a good answer for that.

346
00:20:59,840 --> 00:21:04,400
I think that's, to me, I mean, sometimes there's like, we almost expect too much from

347
00:21:04,400 --> 00:21:05,040
our theory.

348
00:21:05,040 --> 00:21:05,540
Okay.

349
00:21:05,920 --> 00:21:06,640
And that's okay.

350
00:21:08,080 --> 00:21:13,120
I think there are a variety of there are a variety of explanations.

351
00:21:13,920 --> 00:21:19,440
Some of them, some of them are mathematical, and then some of them are just sociological,

352
00:21:19,440 --> 00:21:19,940
I think.

353
00:21:20,960 --> 00:21:25,120
I think that, you know, we pick problems very carefully in machine learning, such that they're

354
00:21:25,120 --> 00:21:26,720
not hurt by overfitting.

355
00:21:26,720 --> 00:21:27,840
I think that's part of it.

356
00:21:27,840 --> 00:21:32,720
Like we're, the problems that we're looking at tend to be not the not the weird, egregious

357
00:21:32,720 --> 00:21:36,160
examples that we come up with as counter examples.

358
00:21:36,720 --> 00:21:37,840
So I think that's one part.

359
00:21:38,880 --> 00:21:45,120
I think another part is that there are a lot of the analysis that would predict overfitting

360
00:21:45,120 --> 00:21:51,440
assume some very pathological behavior in the statistics to happen.

361
00:21:51,440 --> 00:21:55,760
And, you know, the way that we actually work, there are a variety of different ways that,

362
00:21:55,760 --> 00:22:02,400
you know, if, for example, if predictors kind of are correlated, like they kind of

363
00:22:02,400 --> 00:22:06,480
give you correlated answers, you know, the bounds on overfitting get better.

364
00:22:06,480 --> 00:22:08,800
So it looks like you can actually have more models.

365
00:22:09,840 --> 00:22:18,240
If you don't really hyper optimize to the test error score, that also can kind of prevent

366
00:22:18,240 --> 00:22:18,720
overfitting.

367
00:22:18,720 --> 00:22:20,800
So there's a variety of like these little tricks.

368
00:22:20,800 --> 00:22:24,000
And maybe each one of those is explains a little piece.

369
00:22:24,800 --> 00:22:25,760
But I don't think there is.

370
00:22:26,640 --> 00:22:32,880
Right now, I don't have a satisfactory explanation for why we don't actually see this kind of

371
00:22:32,880 --> 00:22:34,240
overfitting behavior.

372
00:22:34,240 --> 00:22:37,280
I'll say this, though, the reason why I'm not spending too much time worrying about

373
00:22:37,280 --> 00:22:42,640
it, because the other thing we observed is that while you see the models that perform

374
00:22:42,640 --> 00:22:48,320
really well on the new on the old tests that perform really well, on the newer data, they

375
00:22:48,320 --> 00:22:54,720
almost always perform worse on the newer data than they did on the original old data.

376
00:22:55,760 --> 00:22:58,160
So now everybody's performing worse.

377
00:22:58,720 --> 00:23:02,480
It's not that, you know, so again, it's not that we overfit necessarily.

378
00:23:02,480 --> 00:23:09,600
It's just that small changes in the data creation process can create large changes in the actual

379
00:23:09,600 --> 00:23:10,720
predictive performance.

380
00:23:10,720 --> 00:23:11,520
And that's worrying.

381
00:23:12,160 --> 00:23:13,840
That's not really an issue of overfitting.

382
00:23:13,840 --> 00:23:15,520
That's just an issue of extrapolation.

383
00:23:16,320 --> 00:23:23,200
And it's interesting how sensitive machine learning methods can be to small changes in

384
00:23:23,200 --> 00:23:25,120
how data presents itself.

385
00:23:26,080 --> 00:23:28,480
There are so many directions that I can take off from here.

386
00:23:28,480 --> 00:23:34,080
I mean, well, first of all, I guess the first naive comment on this issue is that it seems

387
00:23:34,080 --> 00:23:41,200
to me that this could be an issue of generosity in some way, or like the data that we get

388
00:23:42,000 --> 00:23:48,720
or better, the way we build models on the data that we get somehow do not suffer from

389
00:23:48,720 --> 00:23:51,520
some sort of generosity problem and vice versa.

390
00:23:52,240 --> 00:23:56,560
We may be too sensitive to the data somehow.

391
00:23:56,560 --> 00:24:01,840
And so therefore, we get worse errors on new data whenever we get new data.

392
00:24:02,720 --> 00:24:03,220
Yeah.

393
00:24:04,320 --> 00:24:06,800
Would this be fair to say from your point of view?

394
00:24:07,520 --> 00:24:10,080
So I don't claim to have answers for this.

395
00:24:10,080 --> 00:24:11,680
So I don't know yet.

396
00:24:11,680 --> 00:24:16,640
But I do think there are some things that we don't necessarily consider very frequently

397
00:24:16,640 --> 00:24:23,120
in machine learning that we should, which is just that they're all of experimental science

398
00:24:23,120 --> 00:24:25,120
kind of suffers from this generalization problem.

399
00:24:26,640 --> 00:24:31,040
There are things that you do in a laboratory setting that you can go do out in a clinical

400
00:24:31,040 --> 00:24:34,000
setting, and they just don't pan out the way you want them to.

401
00:24:34,800 --> 00:24:35,040
Right.

402
00:24:35,040 --> 00:24:39,360
I mean, I think this is kind of very, very common in medicine, where you do, you know,

403
00:24:39,360 --> 00:24:42,880
you build this drug, it looks really good, you go and deploy it, and it's not quite as

404
00:24:42,880 --> 00:24:43,920
good as you want it to be.

405
00:24:45,120 --> 00:24:49,920
And this is also true in psychology, where you do a study, and, you know, you do a study

406
00:24:49,920 --> 00:24:55,280
on a bunch of college aged males, perhaps, and then you try to see does this apply to,

407
00:24:55,280 --> 00:24:59,840
you know, middle aged women, and it doesn't, because that's obviously a huge enough context

408
00:24:59,840 --> 00:25:00,800
shift that you see it.

409
00:25:02,320 --> 00:25:08,640
So on the one hand, understanding how to deal with those shifts between contexts and

410
00:25:08,640 --> 00:25:13,360
populations, from the experimental setting, to the general setting is hugely important.

411
00:25:13,360 --> 00:25:18,400
And something very like that, that's something that we're doing a lot of active research on

412
00:25:19,120 --> 00:25:19,680
in my group.

413
00:25:20,240 --> 00:25:21,520
So that's one thing.

414
00:25:21,520 --> 00:25:26,640
I think the warning, though, that's still very troubling, is that in the machine learning

415
00:25:26,640 --> 00:25:29,760
context, these data sets don't really look that different.

416
00:25:29,760 --> 00:25:33,280
And you see a large difference, the fact that you could have two data sets that kind of

417
00:25:33,280 --> 00:25:37,600
look the same, and have a huge difference in performance, really freaks me out about

418
00:25:37,600 --> 00:25:38,320
machine learning.

419
00:25:38,320 --> 00:25:41,440
Indicating some sort of high sensitivity, somehow.

420
00:25:41,440 --> 00:25:43,120
Super high sensitivity, yeah.

421
00:25:43,120 --> 00:25:48,000
And what people do in practice, and so we can tell our listeners, you know, the fix

422
00:25:48,000 --> 00:25:50,960
for this in practice is constant retraining.

423
00:25:50,960 --> 00:25:52,160
And there's a lot of that.

424
00:25:52,160 --> 00:25:56,400
Could you explain, like, give an analogy of what that means, maybe, for the audience?

425
00:25:56,400 --> 00:26:02,320
So this would be the kind of thing where you constantly look for your, you know, as you

426
00:26:02,320 --> 00:26:08,400
have something deployed, you constantly look for errors in your deployed system.

427
00:26:08,400 --> 00:26:12,560
And as soon as you see them, or even maybe not if you see them, you're still collecting

428
00:26:12,560 --> 00:26:15,120
data, and you use that data to retrain the model.

429
00:26:15,920 --> 00:26:20,000
And this is definitely a very commonplace way to get around the sensitivity, and something

430
00:26:20,000 --> 00:26:25,200
that's very common in industrial applications of machine learning.

431
00:26:25,200 --> 00:26:28,160
You do have to refine the models, you do have to retrain the models.

432
00:26:28,880 --> 00:26:33,360
They're very unstable to the ravages of time.

433
00:26:34,000 --> 00:26:39,280
Maybe for the control theoretically oriented audience out there, I suppose that we are

434
00:26:39,280 --> 00:26:42,240
kind of speaking of robustness in this guys.

435
00:26:42,240 --> 00:26:43,600
Am I wrong?

436
00:26:43,600 --> 00:26:44,720
Probably, probably.

437
00:26:44,720 --> 00:26:50,480
I think one thing that's interesting that is missing, and that changes things pretty

438
00:26:50,480 --> 00:26:52,640
dramatically, is feedback.

439
00:26:53,760 --> 00:26:57,440
So these predictive things, you know, you're at the whim of the prediction.

440
00:26:58,240 --> 00:27:01,120
So you're kind of, you're never able to really course correct.

441
00:27:01,120 --> 00:27:06,240
I think one of the fascinating things about control is we know that even with poor prediction,

442
00:27:06,240 --> 00:27:12,080
we can have stable and robust behavior by proper design and feedback design, right?

443
00:27:12,080 --> 00:27:16,880
And that's one of the more fascinating, underappreciated aspects of control.

444
00:27:16,880 --> 00:27:24,960
And you're giving me another assist to shift gears and talk about your excursion into control

445
00:27:24,960 --> 00:27:25,840
theory, I suppose.

446
00:27:25,840 --> 00:27:26,340
Sure.

447
00:27:27,060 --> 00:27:33,700
You've written a fantastic series of blog posts on Argmin about reinforcement learning

448
00:27:33,700 --> 00:27:36,900
and its connection to optimal control.

449
00:27:36,900 --> 00:27:41,540
And this turned out to be also a fantastic paper on the annual reviews on control.

450
00:27:41,540 --> 00:27:44,180
Maybe we can speak about that if you like.

451
00:27:44,180 --> 00:27:45,460
Absolutely, absolutely.

452
00:27:47,380 --> 00:27:54,180
So summarizing your journey in this area would be impossible in a few seconds.

453
00:27:54,740 --> 00:28:01,460
But maybe if it's not too much to ask to you, could you give us a sense of your personal

454
00:28:01,460 --> 00:28:03,620
perspective on your journey in control, maybe?

455
00:28:05,380 --> 00:28:05,940
Yeah.

456
00:28:05,940 --> 00:28:10,020
So this is now, I have to think a little, I have to think back to it.

457
00:28:10,020 --> 00:28:13,140
I used to have a good story for how I got into it again.

458
00:28:13,140 --> 00:28:18,340
I mean, I'm assuming that what we're talking about is, you know, my recent engagement with

459
00:28:19,380 --> 00:28:20,420
reinforcement learning.

460
00:28:20,420 --> 00:28:20,660
Yes.

461
00:28:20,660 --> 00:28:26,020
Because I've, you know, I had, I did some work on distributed control as a graduate

462
00:28:26,020 --> 00:28:26,500
student.

463
00:28:26,500 --> 00:28:28,340
I've done some work on system identification.

464
00:28:29,380 --> 00:28:33,460
As a postdoc, I go back and forth with control and dynamical systems.

465
00:28:34,500 --> 00:28:37,860
I'm trying to remember exactly what got me interested.

466
00:28:39,940 --> 00:28:42,420
Because it's been so long, but I'm guessing what happened.

467
00:28:42,420 --> 00:28:44,500
I'll just again, this is a little bit of a guess.

468
00:28:44,500 --> 00:28:51,460
But I'm just trying to try to put myself back in a 2014 mindset is there were a few

469
00:28:51,460 --> 00:28:52,260
things that were afoot.

470
00:28:52,260 --> 00:28:56,980
First of all, there was a lot of hype around reinforcement learning in 2014.

471
00:28:56,980 --> 00:28:59,460
And it is crazy that that's now eight years ago.

472
00:29:00,020 --> 00:29:01,380
That's really wild to me.

473
00:29:01,380 --> 00:29:02,020
Yeah.

474
00:29:02,020 --> 00:29:02,980
It definitely did not.

475
00:29:02,980 --> 00:29:07,860
I don't think it actually ended up paying out the way that people were talking, the

476
00:29:07,860 --> 00:29:11,220
way they were talking about it, like self-driving cars in 2020.

477
00:29:11,220 --> 00:29:14,500
They were definitely saying that in 2014, and it just didn't happen.

478
00:29:15,300 --> 00:29:19,220
And it'd be nice if they could actually, you know, reflect on that.

479
00:29:19,220 --> 00:29:21,060
But I think they've moved on to Bitcoin or something.

480
00:29:21,060 --> 00:29:21,940
They're onto something new.

481
00:29:21,940 --> 00:29:22,500
I don't know.

482
00:29:22,500 --> 00:29:23,860
Elon Musk is buying Twitter.

483
00:29:23,860 --> 00:29:24,260
I don't know.

484
00:29:24,260 --> 00:29:25,140
We're doing other stuff.

485
00:29:26,100 --> 00:29:27,060
We're moving on.

486
00:29:28,580 --> 00:29:32,660
But I think it would be helpful to have some reflection on why didn't that pay out.

487
00:29:32,660 --> 00:29:34,660
We're here also for that purpose.

488
00:29:34,660 --> 00:29:37,300
I mean, locomotion was another big one, I suppose.

489
00:29:37,620 --> 00:29:41,060
Yeah, so locomotion, I think, came a little bit later.

490
00:29:41,060 --> 00:29:44,660
But for me, the big things that were, I think, you know, happening at Berkeley was there

491
00:29:44,660 --> 00:29:46,340
was a lot of interest in self-driving cars.

492
00:29:46,900 --> 00:29:49,940
And then there were a few really interesting papers that were fusing.

493
00:29:50,580 --> 00:29:56,500
I was very interested in this paper by Chelsea Finn and Sergey Levine and Peter Abbeel, Trevor

494
00:29:56,500 --> 00:30:01,060
Darrell, that kind of showed that you could like learn these visual, you could do control

495
00:30:01,060 --> 00:30:02,500
directly from video.

496
00:30:02,500 --> 00:30:04,900
That was a very, very impressive.

497
00:30:04,980 --> 00:30:10,180
It's one of the holy grails, I suppose, like merging perception and control in a principled

498
00:30:10,180 --> 00:30:10,420
way.

499
00:30:11,140 --> 00:30:14,260
This is one of the first to do it in a compelling way, I think.

500
00:30:14,980 --> 00:30:16,980
So people were very excited about that.

501
00:30:16,980 --> 00:30:20,660
So I think we just started digging in a little bit in the group about, you know, what's going

502
00:30:20,660 --> 00:30:21,060
on here.

503
00:30:21,060 --> 00:30:22,740
I was like, I know control theory.

504
00:30:22,740 --> 00:30:23,700
I know machine learning.

505
00:30:23,700 --> 00:30:24,740
I should be able to do this.

506
00:30:24,740 --> 00:30:25,700
I think it was something like that.

507
00:30:27,060 --> 00:30:29,860
I was like, also, I didn't believe a lot of what people were selling.

508
00:30:29,860 --> 00:30:32,020
So we started looking into that.

509
00:30:32,020 --> 00:30:36,420
And yeah, it led us down quite a path from there, I think.

510
00:30:36,420 --> 00:30:36,900
Yeah.

511
00:30:36,900 --> 00:30:43,220
I mean, one of the cool things that ended up happening was that of merging or at least

512
00:30:43,220 --> 00:30:50,100
connecting the world of robust control and with that of high dimensional statistics.

513
00:30:50,100 --> 00:30:55,940
And I think that was a major achievement from my very humble perspective in the sense that

514
00:30:57,300 --> 00:31:01,460
robust control tends to assume that uncertainties are somehow bounded.

515
00:31:02,020 --> 00:31:05,780
But we never really explain where those bounds come from.

516
00:31:05,780 --> 00:31:14,100
Whereas maybe machine learning can kind of help us in estimating those uncertainties

517
00:31:14,100 --> 00:31:19,940
from finite sample statistics, finite sample, finite data, if you want.

518
00:31:21,140 --> 00:31:25,380
Would that make sense to you as a short summary of your excursion?

519
00:31:25,380 --> 00:31:26,260
No, absolutely.

520
00:31:27,300 --> 00:31:28,100
That's part of it.

521
00:31:28,260 --> 00:31:30,180
I think definitely there was a...

522
00:31:31,140 --> 00:31:35,460
I mean, the other way I'd say that is that machine learning is also very bad about quantifying

523
00:31:35,460 --> 00:31:36,740
the uncertainty in its predictions.

524
00:31:37,540 --> 00:31:38,180
It's terrible.

525
00:31:39,300 --> 00:31:45,860
So it's this kind of thing where if you assume a lot of things about a generative process,

526
00:31:46,420 --> 00:31:51,140
if you assume a lot of things about how the data is coming to you, I could give you some

527
00:31:51,140 --> 00:31:53,140
kind of uncertainty quantification.

528
00:31:53,140 --> 00:31:56,580
But you can never check whether or not those things you're supposed to assume are true.

529
00:31:57,540 --> 00:32:00,660
It's actually, I think one of these things that's a little worrying sometimes in machine

530
00:32:00,660 --> 00:32:04,180
learning is this reliance on probability.

531
00:32:05,460 --> 00:32:11,780
I think, honestly, this is a John Doyle thing that was hammered into me, is that you should

532
00:32:11,780 --> 00:32:13,700
never add stochastics until you need to.

533
00:32:14,340 --> 00:32:14,820
Okay.

534
00:32:14,820 --> 00:32:16,420
Which I think is a very interesting perspective.

535
00:32:16,420 --> 00:32:18,020
Don't add stochastics until you need to.

536
00:32:18,580 --> 00:32:19,080
And...

537
00:32:19,700 --> 00:32:21,220
So when is it that you need to then?

538
00:32:22,340 --> 00:32:23,460
That's the hard question.

539
00:32:24,260 --> 00:32:28,100
So in machine learning, we don't have any theory without some kind of notion of...

540
00:32:30,740 --> 00:32:31,240
Well...

541
00:32:31,620 --> 00:32:34,260
A fully deterministic machine learning.

542
00:32:34,260 --> 00:32:37,700
So, no, no, there is actually, and this is actually really fascinating.

543
00:32:37,700 --> 00:32:39,460
There is a fully deterministic machine learning.

544
00:32:39,460 --> 00:32:40,820
We just don't really appreciate it.

545
00:32:41,940 --> 00:32:44,180
It's just because it's not accessible.

546
00:32:44,180 --> 00:32:48,260
I'll say this, I love my friends who work on this stuff, but it's not very accessible.

547
00:32:48,260 --> 00:32:51,460
So there is a notion in machine learning that's called regret.

548
00:32:52,260 --> 00:32:52,740
Yes.

549
00:32:52,740 --> 00:32:58,100
And regret has this idea that what you will look at is, you know, you'll have some sequence

550
00:32:58,100 --> 00:33:02,020
and you'll say, you know, this is the best thing that could have been predicted.

551
00:33:02,020 --> 00:33:07,220
The best predictions you could have made about the sequence, somehow, if you were omniscient,

552
00:33:07,220 --> 00:33:09,140
or like you saw the whole sequence in advance.

553
00:33:10,500 --> 00:33:14,260
And then you compare that to how well your algorithm makes predictions.

554
00:33:14,260 --> 00:33:14,500
Okay.

555
00:33:14,500 --> 00:33:18,660
So you have something like, if I somehow saw the whole sequence, what would be the best

556
00:33:18,660 --> 00:33:20,020
predictor I could build?

557
00:33:20,020 --> 00:33:21,620
Which would be funny, right?

558
00:33:21,620 --> 00:33:25,300
You're somehow saying, I saw everything that happened, and now you're going to tell me,

559
00:33:25,300 --> 00:33:30,180
how could I predict, you know, Tuesday from Monday, given that I've already seen Wednesday,

560
00:33:30,180 --> 00:33:30,900
Thursday, Friday.

561
00:33:30,900 --> 00:33:35,300
It's a little bit of a confusing setup, but that's a very strong assumption, right?

562
00:33:35,300 --> 00:33:39,940
So assuming that you could see all of that future information, what would be the best

563
00:33:39,940 --> 00:33:41,540
prediction you could have ever made?

564
00:33:41,540 --> 00:33:42,580
And that's deterministic.

565
00:33:42,580 --> 00:33:48,180
It only depends on the actual values that you saw Monday, on Tuesday, on Wednesday.

566
00:33:48,180 --> 00:33:49,380
There's no distribution.

567
00:33:49,380 --> 00:33:50,340
There's no sampling.

568
00:33:50,900 --> 00:33:51,620
Nothing like that.

569
00:33:52,260 --> 00:33:59,620
I found absolutely interesting the linearization principle, by which you say that if a machine

570
00:33:59,620 --> 00:34:04,660
learning algorithm does crazy things when restricted to linear models, it's going to

571
00:34:04,660 --> 00:34:07,300
do crazy things on complex nonlinear models too.

572
00:34:07,300 --> 00:34:12,660
And I think that if we remove machine learning and just say control, probably we get the

573
00:34:12,660 --> 00:34:14,100
same output.

574
00:34:14,100 --> 00:34:15,140
I think it's similar.

575
00:34:15,140 --> 00:34:18,660
And honestly, Aizerman is kind of like, was a bit of an inspiration for that as well.

576
00:34:18,660 --> 00:34:23,940
I mean, I feel like a lot of the, I know Aizerman was talking about feedback with,

577
00:34:24,820 --> 00:34:30,900
now we're getting into the weeds of memoryless nonlinearity, but I think that this, we learned

578
00:34:30,900 --> 00:34:37,220
a lot about optimization from kind of looking at Aizerman's conjecture.

579
00:34:37,220 --> 00:34:42,180
And so I think there is this, that interplay happens everywhere.

580
00:34:42,180 --> 00:34:43,540
I think it just happens everywhere.

581
00:34:43,540 --> 00:34:47,380
Sometimes you just, you're surprised to see how many insights you can get from linear

582
00:34:47,380 --> 00:34:49,220
systems or from linear regression.

583
00:34:49,860 --> 00:34:50,500
Absolutely.

584
00:34:50,500 --> 00:34:55,540
I'm just going to mention for our audience that Aizerman is very well known for a conjecture,

585
00:34:56,180 --> 00:35:03,380
an eponymous conjecture related to the stability of a feedback loop that involves a nonlinearity

586
00:35:03,380 --> 00:35:05,060
plus some kind of bounds.

587
00:35:05,060 --> 00:35:07,460
You can Google that, probably find it on Wikipedia.

588
00:35:08,020 --> 00:35:08,260
Yeah.

589
00:35:08,260 --> 00:35:14,740
And touching on this point, I suppose the running example of your whole enterprise in

590
00:35:14,740 --> 00:35:16,180
control has been LQR.

591
00:35:18,180 --> 00:35:18,900
Better for worse.

592
00:35:19,860 --> 00:35:20,900
For better, for worse.

593
00:35:20,900 --> 00:35:21,620
Yeah.

594
00:35:21,620 --> 00:35:29,300
And it was just so fascinating to see, I mean, how much else can we say about it, at least

595
00:35:29,300 --> 00:35:35,860
regarding this somewhat old problem with new eyes, with somebody, with the eyes of someone

596
00:35:35,860 --> 00:35:41,300
who has been extensively working also with other communities, such as machine learning.

597
00:35:41,380 --> 00:35:47,380
And I guess your main finding in this respect would be that certainty equivalents work.

598
00:35:48,100 --> 00:35:49,060
That's a big one.

599
00:35:49,060 --> 00:35:49,700
I think, let's see.

600
00:35:50,420 --> 00:35:54,420
So yeah, my group did a lot of stuff on LQR.

601
00:35:56,260 --> 00:35:58,740
I will say that, like, I love that certainty.

602
00:35:58,740 --> 00:36:02,420
Let me come back to certainty equivalence, because I do love that result.

603
00:36:02,420 --> 00:36:08,580
But I think the thing that we got the whole group super excited was Nikolai Motny, who's

604
00:36:08,580 --> 00:36:12,740
now a faculty member at Penn, visited us as a postdoc.

605
00:36:13,700 --> 00:36:19,860
And we had been really trying to figure out how to understand, well, what do you do when

606
00:36:19,860 --> 00:36:23,460
you have this kind of bad fit LQR model and you do control?

607
00:36:23,460 --> 00:36:25,540
And what's the right way to analyze it?

608
00:36:25,540 --> 00:36:30,260
Like, how can you even talk about how to analyze this thing where, you know, you're doing some

609
00:36:30,260 --> 00:36:31,700
kind of reinforcement learning?

610
00:36:31,700 --> 00:36:32,660
Do I fit the model?

611
00:36:32,660 --> 00:36:33,620
Do I do something else?

612
00:36:34,420 --> 00:36:40,820
And my students, Sarah Dean, Hori Amania, Stephen Tu, figured out a cool way to, like,

613
00:36:40,820 --> 00:36:45,700
kind of quantify the uncertainty when you fit an LQR model to data.

614
00:36:45,700 --> 00:36:48,900
And they have really, they've done a lot since on that, too.

615
00:36:48,900 --> 00:36:53,460
So they really, but we couldn't figure out what you do with that certainty equivalence.

616
00:36:53,460 --> 00:36:55,220
And so we tried a bunch of different ideas.

617
00:36:56,180 --> 00:36:57,540
Sorry, not with certainty equivalence.

618
00:36:57,540 --> 00:36:58,020
Apologies.

619
00:36:58,020 --> 00:36:58,660
Let me step back.

620
00:36:58,660 --> 00:37:01,460
What do you do with that uncertainty quantification, right?

621
00:37:02,180 --> 00:37:07,460
And so now you have this thing where I have a linear system with uncertain parameters,

622
00:37:07,460 --> 00:37:10,900
and we tried to use a bunch of different ideas from robust control.

623
00:37:11,700 --> 00:37:15,140
And we kept running into walls in the analyses.

624
00:37:16,180 --> 00:37:20,900
You know, we worked, we were talking a lot with Andy Packard, and we had a lot of interesting

625
00:37:20,900 --> 00:37:24,660
ideas, like mu synthesis related ideas that maybe could have worked.

626
00:37:24,660 --> 00:37:26,180
There's a lot of the stuff that could have worked.

627
00:37:26,180 --> 00:37:30,820
But what was mind blowing is, you know, Nikolai came and within two weeks, he applied this

628
00:37:30,820 --> 00:37:34,500
stuff that he had been working on, which he calls system level synthesis.

629
00:37:34,500 --> 00:37:41,060
And yeah, within two weeks, we had amazing theorems that showed that how to do it.

630
00:37:41,060 --> 00:37:47,860
You know, we could quantify how many samples you needed to do robust control of a system

631
00:37:47,860 --> 00:37:52,980
where you have a, you know, you want to do quadratic LQR, you want to minimize the quadratic

632
00:37:52,980 --> 00:37:55,220
cost, you have uncertain linear dynamics.

633
00:37:55,220 --> 00:37:59,700
And we could quantify how many samples you needed to get an uncertainty quantification

634
00:37:59,700 --> 00:38:04,420
of the system that was good enough to do near optimal control.

635
00:38:04,420 --> 00:38:06,020
Really fascinating result.

636
00:38:06,660 --> 00:38:11,140
Maybe we can step back here just to help the audience a little bit and just mention what

637
00:38:11,140 --> 00:38:12,340
is SLS.

638
00:38:12,340 --> 00:38:13,700
I am aware of what that is.

639
00:38:14,340 --> 00:38:20,980
But maybe you can help me with an analogy for the audience, like to explain what that

640
00:38:20,980 --> 00:38:22,180
does and what it means.

641
00:38:22,900 --> 00:38:29,540
Yeah, so system level synthesis, I think there are multiple ways to kind of think about what

642
00:38:29,540 --> 00:38:35,940
that, how we approach that problem, or how we approach thinking about that problem.

643
00:38:37,060 --> 00:38:40,740
And I'm trying to think what's the best one that the control theoretic audience will

644
00:38:40,740 --> 00:38:41,620
respond to.

645
00:38:41,620 --> 00:38:44,660
I mean, as always is the case of control, people are going to say, oh, we did it this

646
00:38:44,660 --> 00:38:45,700
way in the 80s.

647
00:38:45,700 --> 00:38:47,060
Oh, we did this way in the 60s.

648
00:38:47,060 --> 00:38:49,140
I don't want to necessarily step on anyone's toes.

649
00:38:49,140 --> 00:38:56,260
Let me just explain roughly what this is thinking is you can think about most of the internal

650
00:38:56,260 --> 00:39:00,820
maps in control systems as mapping from a disturbance, which consists of your initial

651
00:39:00,820 --> 00:39:06,020
state and the actual disturbance signal to either the state.

652
00:39:07,140 --> 00:39:10,740
Controller also is just a map from the disturbance to the control.

653
00:39:11,620 --> 00:39:15,380
And system level synthesis works directly with those maps, the map that maps disturbance

654
00:39:15,380 --> 00:39:17,620
to state, the one that maps disturbance to control.

655
00:39:18,900 --> 00:39:26,020
And just gives you a very straightforward linear algebraic approach to doing

656
00:39:26,020 --> 00:39:30,500
control synthesis where you're working with these abstract maps rather than working with

657
00:39:32,740 --> 00:39:35,940
the actual parameters A, B, and C that you would have in a linear system.

658
00:39:35,940 --> 00:39:37,140
Now, why is that better?

659
00:39:37,700 --> 00:39:42,100
It's just this, in effect, lifts everything into a higher dimensional space where everything

660
00:39:42,100 --> 00:39:43,140
becomes nice.

661
00:39:43,140 --> 00:39:47,700
Problems that weren't convex become convex, which is why we find it really useful.

662
00:39:48,900 --> 00:39:50,580
Why is this the right mapping?

663
00:39:50,580 --> 00:39:51,060
I don't know.

664
00:39:52,180 --> 00:39:52,900
I don't know.

665
00:39:52,980 --> 00:39:58,660
I think another way to think about it, if you're working in finite time, is that the

666
00:39:58,660 --> 00:40:02,100
map from disturbance to state is going to be something like Toplitz matrix with a bunch

667
00:40:02,100 --> 00:40:03,140
of parameters in it.

668
00:40:03,140 --> 00:40:06,260
And you just have to understand that those parameters can't be arbitrary.

669
00:40:06,260 --> 00:40:08,260
They're going to have some relationship.

670
00:40:08,260 --> 00:40:10,100
Essentially a finite impulse response.

671
00:40:10,100 --> 00:40:10,420
Yeah.

672
00:40:10,420 --> 00:40:15,140
And so it's working with those impulse responses and putting these things together in a very

673
00:40:15,140 --> 00:40:15,940
clean fashion.

674
00:40:15,940 --> 00:40:18,260
So it's surprisingly powerful.

675
00:40:18,260 --> 00:40:20,900
And these ideas do appear in other places.

676
00:40:20,900 --> 00:40:25,220
I believe in model predictive control, this is called disturbance feedback.

677
00:40:25,220 --> 00:40:26,260
I think that's the name.

678
00:40:26,260 --> 00:40:31,380
There's definitely something where you operate with this map that maps the error disturbance

679
00:40:31,380 --> 00:40:34,100
into your state and the error disturbance into your controller.

680
00:40:34,100 --> 00:40:38,500
So yeah, that's the high level.

681
00:40:38,500 --> 00:40:41,620
I always feel like it's almost the thing that's funny about it.

682
00:40:41,620 --> 00:40:42,180
I love it.

683
00:40:42,180 --> 00:40:43,940
I think it's very easy to work with.

684
00:40:43,940 --> 00:40:45,940
I could probably teach people how to operate with it.

685
00:40:46,500 --> 00:40:51,460
But it kind of led to the second thing, which came a little bit later, which was work by

686
00:40:51,460 --> 00:40:57,540
Jorge Amania and Steven Tu, which I think it was kind of, it's not surprising, but this

687
00:40:57,540 --> 00:41:00,980
is one of these wonderful things where theory and practice collide.

688
00:41:00,980 --> 00:41:05,220
So what we saw with the SLS is that when you have very little data, you did need to have

689
00:41:05,220 --> 00:41:09,460
some kind of quantification of the uncertainty in order to get stable performance.

690
00:41:10,260 --> 00:41:14,420
But as you start to just get a reasonable amount of data, not too much, just a reasonable

691
00:41:14,420 --> 00:41:19,540
amount of data, what you could just do is fit the model using least squares and then

692
00:41:19,540 --> 00:41:25,140
plug the model in as if it was true and run your optimal controller with this estimated

693
00:41:25,140 --> 00:41:27,140
model as if the estimated model was true.

694
00:41:27,780 --> 00:41:28,020
Right?

695
00:41:28,020 --> 00:41:33,620
So this is the, whenever I say, as if the estimated model was true, or as if the estimated

696
00:41:33,620 --> 00:41:36,580
X was true, we tend to call that certainty equivalence, right?

697
00:41:36,580 --> 00:41:39,780
It's assuming a certain level of uncertainty is not there.

698
00:41:39,780 --> 00:41:41,140
This is the certainty equivalent model.

699
00:41:41,140 --> 00:41:42,980
Assuming there was no noise, this would be what the-

700
00:41:42,980 --> 00:41:45,140
Optimism in the face of uncertainty.

701
00:41:45,140 --> 00:41:47,780
It's a form of, well, that's interesting.

702
00:41:48,500 --> 00:41:51,860
Optimism in the face of uncertainty is a little bit more aggressive than certainty equivalence.

703
00:41:53,060 --> 00:41:54,660
Optimism in the face of uncertainty.

704
00:41:54,660 --> 00:41:56,340
So essentially these three things are different.

705
00:41:56,340 --> 00:42:00,660
So the robust control view is you assume you have data, there's going to be a bunch of

706
00:42:00,660 --> 00:42:02,420
models consistent with your data.

707
00:42:02,420 --> 00:42:06,020
And you're saying, I'm going to pick the one, the robust control view is I'm going to pick

708
00:42:06,020 --> 00:42:10,020
the model that's consistent with my data, but would give me the worst performance.

709
00:42:10,020 --> 00:42:11,380
That's robust control.

710
00:42:11,380 --> 00:42:13,140
Very pessimistic, right?

711
00:42:13,140 --> 00:42:16,020
And they're going to say, well, given that, then what, how do we do design?

712
00:42:17,140 --> 00:42:19,780
Certain equivalence just says, I'm going to ignore the uncertainty.

713
00:42:20,340 --> 00:42:21,380
Just pretend it's not there.

714
00:42:21,380 --> 00:42:22,100
And then what would I do?

715
00:42:23,460 --> 00:42:27,300
Optimism in the face of uncertainty says, given all of the models that are consistent

716
00:42:27,300 --> 00:42:30,340
with my data, I'm going to pick the one that gives me the best performance.

717
00:42:31,140 --> 00:42:31,380
Okay.

718
00:42:31,940 --> 00:42:33,540
Which is incredibly aggressive.

719
00:42:33,540 --> 00:42:38,580
And the idea there is you, you, you, you know, either you're right, in which case, great,

720
00:42:39,220 --> 00:42:40,260
or you're wrong.

721
00:42:40,260 --> 00:42:41,380
And you learn that quickly.

722
00:42:41,380 --> 00:42:44,980
That's a nice way to put it.

723
00:42:44,980 --> 00:42:46,340
Yeah, that's the idea.

724
00:42:46,340 --> 00:42:51,140
But also, you tell that to a control engineer, and they get a little terrified, I think.

725
00:42:51,140 --> 00:42:53,860
They tend to be quite conservative.

726
00:42:53,860 --> 00:42:54,580
Yeah, yeah, yeah.

727
00:42:54,580 --> 00:42:56,180
Nobody wants to, you don't want to fail quickly.

728
00:42:56,180 --> 00:42:56,980
That doesn't seem good.

729
00:42:58,420 --> 00:43:01,380
But so just to go back to the certainty equivalence very quickly.

730
00:43:01,380 --> 00:43:01,780
Yeah, sure.

731
00:43:01,780 --> 00:43:02,340
Absolutely.

732
00:43:02,340 --> 00:43:03,700
So what did we do there?

733
00:43:04,420 --> 00:43:10,420
So this idea where you just, you know, fit some ARMA model to your data, and then pretend

734
00:43:10,420 --> 00:43:15,460
if it was true, it's kind of a staple of engineering, right?

735
00:43:15,460 --> 00:43:18,260
I mean, people do that all the time in control engineering.

736
00:43:18,260 --> 00:43:23,140
And what Jorge and Stephen showed, which is remarkable, is that once you have enough data,

737
00:43:23,140 --> 00:43:24,420
that is the optimal thing to do.

738
00:43:25,140 --> 00:43:27,700
Not only is it a good thing to do, it's the optimal thing to do.

739
00:43:28,420 --> 00:43:30,020
And in fact, that does suggest something.

740
00:43:30,820 --> 00:43:35,940
The first clause I said was, if you have enough data, what it suggests is in the lab, do as

741
00:43:35,940 --> 00:43:37,060
many experiments as you can.

742
00:43:37,780 --> 00:43:42,420
Always be sure that you have enough data, because you're almost never in a lab setting

743
00:43:42,420 --> 00:43:43,220
resource constraint.

744
00:43:43,220 --> 00:43:45,220
So collect as much data as you can.

745
00:43:45,860 --> 00:43:50,260
And then once you do that, you can deploy this model, you could ignore the uncertainty,

746
00:43:50,260 --> 00:43:50,980
and you'll be fine.

747
00:43:51,780 --> 00:43:52,980
It's kind of amazing.

748
00:43:52,980 --> 00:43:57,860
And I do think that basically, I mean, I don't want to, again, don't want to step on any

749
00:43:57,860 --> 00:44:02,980
engineer's toes, but so many engineers who are working on mission critical systems just

750
00:44:02,980 --> 00:44:03,540
do that.

751
00:44:03,540 --> 00:44:04,660
And it's okay.

752
00:44:04,660 --> 00:44:08,980
And I think it's interesting to justify 50 years of practice with a theorem.

753
00:44:08,980 --> 00:44:09,460
It's nice.

754
00:44:10,100 --> 00:44:10,420
Yeah.

755
00:44:10,420 --> 00:44:14,340
Plus the fact that we somehow flew to the moon in the 70s or 60s.

756
00:44:14,340 --> 00:44:14,840
Right?

757
00:44:16,100 --> 00:44:17,220
No, we used feedback.

758
00:44:17,220 --> 00:44:18,740
And so the feedback was important.

759
00:44:18,740 --> 00:44:22,980
But I think it's just kind of understanding how sensitive we really are to those kind

760
00:44:22,980 --> 00:44:23,860
of modeling errors.

761
00:44:24,820 --> 00:44:29,860
And it's I think what this is always one thing that's amazing and fascinating control is

762
00:44:30,740 --> 00:44:34,900
you can interpret the robust control mindset is assuming we're going to be sensitive to

763
00:44:34,900 --> 00:44:35,400
everything.

764
00:44:36,260 --> 00:44:37,620
But I don't think that's fair.

765
00:44:37,620 --> 00:44:42,340
I think robust control at its best is just telling you what you should be looking out

766
00:44:42,340 --> 00:44:42,840
for.

767
00:44:43,940 --> 00:44:44,260
Right?

768
00:44:44,260 --> 00:44:46,740
It's not saying don't don't do anything.

769
00:44:46,740 --> 00:44:49,780
That's the most robust, most robust thing is just don't.

770
00:44:50,340 --> 00:44:54,660
But it's just saying these are the these are some pitfalls that you might not see.

771
00:44:54,660 --> 00:44:56,900
And can we identify where those pitfalls pitfalls are?

772
00:44:57,540 --> 00:45:04,820
One thing that I wanted to talk about is perhaps your outsider's view of control and also

773
00:45:04,820 --> 00:45:05,540
of machine learning.

774
00:45:05,540 --> 00:45:08,260
You've been bouncing back and forth between all these fields.

775
00:45:08,260 --> 00:45:11,060
And I think this is one of your great strengths.

776
00:45:11,060 --> 00:45:15,700
So I was wondering whether you see something that control people should be excited about

777
00:45:16,420 --> 00:45:17,140
for the future.

778
00:45:17,780 --> 00:45:20,340
And in general, what's the future of all these fields?

779
00:45:20,340 --> 00:45:22,900
How will they interact?

780
00:45:23,780 --> 00:45:24,740
Yeah, that's a great question.

781
00:45:26,020 --> 00:45:27,780
That's a complicated question.

782
00:45:27,780 --> 00:45:30,020
What should control theory folks be excited about?

783
00:45:33,140 --> 00:45:39,220
The work we did in reinforcement learning, I think, ended up spanning about five years

784
00:45:39,220 --> 00:45:41,220
of pretty dedicated work.

785
00:45:41,220 --> 00:45:46,100
I think I'm fairly certain it was the span of Sarah Dean's PhD.

786
00:45:46,500 --> 00:45:48,260
We started when Sarah arrived.

787
00:45:48,260 --> 00:45:52,020
And I'm pretty sure that maybe it was one year before, but it may have been right when

788
00:45:52,020 --> 00:45:52,980
she arrived.

789
00:45:52,980 --> 00:45:55,780
And then she graduated last year.

790
00:45:55,780 --> 00:45:59,860
And so the group currently is working on very different projects.

791
00:46:00,980 --> 00:46:07,540
And I think a lot of times in research, that's actually a good horizon for a research agenda

792
00:46:07,540 --> 00:46:08,820
is the length of a PhD.

793
00:46:08,820 --> 00:46:13,540
So I mean, Sarah has done so much amazing stuff in that space.

794
00:46:13,700 --> 00:46:18,420
I think she will continue to do so as faculty.

795
00:46:18,420 --> 00:46:24,660
But I think that so that I have currently now disengaged a bit from control, which I

796
00:46:24,660 --> 00:46:25,300
think is okay.

797
00:46:26,980 --> 00:46:31,060
And I don't want to ever tell control theory folks or anybody really what they should be

798
00:46:31,060 --> 00:46:31,780
working on.

799
00:46:31,780 --> 00:46:33,380
I think that's always hard.

800
00:46:33,380 --> 00:46:39,140
But we do benefit from the influx of new ideas and interact with our neighbors.

801
00:46:39,140 --> 00:46:41,620
So we're just very curious about what you're up to.

802
00:46:42,420 --> 00:46:44,660
Yeah, well, I can tell you that's what I was going to move to.

803
00:46:44,660 --> 00:46:46,340
I could tell you what I'm interested in.

804
00:46:46,340 --> 00:46:49,300
I'm not sure that everybody control is yet going to be interested in.

805
00:46:49,300 --> 00:46:53,700
But I think that that's, you know, I think, well, you know, I guess this was just to say,

806
00:46:53,700 --> 00:46:58,100
I'm sure I'm going to come back and write control theory papers again, just taking a

807
00:46:58,100 --> 00:46:59,540
couple years off right now.

808
00:46:59,540 --> 00:47:00,500
And we'll see where it goes.

809
00:47:01,380 --> 00:47:08,340
So what are we like the things I think we're very interested in in our group are precisely

810
00:47:08,340 --> 00:47:17,220
these issues of external validation, understanding how to deal with predictive systems outside

811
00:47:17,220 --> 00:47:23,460
of the laboratory setting, and also kind of understanding how you can, you know, refine

812
00:47:23,460 --> 00:47:30,500
these predictive systems over time, which, to be fair, that has a bit of a control theory

813
00:47:30,500 --> 00:47:31,700
sort of view, right?

814
00:47:31,700 --> 00:47:33,540
There's a feedback process that has to happen.

815
00:47:34,340 --> 00:47:39,460
I think machine learning, the way it's commonly taught is a very static process.

816
00:47:39,460 --> 00:47:43,140
There's data that's static, you apply it to a test set that's static.

817
00:47:43,140 --> 00:47:46,100
And you can run as many times as you want, but nothing changes.

818
00:47:46,100 --> 00:47:48,740
And the assumption is that the new data looks like the old data.

819
00:47:48,740 --> 00:47:49,940
So there's no dynamics.

820
00:47:50,580 --> 00:47:55,460
But how you run, you use machine learning, or just statistical thinking in a dynamic

821
00:47:55,460 --> 00:47:57,540
world is what we're very interested in.

822
00:47:57,540 --> 00:48:00,500
So in some sense, that could touch on control.

823
00:48:01,300 --> 00:48:02,020
Absolutely.

824
00:48:02,020 --> 00:48:05,460
And the issue of causality, I suppose, is rather causal thinking.

825
00:48:06,020 --> 00:48:07,060
Yes, absolutely.

826
00:48:07,060 --> 00:48:10,420
And I was just going to say, though, I think one thing that's challenging for control theorists

827
00:48:10,420 --> 00:48:13,060
is that not everything in dynamics is control theory.

828
00:48:13,780 --> 00:48:15,380
And I think this is always a tricky part.

829
00:48:15,380 --> 00:48:18,100
I think that sometimes we tend to be touchy.

830
00:48:18,100 --> 00:48:19,380
Yeah, we tend to be touchy.

831
00:48:19,380 --> 00:48:22,100
We tend to think that everything that's dynamic means that it's control.

832
00:48:22,100 --> 00:48:28,180
But I feel like, you know, control theorists, or control theory in general, I think should

833
00:48:29,140 --> 00:48:32,340
not oversell its reach, but it also shouldn't undersell.

834
00:48:33,220 --> 00:48:39,380
I think so many profound developments of the 20th century are due to our understanding

835
00:48:39,380 --> 00:48:42,020
of control, and what exactly that is.

836
00:48:42,020 --> 00:48:50,660
And whether that be, you know, frequency domain models of LTI systems, you know, that's powerful

837
00:48:50,660 --> 00:48:50,900
stuff.

838
00:48:50,900 --> 00:48:51,860
Don't sell that short.

839
00:48:52,420 --> 00:48:54,660
That obviously can't tell you about everything.

840
00:48:54,660 --> 00:49:00,980
But it tells you a lot, you know, in a similar way, you know, understanding optimal control

841
00:49:00,980 --> 00:49:01,700
tells you a lot.

842
00:49:01,700 --> 00:49:05,540
But not everything can be posed in that way, even if it's dynamic.

843
00:49:05,540 --> 00:49:07,940
And I think that that's some of the questions we're looking at now.

844
00:49:07,940 --> 00:49:09,460
We're just trying to figure out how do you pose them?

845
00:49:09,460 --> 00:49:11,060
What's the right way to think about them?

846
00:49:11,060 --> 00:49:15,380
And this is what your new series of blog posts, I suppose, will be all about.

847
00:49:15,940 --> 00:49:16,420
Is that right?

848
00:49:16,420 --> 00:49:17,220
We're trying.

849
00:49:17,220 --> 00:49:18,100
That's right.

850
00:49:18,100 --> 00:49:18,740
That's right.

851
00:49:18,740 --> 00:49:19,700
And it's so tricky.

852
00:49:19,700 --> 00:49:24,180
It's so tricky, especially because some of the applications we're interested in involve,

853
00:49:24,180 --> 00:49:26,500
you know, social systems and people.

854
00:49:27,540 --> 00:49:31,540
You know, I think that one of the things that a lot of the folks in the group are interested

855
00:49:31,540 --> 00:49:37,540
in are, you know, where computation and healthcare end up intersecting each other.

856
00:49:38,740 --> 00:49:43,220
And so that could be in the space of clinical trials, that could be in the space of, you

857
00:49:43,220 --> 00:49:50,500
know, just how, I mean, it could be in the space of protocols for just how we train people

858
00:49:50,500 --> 00:49:53,780
to interact and treat and care for their patients.

859
00:49:54,500 --> 00:49:58,900
And it could also be in the context of directly where machine learning just gets deployed

860
00:49:58,900 --> 00:49:59,860
in a healthcare setting.

861
00:49:59,860 --> 00:50:01,060
And how do you think about that?

862
00:50:01,620 --> 00:50:05,940
So these are three things that end up being a little step removed from what I was doing.

863
00:50:05,940 --> 00:50:09,380
It's interesting that they're a little bit focused on healthcare.

864
00:50:09,380 --> 00:50:12,820
But, you know, certainly there are multiple reasons for that.

865
00:50:12,820 --> 00:50:16,900
I think, one, I come from a family of healthcare providers.

866
00:50:16,900 --> 00:50:21,700
So I was a big disappointment because I didn't become a medical doctor.

867
00:50:22,340 --> 00:50:26,340
So hopefully I'll make my parents a little bit less regretful.

868
00:50:26,340 --> 00:50:26,660
You'll make up for this, I guess.

869
00:50:26,660 --> 00:50:28,580
Yeah, I've got to make up for that for my parents.

870
00:50:28,580 --> 00:50:34,660
No, you absolutely are, let's say, on the point in the sense that you're anticipating

871
00:50:34,660 --> 00:50:39,860
a question that I would have asked you from our audience, essentially about technologies

872
00:50:39,860 --> 00:50:44,420
and ideas to follow in the next one to five years, or one to 10 years, if you will.

873
00:50:44,420 --> 00:50:45,220
Oh, interesting.

874
00:50:45,220 --> 00:50:49,940
And I guess then healthcare and healthcare applications is definitely one of them for

875
00:50:49,940 --> 00:50:50,420
you.

876
00:50:50,420 --> 00:50:52,100
I think healthcare is a huge one.

877
00:50:52,100 --> 00:50:52,340
Yeah.

878
00:50:52,340 --> 00:50:57,620
Well, I was going to say that I've written a few papers on healthcare before.

879
00:50:57,620 --> 00:51:00,340
It's always been a little bit of a back burner to interest.

880
00:51:00,340 --> 00:51:06,420
But I think, you know, I can't lie that the pandemic kind of really got me way back into

881
00:51:06,420 --> 00:51:06,740
it.

882
00:51:06,740 --> 00:51:12,580
I think there's so much, you know, the medical community for the first time ever embraced

883
00:51:12,580 --> 00:51:17,060
the preprint server and all of its warts and all of its blessings, right?

884
00:51:17,060 --> 00:51:18,660
I mean, we could read all of these papers.

885
00:51:18,660 --> 00:51:20,340
You're up front against all these papers.

886
00:51:20,340 --> 00:51:23,300
You really start to see how evidence is gathered.

887
00:51:24,020 --> 00:51:25,220
It was an extreme setting.

888
00:51:25,220 --> 00:51:26,260
It was in a crisis setting.

889
00:51:26,260 --> 00:51:29,940
So I don't think it's completely aligned with normal times.

890
00:51:29,940 --> 00:51:33,300
But then you start to dig into normal times and you're like, it's not that far off.

891
00:51:33,300 --> 00:51:36,020
And sometimes their decision making is good and sometimes it's bad.

892
00:51:37,140 --> 00:51:40,340
And I think the other thing that happened, which makes it, you know, I think an interesting

893
00:51:40,340 --> 00:51:46,260
time is people are still pushing very hard to move algorithmic thinking into healthcare.

894
00:51:46,260 --> 00:51:49,140
And it's very messy.

895
00:51:49,140 --> 00:51:52,980
It's not cut and dry as to whether or not these things are going to be valuable.

896
00:51:54,340 --> 00:51:57,380
I think they could be, but I think we want to be careful and we want to think about how

897
00:51:57,380 --> 00:51:57,860
to do that.

898
00:51:57,860 --> 00:52:03,140
So these are the three main reasons why I think that healthcare is going to be a fun

899
00:52:03,140 --> 00:52:04,900
thing to be looking at moving forward.

900
00:52:05,780 --> 00:52:12,660
I have a couple of other questions and then maybe we can move to a bit more of a geeky

901
00:52:12,660 --> 00:52:13,940
territory if you want.

902
00:52:13,940 --> 00:52:15,140
Okay, sounds good.

903
00:52:16,180 --> 00:52:22,740
So one question that I was interested in asking you is about the biological origin of learning.

904
00:52:22,740 --> 00:52:26,740
I was wondering whether you ever got interested in that topic.

905
00:52:26,740 --> 00:52:32,020
And I was listening just the other day to another podcast from Google DeepMind, listening

906
00:52:32,020 --> 00:52:37,860
to Raya Hadsall speaking about essentially the origin of intelligence and the fact that

907
00:52:37,860 --> 00:52:43,700
it's very much related to motion and, you know, bacteria when they first evolved, they

908
00:52:43,700 --> 00:52:49,300
had a competitive advantage in moving towards food or other things, I suppose.

909
00:52:50,340 --> 00:52:54,820
So I wonder whether you ever got interested in this area, if you want, in the biological

910
00:52:54,820 --> 00:52:56,820
connections with learning.

911
00:52:58,500 --> 00:53:00,740
So I didn't, and I'm trying to think about why.

912
00:53:01,460 --> 00:53:03,540
I have a couple answers to why I never engaged.

913
00:53:03,540 --> 00:53:07,460
I think, you know, when I was in graduate school, artificial intelligence was a very

914
00:53:07,460 --> 00:53:08,020
dirty word.

915
00:53:08,820 --> 00:53:10,500
And I still kind of feel that way.

916
00:53:11,620 --> 00:53:12,340
I think historically...

917
00:53:12,340 --> 00:53:13,300
Why do you say so?

918
00:53:13,300 --> 00:53:14,100
I'm curious.

919
00:53:14,100 --> 00:53:17,860
I just think at the time when they were deep in a winter and nobody was doing AI, I think

920
00:53:17,860 --> 00:53:21,860
it's really funny that all the people who are doing AI now are just doing machine learning

921
00:53:21,860 --> 00:53:23,220
and they just kind of stole it.

922
00:53:23,220 --> 00:53:26,740
I don't really understand why that was allowed to happen.

923
00:53:26,740 --> 00:53:29,700
I think a lot of the AI in the early 20th century...

924
00:53:29,700 --> 00:53:31,780
Honestly, AI, sorry, 21st century.

925
00:53:32,500 --> 00:53:40,020
I think a lot of AI in general is just a kind of bizarre fixation that like never quite

926
00:53:40,020 --> 00:53:41,860
makes sense to me.

927
00:53:42,420 --> 00:53:43,780
Other people get into it.

928
00:53:43,780 --> 00:53:48,340
But if we look historically again, because that's all I do these days, the foundations

929
00:53:48,340 --> 00:53:54,900
of AI and the foundations of predictive machine learning, just predictive modeling, they also

930
00:53:54,900 --> 00:53:57,460
arise at the same time in different communities.

931
00:53:57,460 --> 00:54:04,340
There was a big push by the group of scientists to kind of brand something that was how the

932
00:54:04,340 --> 00:54:07,540
brain works and we're going to build machines that mimic how the brain works.

933
00:54:08,100 --> 00:54:13,780
And I think if you read the preface of cybernetics by Wiener, I think he also was really keen

934
00:54:13,780 --> 00:54:21,060
into understanding some kind of building systems that act like people or that mimic people.

935
00:54:21,620 --> 00:54:27,380
I think one of the fascinating reflections that Wiener had was that by the 60s, he could

936
00:54:27,380 --> 00:54:28,420
consider this done.

937
00:54:28,980 --> 00:54:33,540
And what he says, which I think is, if you go again and read the preface to the second

938
00:54:33,540 --> 00:54:40,180
edition of cybernetics, what he says is that while we learned a lot about psychology and

939
00:54:40,180 --> 00:54:44,180
trying to mimic things, really what stood out, the big abstractions that ended up standing

940
00:54:44,180 --> 00:54:45,860
out were things like feedback control.

941
00:54:46,580 --> 00:54:50,020
And feedback control, we can abstract away and then there's no personification.

942
00:54:50,580 --> 00:54:56,500
Feedback control, signal processing, detection and estimation, these are all things that

943
00:54:56,500 --> 00:54:59,060
didn't exist when Wiener wrote cybernetics the first time.

944
00:54:59,060 --> 00:55:03,620
And by the time he wrote the second edition, they were now canon of engineering and it's

945
00:55:03,620 --> 00:55:04,420
amazing.

946
00:55:04,420 --> 00:55:10,420
And those are deep, like deep mathematical and applied concepts that we use all the time.

947
00:55:11,140 --> 00:55:15,620
And I think that that's that big distinction between the AI school and what I would call

948
00:55:16,260 --> 00:55:17,620
whatever school I'm in.

949
00:55:17,620 --> 00:55:23,780
I do machine learning, but again, I'm looking back to what the 60s, we just called that

950
00:55:23,780 --> 00:55:24,820
pattern recognition.

951
00:55:24,820 --> 00:55:26,900
And I'm much happier with that term, honestly.

952
00:55:26,900 --> 00:55:32,820
So pattern recognition, control, detection and estimation, signal processing, these are

953
00:55:32,820 --> 00:55:35,220
the cores of engineering mathematics.

954
00:55:35,860 --> 00:55:42,340
And I think the powerful thing that the electrical engineers did, that the artificial

955
00:55:42,340 --> 00:55:46,180
intelligence people didn't, is they realized that the abstractions themselves are useful

956
00:55:46,740 --> 00:55:50,660
and they built all of the information technology of the late 20th century out of this.

957
00:55:51,300 --> 00:55:57,380
And I think people, the AI folks, I'm not sure what exactly, I don't know, now I'm going

958
00:55:57,380 --> 00:55:58,020
to get in trouble.

959
00:56:00,820 --> 00:56:02,340
Yeah, sure, let me get myself in trouble.

960
00:56:02,340 --> 00:56:07,460
What can we point to that are the technical artifacts of AI, really, as opposed to the

961
00:56:07,460 --> 00:56:10,580
technological artifacts of this other cybernetical school?

962
00:56:11,140 --> 00:56:12,820
Again, I don't want to give Wiener all the credit.

963
00:56:12,820 --> 00:56:16,100
It's just that his reflection, I think, was really nice.

964
00:56:16,100 --> 00:56:20,820
And it put in my mind an understanding of why these things split so hard.

965
00:56:22,500 --> 00:56:27,140
When you start post-World War II, there's no foundations of electrical engineering.

966
00:56:27,140 --> 00:56:29,060
I mean, we knew about feedback.

967
00:56:29,060 --> 00:56:31,460
We knew about detection estimation.

968
00:56:31,460 --> 00:56:37,300
We built a lot of that stuff into, unfortunately, the technology of war.

969
00:56:37,860 --> 00:56:42,500
But solidifying it into a real discipline happened in the subsequent time after the

970
00:56:42,500 --> 00:56:43,300
war.

971
00:56:43,300 --> 00:56:50,580
And it led to, I mean, just compare 1959 to 2022, right?

972
00:56:50,580 --> 00:56:56,980
And all of the things that went into that with feedback control, with signal processing,

973
00:56:56,980 --> 00:56:59,460
detection estimation, it's just mind-blowing.

974
00:57:00,100 --> 00:57:06,820
Yeah, I mean, maybe my personal trajectory also biases here the analysis, but I kind

975
00:57:06,820 --> 00:57:14,500
of see that somehow we are approaching a sort of renaissance of cybernetics in a way.

976
00:57:14,500 --> 00:57:19,300
I mean, I see a lot of interest in the brain and neuroscience as well.

977
00:57:19,300 --> 00:57:26,100
Neuralink is building these new chips that are probably going to read the signals in

978
00:57:26,100 --> 00:57:29,140
our brains incredibly fast and maybe act on them.

979
00:57:29,140 --> 00:57:29,940
We don't know.

980
00:57:29,940 --> 00:57:30,500
I hope not.

981
00:57:31,060 --> 00:57:37,460
And, you know, you have all these other buzzwords like synthetic biology and even, you know,

982
00:57:37,460 --> 00:57:40,740
molds that solve optimization problems.

983
00:57:40,740 --> 00:57:43,060
So I wonder whether that will play a role.

984
00:57:43,060 --> 00:57:46,340
But I guess this is just an open question to tease you.

985
00:57:46,340 --> 00:57:47,300
We don't know.

986
00:57:47,300 --> 00:57:48,020
We'll find out.

987
00:57:48,020 --> 00:57:48,660
We'll find out.

988
00:57:48,660 --> 00:57:50,180
I think it is interesting.

989
00:57:50,180 --> 00:57:53,940
I think that the, to me, I think that the thing that's funny, though, is that this

990
00:57:54,580 --> 00:57:58,580
overloading of the word learning, I think, is the part that's actually a little suspect

991
00:57:58,580 --> 00:57:59,300
to me.

992
00:57:59,300 --> 00:58:03,300
It took me until about 2016 to admit that I did machine learning.

993
00:58:03,300 --> 00:58:05,780
I always told people I do statistics and optimization.

994
00:58:05,780 --> 00:58:09,860
I just would refuse to engage with the name because it's always struck me as odd.

995
00:58:09,860 --> 00:58:14,260
And honestly, I think it was useful to just say it's pattern recognition, it's pattern

996
00:58:14,260 --> 00:58:15,220
classification.

997
00:58:15,220 --> 00:58:18,100
Those are just, that's just much, it's kind of boring.

998
00:58:18,100 --> 00:58:20,820
Pattern classification is super boring, but it's honest.

999
00:58:21,460 --> 00:58:22,020
And useful.

1000
00:58:23,060 --> 00:58:23,860
And useful, yeah.

1001
00:58:24,500 --> 00:58:28,340
OK, so maybe it's a good time now to move on, shift gears again.

1002
00:58:28,340 --> 00:58:29,860
We're going to play a little game.

1003
00:58:29,860 --> 00:58:30,660
Oh boy.

1004
00:58:30,660 --> 00:58:32,180
The rules, yes.

1005
00:58:33,140 --> 00:58:34,980
So the rules are simple.

1006
00:58:34,980 --> 00:58:37,460
I'm going to ask you a this or that question.

1007
00:58:38,420 --> 00:58:40,900
You need to answer in at most two seconds.

1008
00:58:41,460 --> 00:58:44,420
And I really want to see system one playing.

1009
00:58:44,420 --> 00:58:49,620
So, you know, in Kahneman's, Daniel Kahneman's thinking fast and slow language.

1010
00:58:49,620 --> 00:58:50,980
OK, we're going to go fast.

1011
00:58:50,980 --> 00:58:51,620
All right.

1012
00:58:51,620 --> 00:58:52,580
We're going to go fast.

1013
00:58:52,580 --> 00:58:56,180
So every answer that you give after three seconds is not valid.

1014
00:58:56,180 --> 00:58:56,580
OK.

1015
00:58:56,580 --> 00:58:59,780
And I'm going to just shoot at you a lot of questions.

1016
00:58:59,780 --> 00:59:00,180
OK.

1017
00:59:00,180 --> 00:59:02,020
Tell me when you're ready, we can start.

1018
00:59:03,060 --> 00:59:03,540
I'm ready.

1019
00:59:03,540 --> 00:59:03,940
Let's go.

1020
00:59:04,500 --> 00:59:07,220
OK, so working hard or hardly working?

1021
00:59:07,220 --> 00:59:07,860
Working hard.

1022
00:59:08,420 --> 00:59:09,620
Robots or dinosaurs?

1023
00:59:10,420 --> 00:59:11,060
Dinosaurs.

1024
00:59:11,700 --> 00:59:13,220
Success or happiness?

1025
00:59:13,220 --> 00:59:14,100
Happiness.

1026
00:59:14,100 --> 00:59:15,780
Growth or security?

1027
00:59:15,780 --> 00:59:16,580
Oh, growth.

1028
00:59:17,220 --> 00:59:17,860
I don't like either.

1029
00:59:18,420 --> 00:59:19,860
Guacamole or salsa?

1030
00:59:20,740 --> 00:59:21,460
Salsa.

1031
00:59:21,460 --> 00:59:23,220
Loud neighbors or nosy neighbors?

1032
00:59:24,180 --> 00:59:24,980
Nosy neighbors.

1033
00:59:24,980 --> 00:59:26,660
Pineapple pizza or candy corn?

1034
00:59:27,220 --> 00:59:27,860
Oh, God, no.

1035
00:59:34,740 --> 00:59:35,240
Can't do it.

1036
00:59:36,340 --> 00:59:38,580
Test the waters or dive in the deep end?

1037
00:59:40,100 --> 00:59:42,100
Test the waters, but that's a swimming issue.

1038
00:59:43,620 --> 00:59:45,300
OK, we're going to go back to that later.

1039
00:59:45,300 --> 00:59:46,100
Sure.

1040
00:59:46,100 --> 00:59:47,700
Logic or emotion?

1041
00:59:47,700 --> 00:59:48,660
Emotion.

1042
00:59:48,660 --> 00:59:50,260
Zombies or vampires?

1043
00:59:50,260 --> 00:59:51,140
Vampires.

1044
00:59:51,140 --> 00:59:52,180
Nyquist or Kalman?

1045
00:59:52,980 --> 00:59:53,940
Nyquist.

1046
00:59:53,940 --> 00:59:56,580
Glass half full or glass half empty?

1047
00:59:56,580 --> 00:59:57,460
Why can't it be both?

1048
00:59:58,660 --> 01:00:01,140
Couch potato or fitness fiend?

1049
01:00:01,140 --> 01:00:02,340
Fitness fiend.

1050
01:00:02,340 --> 01:00:03,540
Money or love?

1051
01:00:03,540 --> 01:00:04,040
Love.

1052
01:00:04,500 --> 01:00:06,340
Bayesian or frequentist?

1053
01:00:06,340 --> 01:00:07,300
No, that one I won't answer.

1054
01:00:09,620 --> 01:00:11,300
I have a story for that one, too, if you want.

1055
01:00:11,300 --> 01:00:12,020
OK.

1056
01:00:12,020 --> 01:00:12,520
No.

1057
01:00:13,620 --> 01:00:15,700
We're going to go back to that as well.

1058
01:00:16,260 --> 01:00:19,220
So see the future or change the past?

1059
01:00:19,220 --> 01:00:22,260
Whoa, those are like the same, aren't they?

1060
01:00:23,220 --> 01:00:24,260
Those are the same.

1061
01:00:25,620 --> 01:00:27,540
Time machine or magic wand?

1062
01:00:27,540 --> 01:00:28,500
Magic wand.

1063
01:00:28,500 --> 01:00:29,780
Vacation or staycation?

1064
01:00:30,340 --> 01:00:31,380
Vacation.

1065
01:00:31,380 --> 01:00:32,980
LQR or GPT-3?

1066
01:00:34,100 --> 01:00:34,900
LQR.

1067
01:00:34,900 --> 01:00:36,180
Planning or winging it?

1068
01:00:36,740 --> 01:00:37,380
Depends.

1069
01:00:37,380 --> 01:00:38,820
I can't answer that one either.

1070
01:00:38,820 --> 01:00:39,460
That was too hard.

1071
01:00:40,820 --> 01:00:43,060
Dynamic programming or divide and conquer?

1072
01:00:43,940 --> 01:00:45,300
Dynamic programming.

1073
01:00:45,300 --> 01:00:46,980
Skill or popularity?

1074
01:00:46,980 --> 01:00:47,780
Skill.

1075
01:00:47,780 --> 01:00:49,380
Deep mind or Boston Dynamics?

1076
01:00:50,020 --> 01:00:51,060
Boston Dynamics.

1077
01:00:51,060 --> 01:00:53,060
Poor and happy or rich and miserable?

1078
01:00:54,420 --> 01:00:55,060
Poor and happy.

1079
01:00:56,100 --> 01:00:56,740
That was too easy.

1080
01:00:58,820 --> 01:01:00,420
Maths or physics?

1081
01:01:00,420 --> 01:01:00,920
Maths.

1082
01:01:01,460 --> 01:01:03,220
Beethoven or Beatles?

1083
01:01:03,220 --> 01:01:03,720
Beatles.

1084
01:01:04,740 --> 01:01:06,580
Speeding ticket or parking ticket?

1085
01:01:08,660 --> 01:01:09,860
I get more parking tickets.

1086
01:01:09,860 --> 01:01:10,580
I don't know what that says.

1087
01:01:11,140 --> 01:01:12,500
Machine learning or control?

1088
01:01:14,340 --> 01:01:16,740
Man, again, that's another one I can't answer.

1089
01:01:16,740 --> 01:01:17,460
This is the last one.

1090
01:01:17,460 --> 01:01:18,500
It's a good closer.

1091
01:01:18,500 --> 01:01:19,540
That's a good closer.

1092
01:01:19,540 --> 01:01:20,500
I can't answer that one.

1093
01:01:21,220 --> 01:01:23,220
We're going to skip.

1094
01:01:23,220 --> 01:01:24,260
What about the waters?

1095
01:01:24,260 --> 01:01:26,260
Why can't we test the waters?

1096
01:01:26,260 --> 01:01:27,780
I'm just not a very good swimmer.

1097
01:01:28,820 --> 01:01:30,500
This is something I do regret a bit.

1098
01:01:30,500 --> 01:01:31,220
I can swim.

1099
01:01:31,220 --> 01:01:31,700
I'm all right.

1100
01:01:32,820 --> 01:01:34,020
Something I should be better at.

1101
01:01:34,020 --> 01:01:35,380
I just need to practice more.

1102
01:01:35,380 --> 01:01:38,660
What about the Bayesian versus frequentist?

1103
01:01:39,700 --> 01:01:42,100
My favorite answer to that question is when someone says,

1104
01:01:42,100 --> 01:01:43,380
what kind of statistician are you?

1105
01:01:43,380 --> 01:01:44,340
It's to say difficult.

1106
01:01:47,700 --> 01:01:49,220
I think that's the best answer.

1107
01:01:49,220 --> 01:01:52,020
The Bayesian-frequentist divide actually is nonsense.

1108
01:01:52,020 --> 01:01:52,740
You want that middle one.

1109
01:01:52,740 --> 01:01:53,240
Difficult.

1110
01:01:53,940 --> 01:01:57,540
And I learned that from Philip Stark, who's in our statistics department.

1111
01:01:57,540 --> 01:01:58,820
Someone was pressuring him on that.

1112
01:01:58,820 --> 01:01:59,380
That's what he said.

1113
01:02:00,340 --> 01:02:02,820
There is an interesting aspect to that, too.

1114
01:02:02,820 --> 01:02:07,220
And I think if you look at the way that statisticians, there are different statisticians.

1115
01:02:07,220 --> 01:02:11,140
And I think there are some who, I think when you say Bayesian or frequentist,

1116
01:02:11,140 --> 01:02:16,660
it kind of imbues a certain kind of faith that one of those two is going to get you to an answer.

1117
01:02:16,660 --> 01:02:21,380
Where I think some of the best things that you could do as a statistician

1118
01:02:21,380 --> 01:02:24,820
is understand just the limits of both ways of thinking.

1119
01:02:25,460 --> 01:02:28,260
That's why difficult, I think, is kind of a good one.

1120
01:02:28,820 --> 01:02:29,220
OK.

1121
01:02:29,220 --> 01:02:35,700
So maybe this is a good time to take all the questions that we got from the audience offline

1122
01:02:35,700 --> 01:02:37,140
and ask you just a few, maybe.

1123
01:02:38,420 --> 01:02:43,860
One that I particularly like is, what is your daily mantra or your favorite quote?

1124
01:02:45,060 --> 01:02:45,940
Oh, man.

1125
01:02:46,900 --> 01:02:47,940
That's a hard one.

1126
01:02:47,940 --> 01:02:48,980
Do I have a daily mantra?

1127
01:02:48,980 --> 01:02:50,980
I probably have one and now I'm just forgetting it.

1128
01:02:52,420 --> 01:02:55,860
Sometimes you get put on the spot and you really got to think like, let me think for a second.

1129
01:02:57,540 --> 01:03:02,820
And also the same person asked, do you have a specific routine in the morning?

1130
01:03:03,460 --> 01:03:06,660
Like, what do you do in the first 60 minutes of your day?

1131
01:03:06,660 --> 01:03:07,220
See that?

1132
01:03:07,220 --> 01:03:07,940
That's interesting.

1133
01:03:07,940 --> 01:03:08,900
That I can answer.

1134
01:03:08,900 --> 01:03:12,100
Oh, the favorite quote thing is always tough because I have so many.

1135
01:03:12,100 --> 01:03:14,820
And it's always contextual as to which one comes up.

1136
01:03:14,820 --> 01:03:19,220
But routine, I am definitely, for better, for worse, a man of routine.

1137
01:03:19,220 --> 01:03:25,060
So like I wake up, my cat will wake me up usually between 530 and six every morning.

1138
01:03:25,060 --> 01:03:28,100
He comes to the door and really makes sure that I'm awake.

1139
01:03:28,100 --> 01:03:28,980
Very important to him.

1140
01:03:29,540 --> 01:03:34,660
So then usually I'll have to feed the cat and then I'll make a coffee.

1141
01:03:34,660 --> 01:03:36,180
Then usually I'll have a small breakfast.

1142
01:03:36,900 --> 01:03:42,500
And often that's when I'll do a lot of Twitter reading, just because if I do it when I'm awake,

1143
01:03:42,500 --> 01:03:44,420
it just is no fun.

1144
01:03:44,420 --> 01:03:46,420
So that's why I usually try to get that out of the day there.

1145
01:03:47,700 --> 01:03:49,780
I don't want to read email or anything else.

1146
01:03:51,140 --> 01:03:57,060
And then usually after about 60 minutes, I'll go and do some exercise.

1147
01:03:57,060 --> 01:03:59,220
So that's usually that's usually my morning routine.

1148
01:03:59,220 --> 01:04:02,580
I was about to ask if you're the kind of person that actually wakes up,

1149
01:04:02,580 --> 01:04:05,540
goes to the gym first thing and then, you know, starts.

1150
01:04:05,540 --> 01:04:06,020
Pretty much.

1151
01:04:06,020 --> 01:04:06,660
Yeah.

1152
01:04:06,660 --> 01:04:07,140
With the day.

1153
01:04:07,140 --> 01:04:08,420
Yeah, yeah, yeah, yeah, yeah.

1154
01:04:08,420 --> 01:04:13,300
And he says he's usually that's, that's my, you know, the coffee, some yogurt and some fruit.

1155
01:04:13,940 --> 01:04:16,660
But 60 minutes of silliness, then I'll go exercise.

1156
01:04:16,660 --> 01:04:18,980
And then, yeah, then I go, then I'm going to go tackle the day.

1157
01:04:18,980 --> 01:04:22,500
But that's pretty, pretty regimented as to how that will line up.

1158
01:04:22,500 --> 01:04:23,380
I can relate to that.

1159
01:04:23,380 --> 01:04:24,500
I mean, it's a great routine.

1160
01:04:25,460 --> 01:04:29,540
OK, so the other question is about books.

1161
01:04:29,540 --> 01:04:35,060
And in particular, what is the book that you've gifted to the most to other people?

1162
01:04:36,260 --> 01:04:39,140
Oh, well, that's a good question, too.

1163
01:04:39,140 --> 01:04:39,700
What is the book?

1164
01:04:40,420 --> 01:04:44,660
Um, you know, I could just let's just go with recent times, like a book I read very.

1165
01:04:45,860 --> 01:04:50,980
I'm sure I have a better answer for the most of all time, but I tend to read,

1166
01:04:50,980 --> 01:04:54,420
you know, there'll be like one or two books a year that will really resonate with me.

1167
01:04:54,420 --> 01:04:56,660
And I will tell everyone that they have to read them.

1168
01:04:56,660 --> 01:05:00,020
And so the most recent one, which is of the last month,

1169
01:05:00,740 --> 01:05:03,380
is a book called The Knowledge Machine by Michael Strevens,

1170
01:05:03,940 --> 01:05:08,020
which is a philosophy of science book, but written in a very accessible language.

1171
01:05:08,020 --> 01:05:08,980
And it's phenomenal.

1172
01:05:08,980 --> 01:05:12,340
I feel like it's the first philosophy of science book I've read where I was like,

1173
01:05:12,340 --> 01:05:16,660
this guy really is explaining what science is, what the practice of science is.

1174
01:05:16,660 --> 01:05:20,900
And it has a very, I don't know, I thought it was a really interesting

1175
01:05:21,540 --> 01:05:26,180
way to appreciate what we do, especially in the applied sciences.

1176
01:05:26,180 --> 01:05:28,020
So I thought that was a great book.

1177
01:05:28,020 --> 01:05:29,140
The Knowledge Machine.

1178
01:05:29,140 --> 01:05:29,380
Yeah.

1179
01:05:30,180 --> 01:05:30,500
All right.

1180
01:05:30,500 --> 01:05:36,820
So maybe the last question again from the audience is about redesigning machine learning.

1181
01:05:36,820 --> 01:05:43,780
And in particular, they ask, if you could redesign ML from scratch, how would you redesign it?

1182
01:05:45,140 --> 01:05:47,700
Oh, do you think they're asking about the discipline or the course?

1183
01:05:47,700 --> 01:05:50,580
So remove all the hype that surrounds it now.

1184
01:05:50,580 --> 01:05:50,820
Yeah.

1185
01:05:50,820 --> 01:05:52,260
What would you, what would you change?

1186
01:05:52,820 --> 01:05:54,660
Well, so I could sell my book now, right?

1187
01:05:54,660 --> 01:05:56,260
And that's what we do at the end of the podcast.

1188
01:05:56,260 --> 01:05:56,820
Absolutely.

1189
01:05:57,780 --> 01:05:59,540
So there we go.

1190
01:05:59,540 --> 01:06:02,660
So, and even that's incomplete.

1191
01:06:02,820 --> 01:06:08,340
But Moritz Hart and I wrote a book called Patterns, Predictions, and Actions, which

1192
01:06:08,340 --> 01:06:12,340
was our attempt to figure out what would we want to teach a graduate course in machine learning.

1193
01:06:12,900 --> 01:06:16,420
And it's, I really like it.

1194
01:06:16,420 --> 01:06:21,700
I mean, I do think that this is kind of, would be a book I teach from for a long time.

1195
01:06:22,420 --> 01:06:26,340
Of course, as soon as the book went to the publisher, I told Moritz, oh, no, we have

1196
01:06:26,340 --> 01:06:27,300
to rewrite the whole thing.

1197
01:06:27,300 --> 01:06:30,500
And then he got, he didn't want to hear that because he's right.

1198
01:06:30,500 --> 01:06:31,220
I shouldn't do that.

1199
01:06:31,220 --> 01:06:34,740
But I still have ideas about how to iterate and how we might, how we might change it down

1200
01:06:34,740 --> 01:06:35,140
the line.

1201
01:06:35,140 --> 01:06:42,020
But, you know, walking you through that book, we start with the elements of detection theory,

1202
01:06:42,020 --> 01:06:46,820
which is, you know, I do think where it's the reasonable starting point for classification.

1203
01:06:49,700 --> 01:06:53,700
From detection theory alone, just making decisions and prediction and making decisions,

1204
01:06:54,500 --> 01:06:59,060
you already start to run into all of these tricky issues about the best you could possibly

1205
01:06:59,060 --> 01:07:03,060
do and what happens when there's like hidden heterogeneity.

1206
01:07:03,060 --> 01:07:05,220
And you just, there's all sorts of things that come out of that.

1207
01:07:06,420 --> 01:07:09,620
From there though, once we say, okay, well, okay, we're going to do this anyway.

1208
01:07:09,620 --> 01:07:11,860
We're going to do this kind of decision-making.

1209
01:07:11,860 --> 01:07:15,700
The thing we move to is, you know, supervised learning.

1210
01:07:15,700 --> 01:07:20,100
And it's kind of a nice jump, you know, supervised learning is, you know, how you would do prediction

1211
01:07:20,100 --> 01:07:24,580
when I don't necessarily know the way that the data is coming to me.

1212
01:07:24,580 --> 01:07:27,380
And so then we dive into supervised learning.

1213
01:07:28,340 --> 01:07:32,020
I think when we teach the course, about half the course is on supervised learning.

1214
01:07:32,020 --> 01:07:35,460
And we talk about, you know, feature representations and where those might come

1215
01:07:35,460 --> 01:07:37,460
from and how you can build new ones.

1216
01:07:37,460 --> 01:07:41,060
And then we talk about optimization, which is really key.

1217
01:07:41,060 --> 01:07:44,260
We have a simplified view there, but, you know, I think there's, I honestly think that

1218
01:07:44,260 --> 01:07:47,780
most people who do a first course in machine learning really should take a second course

1219
01:07:47,780 --> 01:07:48,500
in optimization.

1220
01:07:49,860 --> 01:07:53,860
We also talk about generalization and generalizability, which to me, we just kind of

1221
01:07:53,860 --> 01:07:55,700
go through a lot of different ideas that people have.

1222
01:07:56,820 --> 01:07:58,980
And some of them are satisfying and some of them are not.

1223
01:07:58,980 --> 01:08:03,380
But we connect back to these ideas from the 70s and 60s about like, we knew a lot of these

1224
01:08:05,300 --> 01:08:06,500
concepts are old.

1225
01:08:06,500 --> 01:08:08,260
And like the way we approach them are old.

1226
01:08:08,260 --> 01:08:10,660
And most of them are sensitivity, just like you said.

1227
01:08:10,660 --> 01:08:16,420
If you have a prediction that's very insensitive to changes in the data, it's probably going

1228
01:08:16,420 --> 01:08:18,500
to be similar on new data.

1229
01:08:18,500 --> 01:08:21,540
So that's an interesting thing there.

1230
01:08:21,540 --> 01:08:26,660
Then we have this whole chapter on data and datasets, which I think I've never, nobody

1231
01:08:26,660 --> 01:08:30,980
else has done in a book, which like that's kind of like most of the field now is about

1232
01:08:30,980 --> 01:08:31,300
data.

1233
01:08:31,300 --> 01:08:36,260
So we really dive into datasets, why they exist, why they persist.

1234
01:08:36,260 --> 01:08:37,220
What do we do after that?

1235
01:08:37,220 --> 01:08:38,100
I mean, I could keep going.

1236
01:08:38,100 --> 01:08:38,420
It's great.

1237
01:08:38,420 --> 01:08:41,380
So after that, we have like five pages on deep learning.

1238
01:08:41,380 --> 01:08:44,260
And then we move to, I think this is the thing that's interesting.

1239
01:08:44,260 --> 01:08:48,100
After the five pages on deep learning, we move to, you know, what happens when you're

1240
01:08:48,100 --> 01:08:51,300
trying to make predictions and you're not in a static world anymore?

1241
01:08:52,020 --> 01:08:56,580
Touching on notions of causality, we talk about optimal control, we talk about reinforcement

1242
01:08:56,580 --> 01:08:57,060
learning.

1243
01:08:57,060 --> 01:08:59,620
So I think that that pivot in the middle is very interesting.

1244
01:08:59,620 --> 01:09:03,220
And I think that that's the stuff that we have to, I mean, it's still, I'm not sure

1245
01:09:03,220 --> 01:09:05,220
I'm happy with any of the answers that we have there.

1246
01:09:05,220 --> 01:09:08,180
I don't think, I think they're just things that we should know as we move forward.

1247
01:09:08,180 --> 01:09:12,980
But I think those are the places where I'm most interested still to look at machine learning

1248
01:09:12,980 --> 01:09:17,700
is like what happens when you move away from the static frame to this dynamic frame?

1249
01:09:17,700 --> 01:09:20,660
It's just, we're still figuring it out.

1250
01:09:20,660 --> 01:09:20,900
Yeah.

1251
01:09:20,900 --> 01:09:27,140
I guess a book always can be seen as a picture of the zeitgeist, if you want, of the time.

1252
01:09:27,140 --> 01:09:31,220
And I'm just particularly interested about the chapter on datasets.

1253
01:09:31,780 --> 01:09:38,260
I think that's a great idea because it doesn't really get talked about, I guess, in any book

1254
01:09:38,260 --> 01:09:39,220
that I've seen.

1255
01:09:39,220 --> 01:09:42,420
Do you touch on the standardization issues?

1256
01:09:43,700 --> 01:09:44,100
Yeah.

1257
01:09:44,100 --> 01:09:46,260
So we touched on a variety of aspects there.

1258
01:09:47,060 --> 01:09:52,500
Also, just for the people listening, the book is available for free, early version of the

1259
01:09:52,500 --> 01:09:55,220
book is available for free at mlstory.org.

1260
01:09:56,180 --> 01:09:58,100
And I think that's right.

1261
01:09:58,820 --> 01:10:04,020
And we'll correct that after the fact or add it to the ladder notes, the correct link there.

1262
01:10:04,020 --> 01:10:11,700
But the dataset chapter talks about some of the existing datasets and how they were created.

1263
01:10:11,700 --> 01:10:16,420
And actually, we go through this whole study of just the history of the dataset.

1264
01:10:16,420 --> 01:10:18,820
Like, why did we even standardize these things to begin with?

1265
01:10:19,540 --> 01:10:20,820
It's really fascinating.

1266
01:10:20,820 --> 01:10:29,220
I mean, there's this amazing story that Moritz and I found about the first dataset for handwritten

1267
01:10:29,220 --> 01:10:33,460
character recognition, which is everybody's bread and butter favorite application of machine

1268
01:10:33,460 --> 01:10:34,340
learning.

1269
01:10:34,340 --> 01:10:36,340
And the first dataset is from 1959.

1270
01:10:36,900 --> 01:10:38,020
It's crazy.

1271
01:10:38,020 --> 01:10:38,580
Wow.

1272
01:10:38,580 --> 01:10:39,860
That's really crazy.

1273
01:10:40,500 --> 01:10:41,220
It's crazy.

1274
01:10:41,300 --> 01:10:43,300
The story behind that dataset is just incredible.

1275
01:10:44,100 --> 01:10:46,420
There was this fellow named Bill Heilemann.

1276
01:10:46,420 --> 01:10:47,860
He was working at Bell Labs.

1277
01:10:48,660 --> 01:10:54,740
He was tasked with making, everybody was trying to make OCR readers at the time, which is

1278
01:10:54,740 --> 01:10:58,020
kind of crazy to think that you're trying to do that in the 50s.

1279
01:10:58,020 --> 01:10:58,980
But they were trying.

1280
01:10:59,620 --> 01:11:03,780
And let me just tell a couple things about this.

1281
01:11:03,780 --> 01:11:05,060
This is one of my favorite stories.

1282
01:11:06,340 --> 01:11:10,340
The first thing was that they didn't realize, I mean, Bill's thought,

1283
01:11:11,700 --> 01:11:14,580
was most people were trying to build end-to-end devices.

1284
01:11:14,580 --> 01:11:18,260
Because that was, you know, personal computers were not really that big a thing.

1285
01:11:18,260 --> 01:11:22,740
And Bill was like, I can't design it unless I could kind of abstract this somehow.

1286
01:11:22,740 --> 01:11:24,820
So his idea was to have two abstractions.

1287
01:11:24,820 --> 01:11:27,540
You would have a scanner that would produce data.

1288
01:11:27,540 --> 01:11:31,220
And then once you had the data stored, you would have the second step of the, you would

1289
01:11:31,220 --> 01:11:34,180
simulate the rest of the process on one of their big computers.

1290
01:11:34,180 --> 01:11:34,740
Wow.

1291
01:11:34,740 --> 01:11:37,940
Like now this sounds obvious, but it was not at all obvious.

1292
01:11:37,940 --> 01:11:40,340
At the time it was quite ambitious, I guess.

1293
01:11:40,580 --> 01:11:41,460
Quite ambitious.

1294
01:11:41,460 --> 01:11:46,100
And so what he did was he built a scanner that was supposed to be a general purpose

1295
01:11:46,100 --> 01:11:46,580
scanner.

1296
01:11:46,580 --> 01:11:53,380
And he collected 50 different alphabets from 50 different writers and put them on punch

1297
01:11:53,380 --> 01:11:56,100
cards and then let people, and then published a paper.

1298
01:11:56,820 --> 01:12:01,460
It was kind of this weird thing where he published a paper on these and someone else grabbed

1299
01:12:01,460 --> 01:12:01,700
them.

1300
01:12:02,500 --> 01:12:04,660
Well, actually, I remember this is a good story.

1301
01:12:04,660 --> 01:12:07,940
He published a paper saying somebody else's method wasn't good.

1302
01:12:08,820 --> 01:12:11,140
That's always the start of a paper, I guess.

1303
01:12:11,140 --> 01:12:11,700
Of course.

1304
01:12:11,700 --> 01:12:12,980
That's always how it goes.

1305
01:12:12,980 --> 01:12:18,820
And that guy, his name was Bledsoe, who was at Stanford at the time or in Palo Alto at

1306
01:12:18,820 --> 01:12:21,700
the time, you know, asked, well, I think you're doing this wrong.

1307
01:12:21,700 --> 01:12:22,820
Can you send me your cards?

1308
01:12:23,540 --> 01:12:28,020
And so Heilemann sent the cards to Bledsoe and then Bledsoe published some other paper.

1309
01:12:28,020 --> 01:12:33,460
And then this fellow Chow was also really famous for a lot of things in machine learning.

1310
01:12:33,620 --> 01:12:34,340
Machine learning.

1311
01:12:35,140 --> 01:12:38,900
Chow got a copy of them, did his own analysis.

1312
01:12:39,620 --> 01:12:41,540
Duda and Hart at SRI got a copy.

1313
01:12:41,540 --> 01:12:47,060
And so, you know, Bill has this amazing short note in one of the IEEE journals that said

1314
01:12:47,060 --> 01:12:50,100
that, like, I made these this data if you want it.

1315
01:12:50,100 --> 01:12:51,380
Here's my mailing address.

1316
01:12:51,380 --> 01:12:52,420
I'll print them for you.

1317
01:12:53,620 --> 01:12:57,380
And he's sending a shoebox of 1800 punch cards around the country, I guess.

1318
01:12:57,380 --> 01:12:59,780
That was what that was the way the data was disseminated.

1319
01:12:59,780 --> 01:13:00,340
Fantastic.

1320
01:13:01,060 --> 01:13:01,560
Physically.

1321
01:13:02,580 --> 01:13:03,940
Physically, physically.

1322
01:13:03,940 --> 01:13:09,940
And I think one of the another, like, just fun quirk about that story is that he well,

1323
01:13:09,940 --> 01:13:10,740
there are two fun quirks.

1324
01:13:10,740 --> 01:13:16,020
First of all, Bill wrote the first paper I've ever seen on training versus test split.

1325
01:13:16,900 --> 01:13:21,860
So I think and everybody just naturally came to this conclusion that you use the first

1326
01:13:21,860 --> 01:13:25,140
40 alphabets for training and last 10 for testing.

1327
01:13:25,140 --> 01:13:26,100
It was very natural.

1328
01:13:26,100 --> 01:13:29,940
And Bill actually wrote a paper trying to justify it, which is great.

1329
01:13:29,940 --> 01:13:35,700
And the second thing that was fascinating is in 1962, he published a PhD thesis on this

1330
01:13:35,700 --> 01:13:38,660
work, and then left machine learning for the rest of his career.

1331
01:13:39,460 --> 01:13:42,500
Because because he just considered the project a failure.

1332
01:13:42,500 --> 01:13:43,460
No way.

1333
01:13:43,460 --> 01:13:44,500
He considered it a failure.

1334
01:13:44,500 --> 01:13:45,220
And it's amazing.

1335
01:13:45,220 --> 01:13:48,500
It led to everything since no one's heard of this man, Bill, Bill Hyman.

1336
01:13:48,500 --> 01:13:53,220
What he went to work on was computer networks, because he's like, I obviously doesn't want

1337
01:13:53,220 --> 01:13:55,700
to send around shoeboxes of punch cards anymore.

1338
01:13:56,340 --> 01:14:01,860
He did a lot of interesting work in high performance computing after after this, but he just considered

1339
01:14:01,860 --> 01:14:03,460
that, you know, the machines weren't there.

1340
01:14:04,260 --> 01:14:10,500
And it's kind of wild to just watch over the course of, you know, several decades, everything

1341
01:14:10,500 --> 01:14:11,300
really came together.

1342
01:14:11,300 --> 01:14:16,740
And now OCR is effectively solved, but not using anything dramatically more sophisticated

1343
01:14:16,740 --> 01:14:17,780
than what he proposed.

1344
01:14:17,780 --> 01:14:20,180
You just needed the technology to catch up.

1345
01:14:20,180 --> 01:14:24,980
I guess this is also a fantastic message for all those PhD students out there hitting their

1346
01:14:24,980 --> 01:14:25,860
PhD blues.

1347
01:14:28,420 --> 01:14:33,620
You may not know whether, you know, your work is going to end up unfolding as pioneering

1348
01:14:33,620 --> 01:14:34,500
in 50 years.

1349
01:14:34,500 --> 01:14:35,140
Amazing.

1350
01:14:35,140 --> 01:14:40,420
And this is also a fantastic assist maybe for my next and possibly last question before

1351
01:14:40,420 --> 01:14:41,540
we move on to music.

1352
01:14:42,820 --> 01:14:48,340
This is something that I would actually probably I'm going to ask to everybody else on the

1353
01:14:48,340 --> 01:14:51,700
podcast, and it's about future students.

1354
01:14:52,660 --> 01:14:54,820
If you were a student today, what would you do?

1355
01:14:55,380 --> 01:15:00,420
Or what would you invest on, rather, in the sense that what would you need to do to live

1356
01:15:00,420 --> 01:15:02,100
a life that you would be proud of?

1357
01:15:02,100 --> 01:15:03,700
What's your best advice?

1358
01:15:03,700 --> 01:15:05,860
I mean, this comes back to what we talked about at the beginning.

1359
01:15:05,860 --> 01:15:12,500
I mean, to me, it's just like if you're there's ups and downs in a research career, and it's

1360
01:15:12,500 --> 01:15:15,220
not necessarily for everyone.

1361
01:15:15,220 --> 01:15:19,860
And if you could think of something else to do, I would just say you should do that.

1362
01:15:19,860 --> 01:15:22,580
Something else that inspires and fulfills you, you should do that.

1363
01:15:22,580 --> 01:15:26,740
But if this is what keeps you up at night, just make sure that you're always reminding

1364
01:15:26,740 --> 01:15:27,780
yourself of why.

1365
01:15:27,780 --> 01:15:31,860
Like, yeah, this is like reminding yourself that, yeah, this case, I'm passionate about

1366
01:15:31,860 --> 01:15:32,100
this.

1367
01:15:32,980 --> 01:15:34,500
This is why I'm passionate about this.

1368
01:15:34,500 --> 01:15:36,180
And this is why I'm going to keep doing it.

1369
01:15:36,180 --> 01:15:39,780
I think that's like, you know, being in touch with that is super important.

1370
01:15:40,420 --> 01:15:44,180
And the other thing I would say to everybody is like the topic never really matters.

1371
01:15:44,180 --> 01:15:47,860
I think it's this making sure you're enjoying the work.

1372
01:15:47,860 --> 01:15:55,060
And then for me, this is not true for everyone, but for me, it was also the interactions with

1373
01:15:55,060 --> 01:15:58,820
my fellow graduate students that really just kept me going.

1374
01:15:58,820 --> 01:16:03,460
I think, you know, chatting with you're in this weird opportunity to be around a bunch

1375
01:16:03,460 --> 01:16:07,540
of brilliant other people who know a lot of different things in you and taking advantage

1376
01:16:07,540 --> 01:16:12,660
of that resource and, you know, stepping out of your department, even, you know, so I again,

1377
01:16:12,660 --> 01:16:16,420
I said, I went to an interesting program at MIT called the Media Lab.

1378
01:16:16,420 --> 01:16:20,980
But there I was surrounded with people who, you know, worked on nanotechnology, who worked

1379
01:16:20,980 --> 01:16:26,340
on machine learning, who worked on education of kids, who worked on electronic art, who

1380
01:16:26,340 --> 01:16:30,500
worked on computer net, which is such a diverse, weird group of people.

1381
01:16:30,500 --> 01:16:37,540
And it's just that kind of getting a bunch of people like that under one roof and letting

1382
01:16:37,540 --> 01:16:38,500
them do whatever they want.

1383
01:16:38,500 --> 01:16:43,220
It's just kind of a powerful, amazing thing to experience.

1384
01:16:43,220 --> 01:16:45,860
And this in general, I think is great advice.

1385
01:16:45,860 --> 01:16:48,580
And I'm pretty sure we will resonate with many out there.

1386
01:16:49,860 --> 01:16:53,060
I guess this brings me to the last topic of today.

1387
01:16:53,060 --> 01:16:59,060
And the next question or question, rather topic of discussion, if you want, will be

1388
01:16:59,060 --> 01:17:00,020
about music.

1389
01:17:00,020 --> 01:17:06,580
So while doing my homework for this podcast episode, I found out that you're a brilliant

1390
01:17:06,580 --> 01:17:08,820
and very accomplished musician.

1391
01:17:08,820 --> 01:17:10,580
I found that super interesting.

1392
01:17:10,580 --> 01:17:18,020
I personally like many of your tracks and I just thought it would be interesting to

1393
01:17:18,020 --> 01:17:23,220
dig into this a little bit and maybe ask you, what's your relationship with music?

1394
01:17:23,220 --> 01:17:24,260
How did it all start?

1395
01:17:24,260 --> 01:17:31,220
And how do you manage to have such a successful career at the academic level and also keep

1396
01:17:31,220 --> 01:17:34,740
such a passion alive in the way you do it?

1397
01:17:34,740 --> 01:17:36,340
Yeah, I've always loved music.

1398
01:17:36,420 --> 01:17:41,620
I bought my first guitar when I was, I think, 13, 12 or 13, started playing there.

1399
01:17:41,620 --> 01:17:42,820
And then I was just kind of hooked.

1400
01:17:42,820 --> 01:17:43,860
Yeah, I was just hooked.

1401
01:17:43,860 --> 01:17:45,460
I was just hooked after that.

1402
01:17:45,460 --> 01:17:51,300
And I think that there was always, so it's always been something that I've been passionate

1403
01:17:51,300 --> 01:17:51,540
about.

1404
01:17:51,540 --> 01:17:56,820
So it's something, of course, also when you're a teenager, feel like you could do that for

1405
01:17:56,820 --> 01:18:01,220
a living, which is not for, I mean, to those who can, more power to you.

1406
01:18:01,220 --> 01:18:03,060
And I love it and just pursue it.

1407
01:18:03,060 --> 01:18:04,260
But not everybody can.

1408
01:18:04,260 --> 01:18:07,940
It's definitely, I think, a little bit easier to become a professor than a professional

1409
01:18:07,940 --> 01:18:09,780
musician, especially when you could live.

1410
01:18:09,780 --> 01:18:14,260
So you're stating that becoming a professor at Berlgrie is easier than becoming a professional

1411
01:18:14,260 --> 01:18:14,740
musician.

1412
01:18:15,380 --> 01:18:16,660
I worry that that might be.

1413
01:18:18,420 --> 01:18:19,220
I don't know, man.

1414
01:18:19,780 --> 01:18:21,060
It's a hard hustle being.

1415
01:18:21,060 --> 01:18:23,940
Sorry, sorry to intervene, but it was just so funny.

1416
01:18:23,940 --> 01:18:27,700
I do worry that it's a hard hustle for my musician friends.

1417
01:18:27,700 --> 01:18:32,900
I mean, I think some of them have been, you know, I think the ones who've been successful

1418
01:18:32,900 --> 01:18:37,940
have been very entrepreneurial and really thought hard about what exactly they wanted

1419
01:18:37,940 --> 01:18:42,900
from that career path that they were choosing, but it's been hard for a lot of them.

1420
01:18:42,900 --> 01:18:45,700
And I think it's, you know, there's a lot of passion that goes into it.

1421
01:18:48,420 --> 01:18:52,820
But I'll say that, like, up until my late 20s, I still kind of had a dream.

1422
01:18:52,820 --> 01:18:59,860
So it took a long time for me to to shake that idea of being a professional musician.

1423
01:18:59,860 --> 01:19:02,340
And you're still an active musician in the sense.

1424
01:19:02,340 --> 01:19:04,660
I mean, we chatted about this a couple of weeks ago.

1425
01:19:05,620 --> 01:19:08,660
Have you been active also during the pandemic?

1426
01:19:08,660 --> 01:19:09,220
Yeah, no.

1427
01:19:09,220 --> 01:19:16,420
And I think the funny thing now is that so I still have one very active recording project

1428
01:19:16,420 --> 01:19:17,700
that's called The Fun Years.

1429
01:19:18,500 --> 01:19:23,460
My bandmate and I, I don't know, we're both, you know, in our 40s and we haven't really

1430
01:19:23,460 --> 01:19:27,780
figured out this, how to release our back catalog, but we're thinking about it.

1431
01:19:27,780 --> 01:19:30,820
It's going to come out and we definitely have some new stuff that we want to put out

1432
01:19:30,820 --> 01:19:31,460
soon.

1433
01:19:31,460 --> 01:19:32,580
We just haven't figured out how.

1434
01:19:32,580 --> 01:19:32,740
Yeah.

1435
01:19:32,740 --> 01:19:37,860
So to the audience out there that are listening to this podcast, please hit a like on Spotify

1436
01:19:37,860 --> 01:19:39,460
or wherever you listen to your music.

1437
01:19:39,460 --> 01:19:40,020
That would be great.

1438
01:19:40,740 --> 01:19:44,340
Shameless advertisement also in this respect.

1439
01:19:44,340 --> 01:19:49,220
Actually, with your permission, I'm going to ask you to close this episode with my favorite

1440
01:19:49,220 --> 01:19:54,020
track, which whose name, if I'm correct, is Breach on the Bowstring.

1441
01:19:54,020 --> 01:19:54,740
Is that right?

1442
01:19:54,740 --> 01:19:55,300
Absolutely.

1443
01:19:55,300 --> 01:19:56,500
Yeah, absolutely.

1444
01:19:56,500 --> 01:19:58,820
So with this, we're going to close this episode.

1445
01:19:58,820 --> 01:20:01,380
And Ben, thanks for being with us.

1446
01:20:01,380 --> 01:20:02,100
Thank you so much.

1447
01:20:02,100 --> 01:20:03,220
It was fun.

1448
01:20:57,380 --> 01:20:58,820
Thank you for listening.

1449
01:20:58,820 --> 01:21:00,260
I hope you liked the show today.

1450
01:21:01,060 --> 01:21:06,420
If you enjoyed the podcast, please consider giving us five stars on Apple Podcasts, follow

1451
01:21:06,420 --> 01:21:12,260
us on Spotify, support on Patreon or PayPal, and connect with us on social media platforms.

1452
01:21:13,460 --> 01:21:14,260
See you next time.

