WEBVTT

00:00.000 --> 00:10.480
Hello and welcome to In Control, the first podcast on control theory.

00:10.480 --> 00:21.920
Here we discuss the science of feedback, decision-making, artificial intelligence, and much more.

00:21.920 --> 00:27.280
I'm your host Alberto Padoan, live from our recording studio at ETH Zurich.

00:27.280 --> 00:31.840
Big thanks to our sponsor, the National Center of Competence in Research on Dependable Ubiquitous

00:31.840 --> 00:36.520
Automation, which you can check following the link in the description.

00:36.520 --> 00:38.360
Our guest today is Ben Recht.

00:38.360 --> 00:43.240
Ben is a full professor in the Department of Electrical Engineering and Computer Sciences

00:43.240 --> 00:46.160
at the University of California, Berkeley.

00:46.160 --> 00:48.200
Ben has received a number of awards.

00:48.200 --> 00:54.840
I'm just going to mention that Ben received the NeurIPS Test of Time Award twice.

00:54.840 --> 00:55.840
Welcome to the show, Ben.

00:56.200 --> 00:57.520
Thanks for having me.

00:57.520 --> 01:04.640
So perhaps one nice way to start this podcast is to say that you predicted the existence

01:04.640 --> 01:06.040
of this podcast.

01:06.040 --> 01:11.520
In an IFAC plenary about two years ago, if I'm not wrong, you said this.

01:11.520 --> 01:16.280
And I'm just kind of considering this like an experiment in podcasting because the world

01:16.280 --> 01:20.320
really needs a control theory podcast.

01:20.320 --> 01:21.320
Maybe there is one.

01:21.320 --> 01:23.880
If there is one, I'm happy to participate.

01:23.880 --> 01:24.880
Send me an email.

01:24.920 --> 01:28.440
I'd love to jump on your control theory podcast.

01:28.440 --> 01:31.200
So Ben, you did envision this happening.

01:31.200 --> 01:33.760
So where should I invest my money?

01:33.760 --> 01:34.760
What else can you predict?

01:34.760 --> 01:38.840
Did I predict or did I cause?

01:38.840 --> 01:42.600
I think that's always the tricky question with all of these things, right?

01:42.600 --> 01:44.240
Well, there is a funny anecdote here.

01:44.240 --> 01:47.880
I was absolutely not aware of you mentioning this thing.

01:47.880 --> 01:53.840
I did not participate to that conference, nor did I watch your lecture before somebody

01:53.840 --> 01:57.080
told me in our department.

01:57.080 --> 02:04.040
And then I went on the internet, I found your plenary on YouTube, and I was so pleased to

02:04.040 --> 02:08.040
see that somehow you managed to predict this thing.

02:08.040 --> 02:10.040
Yeah, man.

02:10.040 --> 02:14.520
So Alberto, why did you decide that you, I mean, independently decide that you wanted

02:14.520 --> 02:16.800
to do a control-centric podcast?

02:16.800 --> 02:22.360
Well, you know, there's so many other podcasts out there about artificial intelligence, machine

02:22.360 --> 02:24.880
learning, even soft robotics.

02:24.880 --> 02:31.920
So I guess it's time for us to step up or step out of our academic rooms and maybe talk

02:31.920 --> 02:34.560
about our cool stuff.

02:34.560 --> 02:35.560
Absolutely.

02:35.560 --> 02:44.140
Perhaps a good starting point for our chat today will be about your personal trajectory.

02:44.140 --> 02:46.520
How did it all start?

02:46.520 --> 02:47.520
How early should I go?

02:47.520 --> 02:48.520
As much as you want.

02:48.520 --> 02:49.520
It's a little bit hard, right?

02:49.520 --> 02:50.520
Six months in, if you want.

02:50.520 --> 03:00.080
I mean, I'm having a hard time remembering how I got to my office today.

03:00.080 --> 03:01.080
So it's a lot.

03:01.080 --> 03:04.000
It's trying to dig back into some ancient times.

03:04.000 --> 03:05.000
I don't know.

03:05.000 --> 03:06.000
It's interesting, right?

03:06.000 --> 03:13.960
I think we're in this funny period of recruiting graduate students, and I always like to ask

03:13.960 --> 03:17.920
graduate students why they want to go to graduate school.

03:18.920 --> 03:20.920
I actually stole this one from my wife.

03:20.920 --> 03:27.240
One of the questions I ask them or tell them is, you know, if you could envision yourself

03:27.240 --> 03:33.680
doing anything else other than going to graduate school, you should probably just do that.

03:33.680 --> 03:35.080
I firmly believe that.

03:35.080 --> 03:39.360
I really feel like if you can figure out some other thing that would make you happy and

03:39.360 --> 03:42.480
fulfilled and that, you know, maybe it's risky, maybe it's not.

03:42.480 --> 03:45.760
I still suggest that people do that.

03:46.600 --> 03:50.320
For whatever reason, I wasn't one of those people.

03:50.320 --> 03:53.280
I kind of fell into the going to graduate school thing after college.

03:53.280 --> 03:54.280
I wasn't sure.

03:54.280 --> 03:59.440
I mean, I was always, you know, good in school, but I always had this.

03:59.440 --> 04:00.440
I had a lot of other passions.

04:00.440 --> 04:06.560
I was really into music as a high school kid and college kid, and by the time I got to

04:06.560 --> 04:09.480
my senior year of college, I thought to myself, either I'm going to do one of two things.

04:09.480 --> 04:17.560
I'm either going to go and try to volunteer at all of these recording studios in Chicago

04:17.560 --> 04:21.080
where I was going to undergraduate or I'll go to graduate school.

04:21.080 --> 04:23.160
And then I looked at the graduate schools.

04:23.160 --> 04:27.360
I found one that looked kind of interesting at MIT where they were doing a bunch of weird

04:27.360 --> 04:28.360
stuff.

04:28.360 --> 04:29.560
And it was like, OK, this looks different.

04:29.560 --> 04:31.200
It would be a big change.

04:31.200 --> 04:32.800
They're kind of doing stuff related to music.

04:32.800 --> 04:33.800
I'll just apply there.

04:33.800 --> 04:35.320
And it was very dumb.

04:35.320 --> 04:37.880
Me as a 22 year old, just I had no, I mean, I don't know.

04:38.280 --> 04:42.520
It was not very well thought out process to how I ended up where I am.

04:42.520 --> 04:48.600
But I think it was, it's interesting to go from, you know, I did pure math as an undergraduate.

04:48.600 --> 04:54.800
I decided pretty early on, well, not pretty early on, actually, I remember that when I

04:54.800 --> 05:00.960
decided I couldn't be a mathematician, it was in a class on commutative algebra.

05:00.960 --> 05:06.280
We had done some kind of proof in this class that required six chalkboards and it was just

05:06.320 --> 05:08.480
a diagram drawing all the other chalkboards.

05:08.480 --> 05:11.440
And then I was, I just thought that was one of those wake up and be like, I don't know

05:11.440 --> 05:13.160
what I'm doing with my life days.

05:13.160 --> 05:16.000
And I was like, I can't do this for the rest of my life.

05:16.000 --> 05:17.680
So I kind of just wandered around a lot.

05:17.680 --> 05:20.840
But it was one of those things where if I had, if I had a better idea, I probably would

05:20.840 --> 05:22.840
not be where I am today.

05:22.840 --> 05:27.960
So I ended up at MIT and that kind of changed a lot of my perspectives, going from someplace

05:27.960 --> 05:30.480
very theoretical to someplace very applied.

05:30.840 --> 05:37.880
And so having those, having that big juxtaposition, that big pendulum shift has been something

05:37.880 --> 05:44.160
that has kind of gone on throughout my career, swinging back and forth between just doing

05:44.160 --> 05:51.520
very pure stuff and then trying to get myself into more applied areas, sometimes successful

05:51.520 --> 05:52.640
and sometimes not.

05:52.640 --> 05:59.960
And then bouncing back into theory and also just having a lot of, I don't know what the

05:59.960 --> 06:04.480
right way to put this is, but like the attention span of a gnat, just like not, not being able

06:04.480 --> 06:08.520
to really necessarily focus too long on one thing.

06:08.520 --> 06:11.560
I think this is one of the fun things about being a professor.

06:11.560 --> 06:13.360
Everybody could be a professor in a different way.

06:13.360 --> 06:18.200
There are some people who are going to spend their entire careers, like improving one inequality

06:18.200 --> 06:21.240
because that's the, that's what they care about.

06:21.240 --> 06:23.040
And then I don't know what I'm doing.

06:23.040 --> 06:24.800
I'm just doing the opposite.

06:24.800 --> 06:28.960
I change what I want to work on every, every couple of weeks, maybe.

06:28.960 --> 06:34.520
No, I think that's selling myself a little short, but I do have a hard time spending

06:34.520 --> 06:38.600
more than a few years on a given topic.

06:38.600 --> 06:43.680
And I guess we'll get to this, but I will say right now, not super deep into control

06:43.680 --> 06:44.680
at the moment.

06:44.680 --> 06:50.200
So that was something that, that phase currently is over, but control is something I've gone

06:50.200 --> 06:53.080
back and forth with for my entire career.

06:53.080 --> 06:54.560
So I'm sure I'll come back.

06:54.560 --> 06:55.560
Yeah.

06:55.560 --> 07:00.720
I'm just curious about how did you stumble on control theory?

07:00.720 --> 07:02.360
Yeah.

07:02.360 --> 07:07.800
That was just one of those weird life coincidences, I think I got very lucky.

07:07.800 --> 07:12.480
So I, I mean, I think with a lot of these things, one of the things we underappreciate

07:12.480 --> 07:17.200
about academics in general is just these network connections that kind of take you to where

07:17.200 --> 07:18.200
you go, right?

07:18.200 --> 07:19.200
It's all about people.

07:19.200 --> 07:24.520
It's so much more about people than it is necessarily about particularities of research

07:24.520 --> 07:27.120
problems as to why you end up in a place.

07:27.120 --> 07:33.360
And so when I was pretty early on in graduate school, I went to, I went to a talk by John

07:33.360 --> 07:40.280
Doyle and I just, I, I appreciated his style, which hopefully a lot of people here have

07:40.280 --> 07:43.320
seen at one point or another.

07:43.320 --> 07:48.280
And it ended up talking to him a bunch after the talk with some graduate students.

07:48.280 --> 07:52.080
I always remember he said to me, I like you, you have no tact.

07:52.080 --> 07:53.080
I'll remember that.

07:53.080 --> 07:54.560
I, that's, will stick with me forever.

07:54.560 --> 08:01.320
But yeah, so once I met John, John actually introduced me to Raf D'Andrea, who had been

08:01.320 --> 08:03.400
doing a sabbatical in Boston.

08:03.400 --> 08:06.320
His wife was working at a company in Boston.

08:06.320 --> 08:07.640
Raf was currently at Cornell.

08:07.640 --> 08:10.000
This was before he was at ETH.

08:10.000 --> 08:13.680
And so, and then Raf and I hung out for a bit and yeah, that was kind of how it all

08:13.680 --> 08:14.680
started.

08:14.680 --> 08:18.920
So it was John, then Raf, and then I met a bunch of other people and it was, yeah, that

08:18.920 --> 08:21.640
was my, so again, random connection.

08:22.640 --> 08:23.840
Randomly, John liked me.

08:23.840 --> 08:27.400
I think that was kind of the, that was the beginning.

08:27.400 --> 08:28.400
This is super cool.

08:28.400 --> 08:34.200
I guess also given that you mentioned the influence of John, this is probably also one

08:34.200 --> 08:39.160
of the reasons a posteriori of your interest on robust control and perhaps bridging robust

08:39.160 --> 08:44.440
control to the world of machine learning.

08:44.440 --> 08:51.760
I guess my last question before we pause and move on, let's say to more technical terrain,

08:51.760 --> 08:55.040
is about the people that influenced your research.

08:55.040 --> 09:00.160
So if you had to name three of them, I guess one would be probably John Doyle, as you mentioned.

09:00.160 --> 09:01.160
Definitely John.

09:01.160 --> 09:02.160
Yeah.

09:02.160 --> 09:03.160
But what about the other two then?

09:03.160 --> 09:07.760
It's a great, wow.

09:07.760 --> 09:09.000
Other people who've been influenced by research.

09:09.000 --> 09:14.000
I mean, I think, I think again, the way you want, the way I think about it is not necessarily,

09:14.560 --> 09:17.120
and I probably could dig into this more and think about like, who are the people who I

09:17.120 --> 09:20.320
read who like really changed the way I thought.

09:20.320 --> 09:24.440
But I think a lot of the times it's just who are the people who were there to mentor me

09:24.440 --> 09:28.080
and to kind of lead me in these different directions.

09:28.080 --> 09:34.960
I would say, you know, Steve Wright had a huge impact on my, both on my professional

09:34.960 --> 09:39.240
career and the way I think about optimization.

09:40.240 --> 09:45.520
Steve was really instrumental in recruiting me to Wisconsin, where I was before Berkeley.

09:45.520 --> 09:52.440
And he was just really both a personal mentor and also just taught me so much about the

09:52.440 --> 09:54.640
ins and outs of optimization research.

09:54.640 --> 10:00.320
So I think Steve is another person who really stands out in my head as a real, a huge influence.

10:00.320 --> 10:07.240
Let me think about who else, because this is always tricky.

10:07.240 --> 10:10.240
Because there have been a lot of other people who I've just really enjoyed working with,

10:10.240 --> 10:15.960
like, you know, like Raph for bringing me into control and taking me in those directions.

10:15.960 --> 10:21.400
Pretty soon after I met Raph and John, Pablo Pirillo moved from ETH to MIT.

10:21.400 --> 10:23.680
Look at all these ETH connections.

10:23.680 --> 10:25.680
He moved from ETH to MIT.

10:25.680 --> 10:30.520
And then, you know, I learned so much from Pablo.

10:30.520 --> 10:35.640
Dimitri Burtsekas also at MIT is someone I've talked with a lot about so many different

10:35.640 --> 10:36.920
things at different points.

10:36.920 --> 10:41.680
I think what Dimitri is interesting because I learned convex analysis from him.

10:41.680 --> 10:44.080
That was my first real interaction with him.

10:44.080 --> 10:51.200
And I learned a lot about optimization algorithms, but it wasn't until many years later that

10:51.200 --> 10:55.400
I got into optimal control, reinforcement learning, and Dimitri had done so much work

10:55.400 --> 10:56.400
there.

10:56.400 --> 10:59.520
So it's kind of fun to reconnect with him in that space, because we had done so much

10:59.520 --> 11:03.840
of our chatting had been about optimization, then optimization and machine learning.

11:03.840 --> 11:07.440
But then it kind of looped back to the stuff that he was just the, you know, he was one

11:07.440 --> 11:11.040
of the most important researchers in dynamic programming.

11:11.040 --> 11:16.000
And it just wasn't, you know, it's just funny that we go through these trend cycles.

11:16.000 --> 11:20.640
Dynamic programming wasn't maybe as hot a topic when I was in graduate school, but it

11:20.640 --> 11:22.920
was kind of fun to kind of loop back with him.

11:22.920 --> 11:26.800
I certainly have more Burtsekas books than I think any other person.

11:26.800 --> 11:30.000
This is an interesting trivia.

11:30.000 --> 11:33.160
I mean, he writes so many books.

11:33.160 --> 11:34.160
It's not really fair.

11:34.160 --> 11:35.560
So many good books, I would say.

11:35.560 --> 11:36.840
I mean, they're all excellent.

11:36.840 --> 11:37.840
They're all excellent.

11:37.840 --> 11:38.840
Yeah, yeah.

11:38.840 --> 11:39.840
Oh, absolutely.

11:39.840 --> 11:42.360
But it's just also not, I mean, I don't know how anyone could keep up with that kind of

11:42.360 --> 11:43.360
prolific.

11:43.360 --> 11:47.560
This is so impressive.

11:47.560 --> 11:48.560
And you're right.

11:48.560 --> 11:51.000
Like, it's amazing how many of them are good, right?

11:51.000 --> 11:56.120
I mean, his nonlinear optimization book is just, it's just filled with amazing things.

11:56.120 --> 11:58.880
And he had that classic book on, I mean, it was amazing.

11:58.880 --> 12:00.000
I forgot about that.

12:00.000 --> 12:05.240
He wrote this book on distributed computing with Sitsiklis, I don't even remember when

12:05.240 --> 12:06.720
that book was from.

12:06.720 --> 12:07.720
It's old.

12:07.720 --> 12:14.480
And it kind of came back in the early 2010s was a very, like, everybody was into distributed

12:14.480 --> 12:15.480
machine learning.

12:15.480 --> 12:19.960
It turns out that anything you wanted was almost certainly in that book already.

12:19.960 --> 12:22.600
And so I remember interacting with him a lot about that.

12:22.600 --> 12:27.160
He was a little bit upset, honestly, that like, some people didn't realize that, but

12:27.160 --> 12:28.160
it worked out.

12:28.160 --> 12:29.160
Right.

12:29.320 --> 12:33.440
So it's distributed computing, he had done dynamic programming, he had done all sorts

12:33.440 --> 12:38.640
of stuff on stochastic gradients, just a really impressive person.

12:38.640 --> 12:45.440
And I think there's this, lots of people will write lots of papers, but there's really very

12:45.440 --> 12:52.520
few people who will, can turn a research program into courses, right?

12:52.520 --> 12:55.640
That's a hard jump to take something that is like, this is my research, and I'm going

12:55.640 --> 13:01.000
to turn this into something that I could teach to, you know, first year graduate students.

13:01.000 --> 13:06.480
That's taking cutting edge stuff and turning it into that kind of synthesis is, you know,

13:06.480 --> 13:07.480
really, really hard.

13:07.480 --> 13:09.640
And Dimitri is just incredibly talented at it.

13:09.640 --> 13:13.040
Yeah, this requires incredible mental clarity.

13:13.040 --> 13:19.760
I'll just take this assist that you served me to mention that all the people that you

13:19.760 --> 13:27.360
mentioned before are very welcome in this podcast, as well.

13:27.360 --> 13:31.280
You should, Dimitri would be a fun person to chat with, honestly, I think he would be,

13:31.280 --> 13:36.160
I mean, they're all fun, John always is fun, but I think Dimitri has a lot of very interesting

13:36.160 --> 13:37.160
perspectives.

13:37.160 --> 13:38.720
You should, I should, I'll send a pink tip.

13:38.720 --> 13:39.720
Let's see what we can do.

13:39.720 --> 13:40.720
Yeah.

13:40.720 --> 13:41.720
Yeah.

13:41.720 --> 13:42.720
Yeah.

13:42.720 --> 13:49.040
Okay, so maybe we can move on to, I would probably start with Argamin, your legendary

13:49.920 --> 13:55.760
blog, at least legendary for the underground community of machine learning and control geeks.

13:57.200 --> 14:01.680
You posted a wealth of super interesting blog posts there.

14:03.280 --> 14:09.120
Perhaps if I had to summarize the context of your own enterprise, there would be something like,

14:09.840 --> 14:13.760
in theory, there's no difference between theory and practice, but in practice, there is.

14:15.680 --> 14:16.640
What do you say about this?

14:17.280 --> 14:18.400
I mean, that's certainly true.

14:19.520 --> 14:22.800
The question is, is the gap bigger in theory or practice?

14:22.800 --> 14:27.120
And I actually, I think most of the time we say that the gap is bigger in practice,

14:27.120 --> 14:29.200
but I don't know, it just depends.

14:29.200 --> 14:33.840
I mean, I see some pretty loose bounds, especially in the things I've been working with.

14:35.280 --> 14:38.640
Actually, many times in my career, you see these things where the theory is much more

14:38.640 --> 14:40.400
pessimistic than it need be.

14:40.400 --> 14:45.600
So, you know, it's interesting to see that interplay.

14:46.240 --> 14:49.600
Yeah, no, the blog has been a very fun outlet for me.

14:50.720 --> 14:57.120
Yeah, I guess, you know, busting myths is really your cottage industry somehow.

14:57.120 --> 15:02.000
So I was wondering, what was your favorite myth to be busted?

15:02.000 --> 15:06.320
Or what would you be willing to do like in the future?

15:06.320 --> 15:09.040
What's the ultimate myth to bust?

15:09.600 --> 15:15.040
I think sometimes the ones I've found most interesting were the ones I believed.

15:16.240 --> 15:18.880
Right, where it just completely changes my mind.

15:20.160 --> 15:24.080
Usually, this is going to be something led by a graduate student or a postdoc in the

15:24.080 --> 15:29.280
group, and they just show you something that, or show me something that I just don't, would

15:29.280 --> 15:31.360
never have believed a day before.

15:32.960 --> 15:36.640
I think probably like the, there are probably lots of good examples here.

15:36.640 --> 15:42.720
But a really notable one, and one I've been writing about, again, recently, is there's

15:42.720 --> 15:47.520
this concept in machine learning and statistics of overfitting to data, right?

15:47.520 --> 15:52.240
I mean, where the idea would be that somehow you believe a little bit too much in your

15:52.240 --> 15:57.280
data and are unable to extrapolate out to new data, because you've really, you know,

15:57.280 --> 16:04.560
you've somehow caught on to aberrations and particularities of the data set you have.

16:04.560 --> 16:08.640
So it's a very common thing, and we teach it to our undergraduates, that, you know,

16:08.640 --> 16:10.000
overfitting is a huge problem.

16:11.360 --> 16:15.760
And in particular, the way that we do machine learning causes overfitting.

16:15.760 --> 16:18.480
So what, let me, let me dive into that.

16:18.480 --> 16:19.920
Yeah, let's jump into this.

16:19.920 --> 16:20.480
Absolutely.

16:20.480 --> 16:21.120
Okay.

16:21.120 --> 16:21.440
Yeah.

16:21.440 --> 16:26.960
So what we do in machine learning, just from the beginning of time, it turns out, and I

16:26.960 --> 16:31.280
could talk more about, I could talk more about the history of this, because I've also been

16:31.280 --> 16:34.320
absolutely involved in trying to dig into some of the history.

16:34.320 --> 16:37.840
Feel free to express your own necessities.

16:37.840 --> 16:39.680
Scratch your own itches if you want.

16:39.680 --> 16:44.000
Yeah, well, so from the beginning of time in machine learning, there was this notion

16:44.000 --> 16:50.160
that you would have a data set that you train on, a data set that you evaluate or test on,

16:50.160 --> 16:52.080
and then the data in the wild.

16:52.880 --> 16:59.840
And so it turns out that this, the earliest we found of this practice goes back to the

16:59.840 --> 17:00.880
late 1950s.

17:00.880 --> 17:02.880
It's a really old idea.

17:03.920 --> 17:08.480
And it's kind of, it's kind of amazing that it was, it was, I mean, a lot of people think

17:08.480 --> 17:10.560
of that as an idea that comes from statistics.

17:10.560 --> 17:13.360
And it's an idea that comes from machine learning before statistics.

17:13.360 --> 17:14.640
It's interesting how early that is.

17:14.640 --> 17:15.680
Hashtag today I learned.

17:16.560 --> 17:20.160
Well, I mean, I think, I think one thing that I've learned, and something I never appreciated

17:20.160 --> 17:27.200
before was, before recently was, you know, that machine learning and statistics are much

17:27.200 --> 17:29.440
more contemporary than I think we realize.

17:29.440 --> 17:33.520
I think we, a lot of people think statistics is very old, because probability is very old.

17:34.400 --> 17:39.440
But statistics is kind of the, you know, the appropriate application of probabilistic thinking

17:39.440 --> 17:42.400
to how we interact with the real world.

17:42.400 --> 17:48.480
And I think those kinds of real aspects weren't nailed down to the 30s, 1930s, which is just

17:48.480 --> 17:49.040
fascinating.

17:49.040 --> 17:55.120
I mean, there was work by, you know, notably Ronald Fisher and some of his predecessors

17:55.440 --> 17:59.280
and Gossett, who was the guy who invented the t-test.

17:59.280 --> 18:01.200
This was done in the 20s or so.

18:01.200 --> 18:05.600
But that wasn't really until the 30s, where there was like a real rigorous body of statistics,

18:05.600 --> 18:09.040
notably by jersey name and kind of nailed that down.

18:09.040 --> 18:11.840
But, and it's kind of crazy to think that that's the 30s.

18:11.840 --> 18:15.280
It's like, you would think that that would be the 1830s, but it's the 1930s.

18:15.280 --> 18:19.040
It's much, much later than, than I think people appreciate.

18:19.280 --> 18:26.480
So machine learning is roughly 1950s, and really was well established by 1960.

18:26.480 --> 18:31.440
So it's only a gap of, you know, what are we talking about here, like 25 years, not

18:31.440 --> 18:35.440
decades, not sorry, not like, you know, it's insane, centuries, it was really a gap of

18:35.440 --> 18:36.320
about 25 years.

18:36.320 --> 18:36.800
Yeah.

18:36.800 --> 18:41.200
To the full foundation of statistical machine learning, and the full foundation of statistics,

18:41.200 --> 18:42.800
they're very intertwined with each other.

18:42.800 --> 18:46.800
So I think that's where does machine learning start for you with a perceptron?

18:47.520 --> 18:51.520
Um, with the kind of like the understanding of how the perceptron works.

18:51.520 --> 18:52.160
Okay.

18:52.160 --> 18:55.360
And it's not, it's not just the perceptron, because there are other things that people

18:55.360 --> 19:00.480
were experimenting with at the time, lots of different even like model based methods,

19:01.280 --> 19:06.000
if you kind of go back to literature, but really just the idea of understanding that

19:06.000 --> 19:10.480
there's this, you know, in sample versus out sample performance, I could characterize in

19:10.480 --> 19:12.720
sample performance and out of sample performance.

19:13.520 --> 19:19.280
And so kind of particular way, this was really, this is late 1950s, through early 1960s was

19:19.280 --> 19:20.960
where these ideas really came together.

19:20.960 --> 19:21.200
Yeah.

19:21.200 --> 19:25.680
And going back to the myth that you actually want to bust here, am I understanding correctly

19:25.680 --> 19:29.040
that what you say is that we should overfit?

19:29.040 --> 19:30.720
Or we shouldn't be scared to overfit?

19:30.720 --> 19:31.040
Or?

19:31.040 --> 19:31.280
No.

19:32.240 --> 19:34.480
Well, I think it's that we shouldn't be scared to overfit.

19:34.480 --> 19:38.640
So one of the things that people kind of thought and statisticians have said this is that if

19:38.640 --> 19:44.400
you, you look at what we're doing, where we have, you know, some data set, and some

19:45.120 --> 19:49.840
testing set, and we're just always, you know, using the this one data set, and then evaluating

19:49.840 --> 19:54.080
on the testing set, and repeating this process, you know, you could construct really weird

19:54.080 --> 19:59.920
examples where you essentially overfit to this testing set, and then you get some new

19:59.920 --> 20:02.560
data, and you've just done you just do terribly.

20:02.560 --> 20:05.600
So there's, there are ways to construct examples where that happens.

20:06.240 --> 20:12.080
And in my group, we've done now just hundreds and hundreds of experiments, we're showing

20:12.080 --> 20:13.280
that that just doesn't happen.

20:14.240 --> 20:19.360
So we've done a lot of reproduction studies where we'll reproduce test sets, trying to

20:19.360 --> 20:23.360
be as close as possible to the generating process of the original test set.

20:23.360 --> 20:28.080
And we've done this for data sets that like the ImageNet data set, or the squad data set,

20:28.080 --> 20:32.560
or a variety of others where we can regenerate these kind of test sets.

20:33.280 --> 20:42.800
And we see that you don't there's no overfitting the models that perform the best on the original

20:42.800 --> 20:45.760
test set are the ones that perform the best on the new data.

20:46.640 --> 20:48.960
So completely goes against what you were told.

20:48.960 --> 20:50.720
So you can't How do you explain this?

20:53.440 --> 20:54.640
That is a wonderful question.

20:57.040 --> 20:59.840
I don't I wish I had a good answer for that.

20:59.840 --> 21:04.400
I think that's, to me, I mean, sometimes there's like, we almost expect too much from

21:04.400 --> 21:05.040
our theory.

21:05.040 --> 21:05.540
Okay.

21:05.920 --> 21:06.640
And that's okay.

21:08.080 --> 21:13.120
I think there are a variety of there are a variety of explanations.

21:13.920 --> 21:19.440
Some of them, some of them are mathematical, and then some of them are just sociological,

21:19.440 --> 21:19.940
I think.

21:20.960 --> 21:25.120
I think that, you know, we pick problems very carefully in machine learning, such that they're

21:25.120 --> 21:26.720
not hurt by overfitting.

21:26.720 --> 21:27.840
I think that's part of it.

21:27.840 --> 21:32.720
Like we're, the problems that we're looking at tend to be not the not the weird, egregious

21:32.720 --> 21:36.160
examples that we come up with as counter examples.

21:36.720 --> 21:37.840
So I think that's one part.

21:38.880 --> 21:45.120
I think another part is that there are a lot of the analysis that would predict overfitting

21:45.120 --> 21:51.440
assume some very pathological behavior in the statistics to happen.

21:51.440 --> 21:55.760
And, you know, the way that we actually work, there are a variety of different ways that,

21:55.760 --> 22:02.400
you know, if, for example, if predictors kind of are correlated, like they kind of

22:02.400 --> 22:06.480
give you correlated answers, you know, the bounds on overfitting get better.

22:06.480 --> 22:08.800
So it looks like you can actually have more models.

22:09.840 --> 22:18.240
If you don't really hyper optimize to the test error score, that also can kind of prevent

22:18.240 --> 22:18.720
overfitting.

22:18.720 --> 22:20.800
So there's a variety of like these little tricks.

22:20.800 --> 22:24.000
And maybe each one of those is explains a little piece.

22:24.800 --> 22:25.760
But I don't think there is.

22:26.640 --> 22:32.880
Right now, I don't have a satisfactory explanation for why we don't actually see this kind of

22:32.880 --> 22:34.240
overfitting behavior.

22:34.240 --> 22:37.280
I'll say this, though, the reason why I'm not spending too much time worrying about

22:37.280 --> 22:42.640
it, because the other thing we observed is that while you see the models that perform

22:42.640 --> 22:48.320
really well on the new on the old tests that perform really well, on the newer data, they

22:48.320 --> 22:54.720
almost always perform worse on the newer data than they did on the original old data.

22:55.760 --> 22:58.160
So now everybody's performing worse.

22:58.720 --> 23:02.480
It's not that, you know, so again, it's not that we overfit necessarily.

23:02.480 --> 23:09.600
It's just that small changes in the data creation process can create large changes in the actual

23:09.600 --> 23:10.720
predictive performance.

23:10.720 --> 23:11.520
And that's worrying.

23:12.160 --> 23:13.840
That's not really an issue of overfitting.

23:13.840 --> 23:15.520
That's just an issue of extrapolation.

23:16.320 --> 23:23.200
And it's interesting how sensitive machine learning methods can be to small changes in

23:23.200 --> 23:25.120
how data presents itself.

23:26.080 --> 23:28.480
There are so many directions that I can take off from here.

23:28.480 --> 23:34.080
I mean, well, first of all, I guess the first naive comment on this issue is that it seems

23:34.080 --> 23:41.200
to me that this could be an issue of generosity in some way, or like the data that we get

23:42.000 --> 23:48.720
or better, the way we build models on the data that we get somehow do not suffer from

23:48.720 --> 23:51.520
some sort of generosity problem and vice versa.

23:52.240 --> 23:56.560
We may be too sensitive to the data somehow.

23:56.560 --> 24:01.840
And so therefore, we get worse errors on new data whenever we get new data.

24:02.720 --> 24:03.220
Yeah.

24:04.320 --> 24:06.800
Would this be fair to say from your point of view?

24:07.520 --> 24:10.080
So I don't claim to have answers for this.

24:10.080 --> 24:11.680
So I don't know yet.

24:11.680 --> 24:16.640
But I do think there are some things that we don't necessarily consider very frequently

24:16.640 --> 24:23.120
in machine learning that we should, which is just that they're all of experimental science

24:23.120 --> 24:25.120
kind of suffers from this generalization problem.

24:26.640 --> 24:31.040
There are things that you do in a laboratory setting that you can go do out in a clinical

24:31.040 --> 24:34.000
setting, and they just don't pan out the way you want them to.

24:34.800 --> 24:35.040
Right.

24:35.040 --> 24:39.360
I mean, I think this is kind of very, very common in medicine, where you do, you know,

24:39.360 --> 24:42.880
you build this drug, it looks really good, you go and deploy it, and it's not quite as

24:42.880 --> 24:43.920
good as you want it to be.

24:45.120 --> 24:49.920
And this is also true in psychology, where you do a study, and, you know, you do a study

24:49.920 --> 24:55.280
on a bunch of college aged males, perhaps, and then you try to see does this apply to,

24:55.280 --> 24:59.840
you know, middle aged women, and it doesn't, because that's obviously a huge enough context

24:59.840 --> 25:00.800
shift that you see it.

25:02.320 --> 25:08.640
So on the one hand, understanding how to deal with those shifts between contexts and

25:08.640 --> 25:13.360
populations, from the experimental setting, to the general setting is hugely important.

25:13.360 --> 25:18.400
And something very like that, that's something that we're doing a lot of active research on

25:19.120 --> 25:19.680
in my group.

25:20.240 --> 25:21.520
So that's one thing.

25:21.520 --> 25:26.640
I think the warning, though, that's still very troubling, is that in the machine learning

25:26.640 --> 25:29.760
context, these data sets don't really look that different.

25:29.760 --> 25:33.280
And you see a large difference, the fact that you could have two data sets that kind of

25:33.280 --> 25:37.600
look the same, and have a huge difference in performance, really freaks me out about

25:37.600 --> 25:38.320
machine learning.

25:38.320 --> 25:41.440
Indicating some sort of high sensitivity, somehow.

25:41.440 --> 25:43.120
Super high sensitivity, yeah.

25:43.120 --> 25:48.000
And what people do in practice, and so we can tell our listeners, you know, the fix

25:48.000 --> 25:50.960
for this in practice is constant retraining.

25:50.960 --> 25:52.160
And there's a lot of that.

25:52.160 --> 25:56.400
Could you explain, like, give an analogy of what that means, maybe, for the audience?

25:56.400 --> 26:02.320
So this would be the kind of thing where you constantly look for your, you know, as you

26:02.320 --> 26:08.400
have something deployed, you constantly look for errors in your deployed system.

26:08.400 --> 26:12.560
And as soon as you see them, or even maybe not if you see them, you're still collecting

26:12.560 --> 26:15.120
data, and you use that data to retrain the model.

26:15.920 --> 26:20.000
And this is definitely a very commonplace way to get around the sensitivity, and something

26:20.000 --> 26:25.200
that's very common in industrial applications of machine learning.

26:25.200 --> 26:28.160
You do have to refine the models, you do have to retrain the models.

26:28.880 --> 26:33.360
They're very unstable to the ravages of time.

26:34.000 --> 26:39.280
Maybe for the control theoretically oriented audience out there, I suppose that we are

26:39.280 --> 26:42.240
kind of speaking of robustness in this guys.

26:42.240 --> 26:43.600
Am I wrong?

26:43.600 --> 26:44.720
Probably, probably.

26:44.720 --> 26:50.480
I think one thing that's interesting that is missing, and that changes things pretty

26:50.480 --> 26:52.640
dramatically, is feedback.

26:53.760 --> 26:57.440
So these predictive things, you know, you're at the whim of the prediction.

26:58.240 --> 27:01.120
So you're kind of, you're never able to really course correct.

27:01.120 --> 27:06.240
I think one of the fascinating things about control is we know that even with poor prediction,

27:06.240 --> 27:12.080
we can have stable and robust behavior by proper design and feedback design, right?

27:12.080 --> 27:16.880
And that's one of the more fascinating, underappreciated aspects of control.

27:16.880 --> 27:24.960
And you're giving me another assist to shift gears and talk about your excursion into control

27:24.960 --> 27:25.840
theory, I suppose.

27:25.840 --> 27:26.340
Sure.

27:27.060 --> 27:33.700
You've written a fantastic series of blog posts on Argmin about reinforcement learning

27:33.700 --> 27:36.900
and its connection to optimal control.

27:36.900 --> 27:41.540
And this turned out to be also a fantastic paper on the annual reviews on control.

27:41.540 --> 27:44.180
Maybe we can speak about that if you like.

27:44.180 --> 27:45.460
Absolutely, absolutely.

27:47.380 --> 27:54.180
So summarizing your journey in this area would be impossible in a few seconds.

27:54.740 --> 28:01.460
But maybe if it's not too much to ask to you, could you give us a sense of your personal

28:01.460 --> 28:03.620
perspective on your journey in control, maybe?

28:05.380 --> 28:05.940
Yeah.

28:05.940 --> 28:10.020
So this is now, I have to think a little, I have to think back to it.

28:10.020 --> 28:13.140
I used to have a good story for how I got into it again.

28:13.140 --> 28:18.340
I mean, I'm assuming that what we're talking about is, you know, my recent engagement with

28:19.380 --> 28:20.420
reinforcement learning.

28:20.420 --> 28:20.660
Yes.

28:20.660 --> 28:26.020
Because I've, you know, I had, I did some work on distributed control as a graduate

28:26.020 --> 28:26.500
student.

28:26.500 --> 28:28.340
I've done some work on system identification.

28:29.380 --> 28:33.460
As a postdoc, I go back and forth with control and dynamical systems.

28:34.500 --> 28:37.860
I'm trying to remember exactly what got me interested.

28:39.940 --> 28:42.420
Because it's been so long, but I'm guessing what happened.

28:42.420 --> 28:44.500
I'll just again, this is a little bit of a guess.

28:44.500 --> 28:51.460
But I'm just trying to try to put myself back in a 2014 mindset is there were a few

28:51.460 --> 28:52.260
things that were afoot.

28:52.260 --> 28:56.980
First of all, there was a lot of hype around reinforcement learning in 2014.

28:56.980 --> 28:59.460
And it is crazy that that's now eight years ago.

29:00.020 --> 29:01.380
That's really wild to me.

29:01.380 --> 29:02.020
Yeah.

29:02.020 --> 29:02.980
It definitely did not.

29:02.980 --> 29:07.860
I don't think it actually ended up paying out the way that people were talking, the

29:07.860 --> 29:11.220
way they were talking about it, like self-driving cars in 2020.

29:11.220 --> 29:14.500
They were definitely saying that in 2014, and it just didn't happen.

29:15.300 --> 29:19.220
And it'd be nice if they could actually, you know, reflect on that.

29:19.220 --> 29:21.060
But I think they've moved on to Bitcoin or something.

29:21.060 --> 29:21.940
They're onto something new.

29:21.940 --> 29:22.500
I don't know.

29:22.500 --> 29:23.860
Elon Musk is buying Twitter.

29:23.860 --> 29:24.260
I don't know.

29:24.260 --> 29:25.140
We're doing other stuff.

29:26.100 --> 29:27.060
We're moving on.

29:28.580 --> 29:32.660
But I think it would be helpful to have some reflection on why didn't that pay out.

29:32.660 --> 29:34.660
We're here also for that purpose.

29:34.660 --> 29:37.300
I mean, locomotion was another big one, I suppose.

29:37.620 --> 29:41.060
Yeah, so locomotion, I think, came a little bit later.

29:41.060 --> 29:44.660
But for me, the big things that were, I think, you know, happening at Berkeley was there

29:44.660 --> 29:46.340
was a lot of interest in self-driving cars.

29:46.900 --> 29:49.940
And then there were a few really interesting papers that were fusing.

29:50.580 --> 29:56.500
I was very interested in this paper by Chelsea Finn and Sergey Levine and Peter Abbeel, Trevor

29:56.500 --> 30:01.060
Darrell, that kind of showed that you could like learn these visual, you could do control

30:01.060 --> 30:02.500
directly from video.

30:02.500 --> 30:04.900
That was a very, very impressive.

30:04.980 --> 30:10.180
It's one of the holy grails, I suppose, like merging perception and control in a principled

30:10.180 --> 30:10.420
way.

30:11.140 --> 30:14.260
This is one of the first to do it in a compelling way, I think.

30:14.980 --> 30:16.980
So people were very excited about that.

30:16.980 --> 30:20.660
So I think we just started digging in a little bit in the group about, you know, what's going

30:20.660 --> 30:21.060
on here.

30:21.060 --> 30:22.740
I was like, I know control theory.

30:22.740 --> 30:23.700
I know machine learning.

30:23.700 --> 30:24.740
I should be able to do this.

30:24.740 --> 30:25.700
I think it was something like that.

30:27.060 --> 30:29.860
I was like, also, I didn't believe a lot of what people were selling.

30:29.860 --> 30:32.020
So we started looking into that.

30:32.020 --> 30:36.420
And yeah, it led us down quite a path from there, I think.

30:36.420 --> 30:36.900
Yeah.

30:36.900 --> 30:43.220
I mean, one of the cool things that ended up happening was that of merging or at least

30:43.220 --> 30:50.100
connecting the world of robust control and with that of high dimensional statistics.

30:50.100 --> 30:55.940
And I think that was a major achievement from my very humble perspective in the sense that

30:57.300 --> 31:01.460
robust control tends to assume that uncertainties are somehow bounded.

31:02.020 --> 31:05.780
But we never really explain where those bounds come from.

31:05.780 --> 31:14.100
Whereas maybe machine learning can kind of help us in estimating those uncertainties

31:14.100 --> 31:19.940
from finite sample statistics, finite sample, finite data, if you want.

31:21.140 --> 31:25.380
Would that make sense to you as a short summary of your excursion?

31:25.380 --> 31:26.260
No, absolutely.

31:27.300 --> 31:28.100
That's part of it.

31:28.260 --> 31:30.180
I think definitely there was a...

31:31.140 --> 31:35.460
I mean, the other way I'd say that is that machine learning is also very bad about quantifying

31:35.460 --> 31:36.740
the uncertainty in its predictions.

31:37.540 --> 31:38.180
It's terrible.

31:39.300 --> 31:45.860
So it's this kind of thing where if you assume a lot of things about a generative process,

31:46.420 --> 31:51.140
if you assume a lot of things about how the data is coming to you, I could give you some

31:51.140 --> 31:53.140
kind of uncertainty quantification.

31:53.140 --> 31:56.580
But you can never check whether or not those things you're supposed to assume are true.

31:57.540 --> 32:00.660
It's actually, I think one of these things that's a little worrying sometimes in machine

32:00.660 --> 32:04.180
learning is this reliance on probability.

32:05.460 --> 32:11.780
I think, honestly, this is a John Doyle thing that was hammered into me, is that you should

32:11.780 --> 32:13.700
never add stochastics until you need to.

32:14.340 --> 32:14.820
Okay.

32:14.820 --> 32:16.420
Which I think is a very interesting perspective.

32:16.420 --> 32:18.020
Don't add stochastics until you need to.

32:18.580 --> 32:19.080
And...

32:19.700 --> 32:21.220
So when is it that you need to then?

32:22.340 --> 32:23.460
That's the hard question.

32:24.260 --> 32:28.100
So in machine learning, we don't have any theory without some kind of notion of...

32:30.740 --> 32:31.240
Well...

32:31.620 --> 32:34.260
A fully deterministic machine learning.

32:34.260 --> 32:37.700
So, no, no, there is actually, and this is actually really fascinating.

32:37.700 --> 32:39.460
There is a fully deterministic machine learning.

32:39.460 --> 32:40.820
We just don't really appreciate it.

32:41.940 --> 32:44.180
It's just because it's not accessible.

32:44.180 --> 32:48.260
I'll say this, I love my friends who work on this stuff, but it's not very accessible.

32:48.260 --> 32:51.460
So there is a notion in machine learning that's called regret.

32:52.260 --> 32:52.740
Yes.

32:52.740 --> 32:58.100
And regret has this idea that what you will look at is, you know, you'll have some sequence

32:58.100 --> 33:02.020
and you'll say, you know, this is the best thing that could have been predicted.

33:02.020 --> 33:07.220
The best predictions you could have made about the sequence, somehow, if you were omniscient,

33:07.220 --> 33:09.140
or like you saw the whole sequence in advance.

33:10.500 --> 33:14.260
And then you compare that to how well your algorithm makes predictions.

33:14.260 --> 33:14.500
Okay.

33:14.500 --> 33:18.660
So you have something like, if I somehow saw the whole sequence, what would be the best

33:18.660 --> 33:20.020
predictor I could build?

33:20.020 --> 33:21.620
Which would be funny, right?

33:21.620 --> 33:25.300
You're somehow saying, I saw everything that happened, and now you're going to tell me,

33:25.300 --> 33:30.180
how could I predict, you know, Tuesday from Monday, given that I've already seen Wednesday,

33:30.180 --> 33:30.900
Thursday, Friday.

33:30.900 --> 33:35.300
It's a little bit of a confusing setup, but that's a very strong assumption, right?

33:35.300 --> 33:39.940
So assuming that you could see all of that future information, what would be the best

33:39.940 --> 33:41.540
prediction you could have ever made?

33:41.540 --> 33:42.580
And that's deterministic.

33:42.580 --> 33:48.180
It only depends on the actual values that you saw Monday, on Tuesday, on Wednesday.

33:48.180 --> 33:49.380
There's no distribution.

33:49.380 --> 33:50.340
There's no sampling.

33:50.900 --> 33:51.620
Nothing like that.

33:52.260 --> 33:59.620
I found absolutely interesting the linearization principle, by which you say that if a machine

33:59.620 --> 34:04.660
learning algorithm does crazy things when restricted to linear models, it's going to

34:04.660 --> 34:07.300
do crazy things on complex nonlinear models too.

34:07.300 --> 34:12.660
And I think that if we remove machine learning and just say control, probably we get the

34:12.660 --> 34:14.100
same output.

34:14.100 --> 34:15.140
I think it's similar.

34:15.140 --> 34:18.660
And honestly, Aizerman is kind of like, was a bit of an inspiration for that as well.

34:18.660 --> 34:23.940
I mean, I feel like a lot of the, I know Aizerman was talking about feedback with,

34:24.820 --> 34:30.900
now we're getting into the weeds of memoryless nonlinearity, but I think that this, we learned

34:30.900 --> 34:37.220
a lot about optimization from kind of looking at Aizerman's conjecture.

34:37.220 --> 34:42.180
And so I think there is this, that interplay happens everywhere.

34:42.180 --> 34:43.540
I think it just happens everywhere.

34:43.540 --> 34:47.380
Sometimes you just, you're surprised to see how many insights you can get from linear

34:47.380 --> 34:49.220
systems or from linear regression.

34:49.860 --> 34:50.500
Absolutely.

34:50.500 --> 34:55.540
I'm just going to mention for our audience that Aizerman is very well known for a conjecture,

34:56.180 --> 35:03.380
an eponymous conjecture related to the stability of a feedback loop that involves a nonlinearity

35:03.380 --> 35:05.060
plus some kind of bounds.

35:05.060 --> 35:07.460
You can Google that, probably find it on Wikipedia.

35:08.020 --> 35:08.260
Yeah.

35:08.260 --> 35:14.740
And touching on this point, I suppose the running example of your whole enterprise in

35:14.740 --> 35:16.180
control has been LQR.

35:18.180 --> 35:18.900
Better for worse.

35:19.860 --> 35:20.900
For better, for worse.

35:20.900 --> 35:21.620
Yeah.

35:21.620 --> 35:29.300
And it was just so fascinating to see, I mean, how much else can we say about it, at least

35:29.300 --> 35:35.860
regarding this somewhat old problem with new eyes, with somebody, with the eyes of someone

35:35.860 --> 35:41.300
who has been extensively working also with other communities, such as machine learning.

35:41.380 --> 35:47.380
And I guess your main finding in this respect would be that certainty equivalents work.

35:48.100 --> 35:49.060
That's a big one.

35:49.060 --> 35:49.700
I think, let's see.

35:50.420 --> 35:54.420
So yeah, my group did a lot of stuff on LQR.

35:56.260 --> 35:58.740
I will say that, like, I love that certainty.

35:58.740 --> 36:02.420
Let me come back to certainty equivalence, because I do love that result.

36:02.420 --> 36:08.580
But I think the thing that we got the whole group super excited was Nikolai Motny, who's

36:08.580 --> 36:12.740
now a faculty member at Penn, visited us as a postdoc.

36:13.700 --> 36:19.860
And we had been really trying to figure out how to understand, well, what do you do when

36:19.860 --> 36:23.460
you have this kind of bad fit LQR model and you do control?

36:23.460 --> 36:25.540
And what's the right way to analyze it?

36:25.540 --> 36:30.260
Like, how can you even talk about how to analyze this thing where, you know, you're doing some

36:30.260 --> 36:31.700
kind of reinforcement learning?

36:31.700 --> 36:32.660
Do I fit the model?

36:32.660 --> 36:33.620
Do I do something else?

36:34.420 --> 36:40.820
And my students, Sarah Dean, Hori Amania, Stephen Tu, figured out a cool way to, like,

36:40.820 --> 36:45.700
kind of quantify the uncertainty when you fit an LQR model to data.

36:45.700 --> 36:48.900
And they have really, they've done a lot since on that, too.

36:48.900 --> 36:53.460
So they really, but we couldn't figure out what you do with that certainty equivalence.

36:53.460 --> 36:55.220
And so we tried a bunch of different ideas.

36:56.180 --> 36:57.540
Sorry, not with certainty equivalence.

36:57.540 --> 36:58.020
Apologies.

36:58.020 --> 36:58.660
Let me step back.

36:58.660 --> 37:01.460
What do you do with that uncertainty quantification, right?

37:02.180 --> 37:07.460
And so now you have this thing where I have a linear system with uncertain parameters,

37:07.460 --> 37:10.900
and we tried to use a bunch of different ideas from robust control.

37:11.700 --> 37:15.140
And we kept running into walls in the analyses.

37:16.180 --> 37:20.900
You know, we worked, we were talking a lot with Andy Packard, and we had a lot of interesting

37:20.900 --> 37:24.660
ideas, like mu synthesis related ideas that maybe could have worked.

37:24.660 --> 37:26.180
There's a lot of the stuff that could have worked.

37:26.180 --> 37:30.820
But what was mind blowing is, you know, Nikolai came and within two weeks, he applied this

37:30.820 --> 37:34.500
stuff that he had been working on, which he calls system level synthesis.

37:34.500 --> 37:41.060
And yeah, within two weeks, we had amazing theorems that showed that how to do it.

37:41.060 --> 37:47.860
You know, we could quantify how many samples you needed to do robust control of a system

37:47.860 --> 37:52.980
where you have a, you know, you want to do quadratic LQR, you want to minimize the quadratic

37:52.980 --> 37:55.220
cost, you have uncertain linear dynamics.

37:55.220 --> 37:59.700
And we could quantify how many samples you needed to get an uncertainty quantification

37:59.700 --> 38:04.420
of the system that was good enough to do near optimal control.

38:04.420 --> 38:06.020
Really fascinating result.

38:06.660 --> 38:11.140
Maybe we can step back here just to help the audience a little bit and just mention what

38:11.140 --> 38:12.340
is SLS.

38:12.340 --> 38:13.700
I am aware of what that is.

38:14.340 --> 38:20.980
But maybe you can help me with an analogy for the audience, like to explain what that

38:20.980 --> 38:22.180
does and what it means.

38:22.900 --> 38:29.540
Yeah, so system level synthesis, I think there are multiple ways to kind of think about what

38:29.540 --> 38:35.940
that, how we approach that problem, or how we approach thinking about that problem.

38:37.060 --> 38:40.740
And I'm trying to think what's the best one that the control theoretic audience will

38:40.740 --> 38:41.620
respond to.

38:41.620 --> 38:44.660
I mean, as always is the case of control, people are going to say, oh, we did it this

38:44.660 --> 38:45.700
way in the 80s.

38:45.700 --> 38:47.060
Oh, we did this way in the 60s.

38:47.060 --> 38:49.140
I don't want to necessarily step on anyone's toes.

38:49.140 --> 38:56.260
Let me just explain roughly what this is thinking is you can think about most of the internal

38:56.260 --> 39:00.820
maps in control systems as mapping from a disturbance, which consists of your initial

39:00.820 --> 39:06.020
state and the actual disturbance signal to either the state.

39:07.140 --> 39:10.740
Controller also is just a map from the disturbance to the control.

39:11.620 --> 39:15.380
And system level synthesis works directly with those maps, the map that maps disturbance

39:15.380 --> 39:17.620
to state, the one that maps disturbance to control.

39:18.900 --> 39:26.020
And just gives you a very straightforward linear algebraic approach to doing

39:26.020 --> 39:30.500
control synthesis where you're working with these abstract maps rather than working with

39:32.740 --> 39:35.940
the actual parameters A, B, and C that you would have in a linear system.

39:35.940 --> 39:37.140
Now, why is that better?

39:37.700 --> 39:42.100
It's just this, in effect, lifts everything into a higher dimensional space where everything

39:42.100 --> 39:43.140
becomes nice.

39:43.140 --> 39:47.700
Problems that weren't convex become convex, which is why we find it really useful.

39:48.900 --> 39:50.580
Why is this the right mapping?

39:50.580 --> 39:51.060
I don't know.

39:52.180 --> 39:52.900
I don't know.

39:52.980 --> 39:58.660
I think another way to think about it, if you're working in finite time, is that the

39:58.660 --> 40:02.100
map from disturbance to state is going to be something like Toplitz matrix with a bunch

40:02.100 --> 40:03.140
of parameters in it.

40:03.140 --> 40:06.260
And you just have to understand that those parameters can't be arbitrary.

40:06.260 --> 40:08.260
They're going to have some relationship.

40:08.260 --> 40:10.100
Essentially a finite impulse response.

40:10.100 --> 40:10.420
Yeah.

40:10.420 --> 40:15.140
And so it's working with those impulse responses and putting these things together in a very

40:15.140 --> 40:15.940
clean fashion.

40:15.940 --> 40:18.260
So it's surprisingly powerful.

40:18.260 --> 40:20.900
And these ideas do appear in other places.

40:20.900 --> 40:25.220
I believe in model predictive control, this is called disturbance feedback.

40:25.220 --> 40:26.260
I think that's the name.

40:26.260 --> 40:31.380
There's definitely something where you operate with this map that maps the error disturbance

40:31.380 --> 40:34.100
into your state and the error disturbance into your controller.

40:34.100 --> 40:38.500
So yeah, that's the high level.

40:38.500 --> 40:41.620
I always feel like it's almost the thing that's funny about it.

40:41.620 --> 40:42.180
I love it.

40:42.180 --> 40:43.940
I think it's very easy to work with.

40:43.940 --> 40:45.940
I could probably teach people how to operate with it.

40:46.500 --> 40:51.460
But it kind of led to the second thing, which came a little bit later, which was work by

40:51.460 --> 40:57.540
Jorge Amania and Steven Tu, which I think it was kind of, it's not surprising, but this

40:57.540 --> 41:00.980
is one of these wonderful things where theory and practice collide.

41:00.980 --> 41:05.220
So what we saw with the SLS is that when you have very little data, you did need to have

41:05.220 --> 41:09.460
some kind of quantification of the uncertainty in order to get stable performance.

41:10.260 --> 41:14.420
But as you start to just get a reasonable amount of data, not too much, just a reasonable

41:14.420 --> 41:19.540
amount of data, what you could just do is fit the model using least squares and then

41:19.540 --> 41:25.140
plug the model in as if it was true and run your optimal controller with this estimated

41:25.140 --> 41:27.140
model as if the estimated model was true.

41:27.780 --> 41:28.020
Right?

41:28.020 --> 41:33.620
So this is the, whenever I say, as if the estimated model was true, or as if the estimated

41:33.620 --> 41:36.580
X was true, we tend to call that certainty equivalence, right?

41:36.580 --> 41:39.780
It's assuming a certain level of uncertainty is not there.

41:39.780 --> 41:41.140
This is the certainty equivalent model.

41:41.140 --> 41:42.980
Assuming there was no noise, this would be what the-

41:42.980 --> 41:45.140
Optimism in the face of uncertainty.

41:45.140 --> 41:47.780
It's a form of, well, that's interesting.

41:48.500 --> 41:51.860
Optimism in the face of uncertainty is a little bit more aggressive than certainty equivalence.

41:53.060 --> 41:54.660
Optimism in the face of uncertainty.

41:54.660 --> 41:56.340
So essentially these three things are different.

41:56.340 --> 42:00.660
So the robust control view is you assume you have data, there's going to be a bunch of

42:00.660 --> 42:02.420
models consistent with your data.

42:02.420 --> 42:06.020
And you're saying, I'm going to pick the one, the robust control view is I'm going to pick

42:06.020 --> 42:10.020
the model that's consistent with my data, but would give me the worst performance.

42:10.020 --> 42:11.380
That's robust control.

42:11.380 --> 42:13.140
Very pessimistic, right?

42:13.140 --> 42:16.020
And they're going to say, well, given that, then what, how do we do design?

42:17.140 --> 42:19.780
Certain equivalence just says, I'm going to ignore the uncertainty.

42:20.340 --> 42:21.380
Just pretend it's not there.

42:21.380 --> 42:22.100
And then what would I do?

42:23.460 --> 42:27.300
Optimism in the face of uncertainty says, given all of the models that are consistent

42:27.300 --> 42:30.340
with my data, I'm going to pick the one that gives me the best performance.

42:31.140 --> 42:31.380
Okay.

42:31.940 --> 42:33.540
Which is incredibly aggressive.

42:33.540 --> 42:38.580
And the idea there is you, you, you, you know, either you're right, in which case, great,

42:39.220 --> 42:40.260
or you're wrong.

42:40.260 --> 42:41.380
And you learn that quickly.

42:41.380 --> 42:44.980
That's a nice way to put it.

42:44.980 --> 42:46.340
Yeah, that's the idea.

42:46.340 --> 42:51.140
But also, you tell that to a control engineer, and they get a little terrified, I think.

42:51.140 --> 42:53.860
They tend to be quite conservative.

42:53.860 --> 42:54.580
Yeah, yeah, yeah.

42:54.580 --> 42:56.180
Nobody wants to, you don't want to fail quickly.

42:56.180 --> 42:56.980
That doesn't seem good.

42:58.420 --> 43:01.380
But so just to go back to the certainty equivalence very quickly.

43:01.380 --> 43:01.780
Yeah, sure.

43:01.780 --> 43:02.340
Absolutely.

43:02.340 --> 43:03.700
So what did we do there?

43:04.420 --> 43:10.420
So this idea where you just, you know, fit some ARMA model to your data, and then pretend

43:10.420 --> 43:15.460
if it was true, it's kind of a staple of engineering, right?

43:15.460 --> 43:18.260
I mean, people do that all the time in control engineering.

43:18.260 --> 43:23.140
And what Jorge and Stephen showed, which is remarkable, is that once you have enough data,

43:23.140 --> 43:24.420
that is the optimal thing to do.

43:25.140 --> 43:27.700
Not only is it a good thing to do, it's the optimal thing to do.

43:28.420 --> 43:30.020
And in fact, that does suggest something.

43:30.820 --> 43:35.940
The first clause I said was, if you have enough data, what it suggests is in the lab, do as

43:35.940 --> 43:37.060
many experiments as you can.

43:37.780 --> 43:42.420
Always be sure that you have enough data, because you're almost never in a lab setting

43:42.420 --> 43:43.220
resource constraint.

43:43.220 --> 43:45.220
So collect as much data as you can.

43:45.860 --> 43:50.260
And then once you do that, you can deploy this model, you could ignore the uncertainty,

43:50.260 --> 43:50.980
and you'll be fine.

43:51.780 --> 43:52.980
It's kind of amazing.

43:52.980 --> 43:57.860
And I do think that basically, I mean, I don't want to, again, don't want to step on any

43:57.860 --> 44:02.980
engineer's toes, but so many engineers who are working on mission critical systems just

44:02.980 --> 44:03.540
do that.

44:03.540 --> 44:04.660
And it's okay.

44:04.660 --> 44:08.980
And I think it's interesting to justify 50 years of practice with a theorem.

44:08.980 --> 44:09.460
It's nice.

44:10.100 --> 44:10.420
Yeah.

44:10.420 --> 44:14.340
Plus the fact that we somehow flew to the moon in the 70s or 60s.

44:14.340 --> 44:14.840
Right?

44:16.100 --> 44:17.220
No, we used feedback.

44:17.220 --> 44:18.740
And so the feedback was important.

44:18.740 --> 44:22.980
But I think it's just kind of understanding how sensitive we really are to those kind

44:22.980 --> 44:23.860
of modeling errors.

44:24.820 --> 44:29.860
And it's I think what this is always one thing that's amazing and fascinating control is

44:30.740 --> 44:34.900
you can interpret the robust control mindset is assuming we're going to be sensitive to

44:34.900 --> 44:35.400
everything.

44:36.260 --> 44:37.620
But I don't think that's fair.

44:37.620 --> 44:42.340
I think robust control at its best is just telling you what you should be looking out

44:42.340 --> 44:42.840
for.

44:43.940 --> 44:44.260
Right?

44:44.260 --> 44:46.740
It's not saying don't don't do anything.

44:46.740 --> 44:49.780
That's the most robust, most robust thing is just don't.

44:50.340 --> 44:54.660
But it's just saying these are the these are some pitfalls that you might not see.

44:54.660 --> 44:56.900
And can we identify where those pitfalls pitfalls are?

44:57.540 --> 45:04.820
One thing that I wanted to talk about is perhaps your outsider's view of control and also

45:04.820 --> 45:05.540
of machine learning.

45:05.540 --> 45:08.260
You've been bouncing back and forth between all these fields.

45:08.260 --> 45:11.060
And I think this is one of your great strengths.

45:11.060 --> 45:15.700
So I was wondering whether you see something that control people should be excited about

45:16.420 --> 45:17.140
for the future.

45:17.780 --> 45:20.340
And in general, what's the future of all these fields?

45:20.340 --> 45:22.900
How will they interact?

45:23.780 --> 45:24.740
Yeah, that's a great question.

45:26.020 --> 45:27.780
That's a complicated question.

45:27.780 --> 45:30.020
What should control theory folks be excited about?

45:33.140 --> 45:39.220
The work we did in reinforcement learning, I think, ended up spanning about five years

45:39.220 --> 45:41.220
of pretty dedicated work.

45:41.220 --> 45:46.100
I think I'm fairly certain it was the span of Sarah Dean's PhD.

45:46.500 --> 45:48.260
We started when Sarah arrived.

45:48.260 --> 45:52.020
And I'm pretty sure that maybe it was one year before, but it may have been right when

45:52.020 --> 45:52.980
she arrived.

45:52.980 --> 45:55.780
And then she graduated last year.

45:55.780 --> 45:59.860
And so the group currently is working on very different projects.

46:00.980 --> 46:07.540
And I think a lot of times in research, that's actually a good horizon for a research agenda

46:07.540 --> 46:08.820
is the length of a PhD.

46:08.820 --> 46:13.540
So I mean, Sarah has done so much amazing stuff in that space.

46:13.700 --> 46:18.420
I think she will continue to do so as faculty.

46:18.420 --> 46:24.660
But I think that so that I have currently now disengaged a bit from control, which I

46:24.660 --> 46:25.300
think is okay.

46:26.980 --> 46:31.060
And I don't want to ever tell control theory folks or anybody really what they should be

46:31.060 --> 46:31.780
working on.

46:31.780 --> 46:33.380
I think that's always hard.

46:33.380 --> 46:39.140
But we do benefit from the influx of new ideas and interact with our neighbors.

46:39.140 --> 46:41.620
So we're just very curious about what you're up to.

46:42.420 --> 46:44.660
Yeah, well, I can tell you that's what I was going to move to.

46:44.660 --> 46:46.340
I could tell you what I'm interested in.

46:46.340 --> 46:49.300
I'm not sure that everybody control is yet going to be interested in.

46:49.300 --> 46:53.700
But I think that that's, you know, I think, well, you know, I guess this was just to say,

46:53.700 --> 46:58.100
I'm sure I'm going to come back and write control theory papers again, just taking a

46:58.100 --> 46:59.540
couple years off right now.

46:59.540 --> 47:00.500
And we'll see where it goes.

47:01.380 --> 47:08.340
So what are we like the things I think we're very interested in in our group are precisely

47:08.340 --> 47:17.220
these issues of external validation, understanding how to deal with predictive systems outside

47:17.220 --> 47:23.460
of the laboratory setting, and also kind of understanding how you can, you know, refine

47:23.460 --> 47:30.500
these predictive systems over time, which, to be fair, that has a bit of a control theory

47:30.500 --> 47:31.700
sort of view, right?

47:31.700 --> 47:33.540
There's a feedback process that has to happen.

47:34.340 --> 47:39.460
I think machine learning, the way it's commonly taught is a very static process.

47:39.460 --> 47:43.140
There's data that's static, you apply it to a test set that's static.

47:43.140 --> 47:46.100
And you can run as many times as you want, but nothing changes.

47:46.100 --> 47:48.740
And the assumption is that the new data looks like the old data.

47:48.740 --> 47:49.940
So there's no dynamics.

47:50.580 --> 47:55.460
But how you run, you use machine learning, or just statistical thinking in a dynamic

47:55.460 --> 47:57.540
world is what we're very interested in.

47:57.540 --> 48:00.500
So in some sense, that could touch on control.

48:01.300 --> 48:02.020
Absolutely.

48:02.020 --> 48:05.460
And the issue of causality, I suppose, is rather causal thinking.

48:06.020 --> 48:07.060
Yes, absolutely.

48:07.060 --> 48:10.420
And I was just going to say, though, I think one thing that's challenging for control theorists

48:10.420 --> 48:13.060
is that not everything in dynamics is control theory.

48:13.780 --> 48:15.380
And I think this is always a tricky part.

48:15.380 --> 48:18.100
I think that sometimes we tend to be touchy.

48:18.100 --> 48:19.380
Yeah, we tend to be touchy.

48:19.380 --> 48:22.100
We tend to think that everything that's dynamic means that it's control.

48:22.100 --> 48:28.180
But I feel like, you know, control theorists, or control theory in general, I think should

48:29.140 --> 48:32.340
not oversell its reach, but it also shouldn't undersell.

48:33.220 --> 48:39.380
I think so many profound developments of the 20th century are due to our understanding

48:39.380 --> 48:42.020
of control, and what exactly that is.

48:42.020 --> 48:50.660
And whether that be, you know, frequency domain models of LTI systems, you know, that's powerful

48:50.660 --> 48:50.900
stuff.

48:50.900 --> 48:51.860
Don't sell that short.

48:52.420 --> 48:54.660
That obviously can't tell you about everything.

48:54.660 --> 49:00.980
But it tells you a lot, you know, in a similar way, you know, understanding optimal control

49:00.980 --> 49:01.700
tells you a lot.

49:01.700 --> 49:05.540
But not everything can be posed in that way, even if it's dynamic.

49:05.540 --> 49:07.940
And I think that that's some of the questions we're looking at now.

49:07.940 --> 49:09.460
We're just trying to figure out how do you pose them?

49:09.460 --> 49:11.060
What's the right way to think about them?

49:11.060 --> 49:15.380
And this is what your new series of blog posts, I suppose, will be all about.

49:15.940 --> 49:16.420
Is that right?

49:16.420 --> 49:17.220
We're trying.

49:17.220 --> 49:18.100
That's right.

49:18.100 --> 49:18.740
That's right.

49:18.740 --> 49:19.700
And it's so tricky.

49:19.700 --> 49:24.180
It's so tricky, especially because some of the applications we're interested in involve,

49:24.180 --> 49:26.500
you know, social systems and people.

49:27.540 --> 49:31.540
You know, I think that one of the things that a lot of the folks in the group are interested

49:31.540 --> 49:37.540
in are, you know, where computation and healthcare end up intersecting each other.

49:38.740 --> 49:43.220
And so that could be in the space of clinical trials, that could be in the space of, you

49:43.220 --> 49:50.500
know, just how, I mean, it could be in the space of protocols for just how we train people

49:50.500 --> 49:53.780
to interact and treat and care for their patients.

49:54.500 --> 49:58.900
And it could also be in the context of directly where machine learning just gets deployed

49:58.900 --> 49:59.860
in a healthcare setting.

49:59.860 --> 50:01.060
And how do you think about that?

50:01.620 --> 50:05.940
So these are three things that end up being a little step removed from what I was doing.

50:05.940 --> 50:09.380
It's interesting that they're a little bit focused on healthcare.

50:09.380 --> 50:12.820
But, you know, certainly there are multiple reasons for that.

50:12.820 --> 50:16.900
I think, one, I come from a family of healthcare providers.

50:16.900 --> 50:21.700
So I was a big disappointment because I didn't become a medical doctor.

50:22.340 --> 50:26.340
So hopefully I'll make my parents a little bit less regretful.

50:26.340 --> 50:26.660
You'll make up for this, I guess.

50:26.660 --> 50:28.580
Yeah, I've got to make up for that for my parents.

50:28.580 --> 50:34.660
No, you absolutely are, let's say, on the point in the sense that you're anticipating

50:34.660 --> 50:39.860
a question that I would have asked you from our audience, essentially about technologies

50:39.860 --> 50:44.420
and ideas to follow in the next one to five years, or one to 10 years, if you will.

50:44.420 --> 50:45.220
Oh, interesting.

50:45.220 --> 50:49.940
And I guess then healthcare and healthcare applications is definitely one of them for

50:49.940 --> 50:50.420
you.

50:50.420 --> 50:52.100
I think healthcare is a huge one.

50:52.100 --> 50:52.340
Yeah.

50:52.340 --> 50:57.620
Well, I was going to say that I've written a few papers on healthcare before.

50:57.620 --> 51:00.340
It's always been a little bit of a back burner to interest.

51:00.340 --> 51:06.420
But I think, you know, I can't lie that the pandemic kind of really got me way back into

51:06.420 --> 51:06.740
it.

51:06.740 --> 51:12.580
I think there's so much, you know, the medical community for the first time ever embraced

51:12.580 --> 51:17.060
the preprint server and all of its warts and all of its blessings, right?

51:17.060 --> 51:18.660
I mean, we could read all of these papers.

51:18.660 --> 51:20.340
You're up front against all these papers.

51:20.340 --> 51:23.300
You really start to see how evidence is gathered.

51:24.020 --> 51:25.220
It was an extreme setting.

51:25.220 --> 51:26.260
It was in a crisis setting.

51:26.260 --> 51:29.940
So I don't think it's completely aligned with normal times.

51:29.940 --> 51:33.300
But then you start to dig into normal times and you're like, it's not that far off.

51:33.300 --> 51:36.020
And sometimes their decision making is good and sometimes it's bad.

51:37.140 --> 51:40.340
And I think the other thing that happened, which makes it, you know, I think an interesting

51:40.340 --> 51:46.260
time is people are still pushing very hard to move algorithmic thinking into healthcare.

51:46.260 --> 51:49.140
And it's very messy.

51:49.140 --> 51:52.980
It's not cut and dry as to whether or not these things are going to be valuable.

51:54.340 --> 51:57.380
I think they could be, but I think we want to be careful and we want to think about how

51:57.380 --> 51:57.860
to do that.

51:57.860 --> 52:03.140
So these are the three main reasons why I think that healthcare is going to be a fun

52:03.140 --> 52:04.900
thing to be looking at moving forward.

52:05.780 --> 52:12.660
I have a couple of other questions and then maybe we can move to a bit more of a geeky

52:12.660 --> 52:13.940
territory if you want.

52:13.940 --> 52:15.140
Okay, sounds good.

52:16.180 --> 52:22.740
So one question that I was interested in asking you is about the biological origin of learning.

52:22.740 --> 52:26.740
I was wondering whether you ever got interested in that topic.

52:26.740 --> 52:32.020
And I was listening just the other day to another podcast from Google DeepMind, listening

52:32.020 --> 52:37.860
to Raya Hadsall speaking about essentially the origin of intelligence and the fact that

52:37.860 --> 52:43.700
it's very much related to motion and, you know, bacteria when they first evolved, they

52:43.700 --> 52:49.300
had a competitive advantage in moving towards food or other things, I suppose.

52:50.340 --> 52:54.820
So I wonder whether you ever got interested in this area, if you want, in the biological

52:54.820 --> 52:56.820
connections with learning.

52:58.500 --> 53:00.740
So I didn't, and I'm trying to think about why.

53:01.460 --> 53:03.540
I have a couple answers to why I never engaged.

53:03.540 --> 53:07.460
I think, you know, when I was in graduate school, artificial intelligence was a very

53:07.460 --> 53:08.020
dirty word.

53:08.820 --> 53:10.500
And I still kind of feel that way.

53:11.620 --> 53:12.340
I think historically...

53:12.340 --> 53:13.300
Why do you say so?

53:13.300 --> 53:14.100
I'm curious.

53:14.100 --> 53:17.860
I just think at the time when they were deep in a winter and nobody was doing AI, I think

53:17.860 --> 53:21.860
it's really funny that all the people who are doing AI now are just doing machine learning

53:21.860 --> 53:23.220
and they just kind of stole it.

53:23.220 --> 53:26.740
I don't really understand why that was allowed to happen.

53:26.740 --> 53:29.700
I think a lot of the AI in the early 20th century...

53:29.700 --> 53:31.780
Honestly, AI, sorry, 21st century.

53:32.500 --> 53:40.020
I think a lot of AI in general is just a kind of bizarre fixation that like never quite

53:40.020 --> 53:41.860
makes sense to me.

53:42.420 --> 53:43.780
Other people get into it.

53:43.780 --> 53:48.340
But if we look historically again, because that's all I do these days, the foundations

53:48.340 --> 53:54.900
of AI and the foundations of predictive machine learning, just predictive modeling, they also

53:54.900 --> 53:57.460
arise at the same time in different communities.

53:57.460 --> 54:04.340
There was a big push by the group of scientists to kind of brand something that was how the

54:04.340 --> 54:07.540
brain works and we're going to build machines that mimic how the brain works.

54:08.100 --> 54:13.780
And I think if you read the preface of cybernetics by Wiener, I think he also was really keen

54:13.780 --> 54:21.060
into understanding some kind of building systems that act like people or that mimic people.

54:21.620 --> 54:27.380
I think one of the fascinating reflections that Wiener had was that by the 60s, he could

54:27.380 --> 54:28.420
consider this done.

54:28.980 --> 54:33.540
And what he says, which I think is, if you go again and read the preface to the second

54:33.540 --> 54:40.180
edition of cybernetics, what he says is that while we learned a lot about psychology and

54:40.180 --> 54:44.180
trying to mimic things, really what stood out, the big abstractions that ended up standing

54:44.180 --> 54:45.860
out were things like feedback control.

54:46.580 --> 54:50.020
And feedback control, we can abstract away and then there's no personification.

54:50.580 --> 54:56.500
Feedback control, signal processing, detection and estimation, these are all things that

54:56.500 --> 54:59.060
didn't exist when Wiener wrote cybernetics the first time.

54:59.060 --> 55:03.620
And by the time he wrote the second edition, they were now canon of engineering and it's

55:03.620 --> 55:04.420
amazing.

55:04.420 --> 55:10.420
And those are deep, like deep mathematical and applied concepts that we use all the time.

55:11.140 --> 55:15.620
And I think that that's that big distinction between the AI school and what I would call

55:16.260 --> 55:17.620
whatever school I'm in.

55:17.620 --> 55:23.780
I do machine learning, but again, I'm looking back to what the 60s, we just called that

55:23.780 --> 55:24.820
pattern recognition.

55:24.820 --> 55:26.900
And I'm much happier with that term, honestly.

55:26.900 --> 55:32.820
So pattern recognition, control, detection and estimation, signal processing, these are

55:32.820 --> 55:35.220
the cores of engineering mathematics.

55:35.860 --> 55:42.340
And I think the powerful thing that the electrical engineers did, that the artificial

55:42.340 --> 55:46.180
intelligence people didn't, is they realized that the abstractions themselves are useful

55:46.740 --> 55:50.660
and they built all of the information technology of the late 20th century out of this.

55:51.300 --> 55:57.380
And I think people, the AI folks, I'm not sure what exactly, I don't know, now I'm going

55:57.380 --> 55:58.020
to get in trouble.

56:00.820 --> 56:02.340
Yeah, sure, let me get myself in trouble.

56:02.340 --> 56:07.460
What can we point to that are the technical artifacts of AI, really, as opposed to the

56:07.460 --> 56:10.580
technological artifacts of this other cybernetical school?

56:11.140 --> 56:12.820
Again, I don't want to give Wiener all the credit.

56:12.820 --> 56:16.100
It's just that his reflection, I think, was really nice.

56:16.100 --> 56:20.820
And it put in my mind an understanding of why these things split so hard.

56:22.500 --> 56:27.140
When you start post-World War II, there's no foundations of electrical engineering.

56:27.140 --> 56:29.060
I mean, we knew about feedback.

56:29.060 --> 56:31.460
We knew about detection estimation.

56:31.460 --> 56:37.300
We built a lot of that stuff into, unfortunately, the technology of war.

56:37.860 --> 56:42.500
But solidifying it into a real discipline happened in the subsequent time after the

56:42.500 --> 56:43.300
war.

56:43.300 --> 56:50.580
And it led to, I mean, just compare 1959 to 2022, right?

56:50.580 --> 56:56.980
And all of the things that went into that with feedback control, with signal processing,

56:56.980 --> 56:59.460
detection estimation, it's just mind-blowing.

57:00.100 --> 57:06.820
Yeah, I mean, maybe my personal trajectory also biases here the analysis, but I kind

57:06.820 --> 57:14.500
of see that somehow we are approaching a sort of renaissance of cybernetics in a way.

57:14.500 --> 57:19.300
I mean, I see a lot of interest in the brain and neuroscience as well.

57:19.300 --> 57:26.100
Neuralink is building these new chips that are probably going to read the signals in

57:26.100 --> 57:29.140
our brains incredibly fast and maybe act on them.

57:29.140 --> 57:29.940
We don't know.

57:29.940 --> 57:30.500
I hope not.

57:31.060 --> 57:37.460
And, you know, you have all these other buzzwords like synthetic biology and even, you know,

57:37.460 --> 57:40.740
molds that solve optimization problems.

57:40.740 --> 57:43.060
So I wonder whether that will play a role.

57:43.060 --> 57:46.340
But I guess this is just an open question to tease you.

57:46.340 --> 57:47.300
We don't know.

57:47.300 --> 57:48.020
We'll find out.

57:48.020 --> 57:48.660
We'll find out.

57:48.660 --> 57:50.180
I think it is interesting.

57:50.180 --> 57:53.940
I think that the, to me, I think that the thing that's funny, though, is that this

57:54.580 --> 57:58.580
overloading of the word learning, I think, is the part that's actually a little suspect

57:58.580 --> 57:59.300
to me.

57:59.300 --> 58:03.300
It took me until about 2016 to admit that I did machine learning.

58:03.300 --> 58:05.780
I always told people I do statistics and optimization.

58:05.780 --> 58:09.860
I just would refuse to engage with the name because it's always struck me as odd.

58:09.860 --> 58:14.260
And honestly, I think it was useful to just say it's pattern recognition, it's pattern

58:14.260 --> 58:15.220
classification.

58:15.220 --> 58:18.100
Those are just, that's just much, it's kind of boring.

58:18.100 --> 58:20.820
Pattern classification is super boring, but it's honest.

58:21.460 --> 58:22.020
And useful.

58:23.060 --> 58:23.860
And useful, yeah.

58:24.500 --> 58:28.340
OK, so maybe it's a good time now to move on, shift gears again.

58:28.340 --> 58:29.860
We're going to play a little game.

58:29.860 --> 58:30.660
Oh boy.

58:30.660 --> 58:32.180
The rules, yes.

58:33.140 --> 58:34.980
So the rules are simple.

58:34.980 --> 58:37.460
I'm going to ask you a this or that question.

58:38.420 --> 58:40.900
You need to answer in at most two seconds.

58:41.460 --> 58:44.420
And I really want to see system one playing.

58:44.420 --> 58:49.620
So, you know, in Kahneman's, Daniel Kahneman's thinking fast and slow language.

58:49.620 --> 58:50.980
OK, we're going to go fast.

58:50.980 --> 58:51.620
All right.

58:51.620 --> 58:52.580
We're going to go fast.

58:52.580 --> 58:56.180
So every answer that you give after three seconds is not valid.

58:56.180 --> 58:56.580
OK.

58:56.580 --> 58:59.780
And I'm going to just shoot at you a lot of questions.

58:59.780 --> 59:00.180
OK.

59:00.180 --> 59:02.020
Tell me when you're ready, we can start.

59:03.060 --> 59:03.540
I'm ready.

59:03.540 --> 59:03.940
Let's go.

59:04.500 --> 59:07.220
OK, so working hard or hardly working?

59:07.220 --> 59:07.860
Working hard.

59:08.420 --> 59:09.620
Robots or dinosaurs?

59:10.420 --> 59:11.060
Dinosaurs.

59:11.700 --> 59:13.220
Success or happiness?

59:13.220 --> 59:14.100
Happiness.

59:14.100 --> 59:15.780
Growth or security?

59:15.780 --> 59:16.580
Oh, growth.

59:17.220 --> 59:17.860
I don't like either.

59:18.420 --> 59:19.860
Guacamole or salsa?

59:20.740 --> 59:21.460
Salsa.

59:21.460 --> 59:23.220
Loud neighbors or nosy neighbors?

59:24.180 --> 59:24.980
Nosy neighbors.

59:24.980 --> 59:26.660
Pineapple pizza or candy corn?

59:27.220 --> 59:27.860
Oh, God, no.

59:34.740 --> 59:35.240
Can't do it.

59:36.340 --> 59:38.580
Test the waters or dive in the deep end?

59:40.100 --> 59:42.100
Test the waters, but that's a swimming issue.

59:43.620 --> 59:45.300
OK, we're going to go back to that later.

59:45.300 --> 59:46.100
Sure.

59:46.100 --> 59:47.700
Logic or emotion?

59:47.700 --> 59:48.660
Emotion.

59:48.660 --> 59:50.260
Zombies or vampires?

59:50.260 --> 59:51.140
Vampires.

59:51.140 --> 59:52.180
Nyquist or Kalman?

59:52.980 --> 59:53.940
Nyquist.

59:53.940 --> 59:56.580
Glass half full or glass half empty?

59:56.580 --> 59:57.460
Why can't it be both?

59:58.660 --> 01:00:01.140
Couch potato or fitness fiend?

01:00:01.140 --> 01:00:02.340
Fitness fiend.

01:00:02.340 --> 01:00:03.540
Money or love?

01:00:03.540 --> 01:00:04.040
Love.

01:00:04.500 --> 01:00:06.340
Bayesian or frequentist?

01:00:06.340 --> 01:00:07.300
No, that one I won't answer.

01:00:09.620 --> 01:00:11.300
I have a story for that one, too, if you want.

01:00:11.300 --> 01:00:12.020
OK.

01:00:12.020 --> 01:00:12.520
No.

01:00:13.620 --> 01:00:15.700
We're going to go back to that as well.

01:00:16.260 --> 01:00:19.220
So see the future or change the past?

01:00:19.220 --> 01:00:22.260
Whoa, those are like the same, aren't they?

01:00:23.220 --> 01:00:24.260
Those are the same.

01:00:25.620 --> 01:00:27.540
Time machine or magic wand?

01:00:27.540 --> 01:00:28.500
Magic wand.

01:00:28.500 --> 01:00:29.780
Vacation or staycation?

01:00:30.340 --> 01:00:31.380
Vacation.

01:00:31.380 --> 01:00:32.980
LQR or GPT-3?

01:00:34.100 --> 01:00:34.900
LQR.

01:00:34.900 --> 01:00:36.180
Planning or winging it?

01:00:36.740 --> 01:00:37.380
Depends.

01:00:37.380 --> 01:00:38.820
I can't answer that one either.

01:00:38.820 --> 01:00:39.460
That was too hard.

01:00:40.820 --> 01:00:43.060
Dynamic programming or divide and conquer?

01:00:43.940 --> 01:00:45.300
Dynamic programming.

01:00:45.300 --> 01:00:46.980
Skill or popularity?

01:00:46.980 --> 01:00:47.780
Skill.

01:00:47.780 --> 01:00:49.380
Deep mind or Boston Dynamics?

01:00:50.020 --> 01:00:51.060
Boston Dynamics.

01:00:51.060 --> 01:00:53.060
Poor and happy or rich and miserable?

01:00:54.420 --> 01:00:55.060
Poor and happy.

01:00:56.100 --> 01:00:56.740
That was too easy.

01:00:58.820 --> 01:01:00.420
Maths or physics?

01:01:00.420 --> 01:01:00.920
Maths.

01:01:01.460 --> 01:01:03.220
Beethoven or Beatles?

01:01:03.220 --> 01:01:03.720
Beatles.

01:01:04.740 --> 01:01:06.580
Speeding ticket or parking ticket?

01:01:08.660 --> 01:01:09.860
I get more parking tickets.

01:01:09.860 --> 01:01:10.580
I don't know what that says.

01:01:11.140 --> 01:01:12.500
Machine learning or control?

01:01:14.340 --> 01:01:16.740
Man, again, that's another one I can't answer.

01:01:16.740 --> 01:01:17.460
This is the last one.

01:01:17.460 --> 01:01:18.500
It's a good closer.

01:01:18.500 --> 01:01:19.540
That's a good closer.

01:01:19.540 --> 01:01:20.500
I can't answer that one.

01:01:21.220 --> 01:01:23.220
We're going to skip.

01:01:23.220 --> 01:01:24.260
What about the waters?

01:01:24.260 --> 01:01:26.260
Why can't we test the waters?

01:01:26.260 --> 01:01:27.780
I'm just not a very good swimmer.

01:01:28.820 --> 01:01:30.500
This is something I do regret a bit.

01:01:30.500 --> 01:01:31.220
I can swim.

01:01:31.220 --> 01:01:31.700
I'm all right.

01:01:32.820 --> 01:01:34.020
Something I should be better at.

01:01:34.020 --> 01:01:35.380
I just need to practice more.

01:01:35.380 --> 01:01:38.660
What about the Bayesian versus frequentist?

01:01:39.700 --> 01:01:42.100
My favorite answer to that question is when someone says,

01:01:42.100 --> 01:01:43.380
what kind of statistician are you?

01:01:43.380 --> 01:01:44.340
It's to say difficult.

01:01:47.700 --> 01:01:49.220
I think that's the best answer.

01:01:49.220 --> 01:01:52.020
The Bayesian-frequentist divide actually is nonsense.

01:01:52.020 --> 01:01:52.740
You want that middle one.

01:01:52.740 --> 01:01:53.240
Difficult.

01:01:53.940 --> 01:01:57.540
And I learned that from Philip Stark, who's in our statistics department.

01:01:57.540 --> 01:01:58.820
Someone was pressuring him on that.

01:01:58.820 --> 01:01:59.380
That's what he said.

01:02:00.340 --> 01:02:02.820
There is an interesting aspect to that, too.

01:02:02.820 --> 01:02:07.220
And I think if you look at the way that statisticians, there are different statisticians.

01:02:07.220 --> 01:02:11.140
And I think there are some who, I think when you say Bayesian or frequentist,

01:02:11.140 --> 01:02:16.660
it kind of imbues a certain kind of faith that one of those two is going to get you to an answer.

01:02:16.660 --> 01:02:21.380
Where I think some of the best things that you could do as a statistician

01:02:21.380 --> 01:02:24.820
is understand just the limits of both ways of thinking.

01:02:25.460 --> 01:02:28.260
That's why difficult, I think, is kind of a good one.

01:02:28.820 --> 01:02:29.220
OK.

01:02:29.220 --> 01:02:35.700
So maybe this is a good time to take all the questions that we got from the audience offline

01:02:35.700 --> 01:02:37.140
and ask you just a few, maybe.

01:02:38.420 --> 01:02:43.860
One that I particularly like is, what is your daily mantra or your favorite quote?

01:02:45.060 --> 01:02:45.940
Oh, man.

01:02:46.900 --> 01:02:47.940
That's a hard one.

01:02:47.940 --> 01:02:48.980
Do I have a daily mantra?

01:02:48.980 --> 01:02:50.980
I probably have one and now I'm just forgetting it.

01:02:52.420 --> 01:02:55.860
Sometimes you get put on the spot and you really got to think like, let me think for a second.

01:02:57.540 --> 01:03:02.820
And also the same person asked, do you have a specific routine in the morning?

01:03:03.460 --> 01:03:06.660
Like, what do you do in the first 60 minutes of your day?

01:03:06.660 --> 01:03:07.220
See that?

01:03:07.220 --> 01:03:07.940
That's interesting.

01:03:07.940 --> 01:03:08.900
That I can answer.

01:03:08.900 --> 01:03:12.100
Oh, the favorite quote thing is always tough because I have so many.

01:03:12.100 --> 01:03:14.820
And it's always contextual as to which one comes up.

01:03:14.820 --> 01:03:19.220
But routine, I am definitely, for better, for worse, a man of routine.

01:03:19.220 --> 01:03:25.060
So like I wake up, my cat will wake me up usually between 530 and six every morning.

01:03:25.060 --> 01:03:28.100
He comes to the door and really makes sure that I'm awake.

01:03:28.100 --> 01:03:28.980
Very important to him.

01:03:29.540 --> 01:03:34.660
So then usually I'll have to feed the cat and then I'll make a coffee.

01:03:34.660 --> 01:03:36.180
Then usually I'll have a small breakfast.

01:03:36.900 --> 01:03:42.500
And often that's when I'll do a lot of Twitter reading, just because if I do it when I'm awake,

01:03:42.500 --> 01:03:44.420
it just is no fun.

01:03:44.420 --> 01:03:46.420
So that's why I usually try to get that out of the day there.

01:03:47.700 --> 01:03:49.780
I don't want to read email or anything else.

01:03:51.140 --> 01:03:57.060
And then usually after about 60 minutes, I'll go and do some exercise.

01:03:57.060 --> 01:03:59.220
So that's usually that's usually my morning routine.

01:03:59.220 --> 01:04:02.580
I was about to ask if you're the kind of person that actually wakes up,

01:04:02.580 --> 01:04:05.540
goes to the gym first thing and then, you know, starts.

01:04:05.540 --> 01:04:06.020
Pretty much.

01:04:06.020 --> 01:04:06.660
Yeah.

01:04:06.660 --> 01:04:07.140
With the day.

01:04:07.140 --> 01:04:08.420
Yeah, yeah, yeah, yeah, yeah.

01:04:08.420 --> 01:04:13.300
And he says he's usually that's, that's my, you know, the coffee, some yogurt and some fruit.

01:04:13.940 --> 01:04:16.660
But 60 minutes of silliness, then I'll go exercise.

01:04:16.660 --> 01:04:18.980
And then, yeah, then I go, then I'm going to go tackle the day.

01:04:18.980 --> 01:04:22.500
But that's pretty, pretty regimented as to how that will line up.

01:04:22.500 --> 01:04:23.380
I can relate to that.

01:04:23.380 --> 01:04:24.500
I mean, it's a great routine.

01:04:25.460 --> 01:04:29.540
OK, so the other question is about books.

01:04:29.540 --> 01:04:35.060
And in particular, what is the book that you've gifted to the most to other people?

01:04:36.260 --> 01:04:39.140
Oh, well, that's a good question, too.

01:04:39.140 --> 01:04:39.700
What is the book?

01:04:40.420 --> 01:04:44.660
Um, you know, I could just let's just go with recent times, like a book I read very.

01:04:45.860 --> 01:04:50.980
I'm sure I have a better answer for the most of all time, but I tend to read,

01:04:50.980 --> 01:04:54.420
you know, there'll be like one or two books a year that will really resonate with me.

01:04:54.420 --> 01:04:56.660
And I will tell everyone that they have to read them.

01:04:56.660 --> 01:05:00.020
And so the most recent one, which is of the last month,

01:05:00.740 --> 01:05:03.380
is a book called The Knowledge Machine by Michael Strevens,

01:05:03.940 --> 01:05:08.020
which is a philosophy of science book, but written in a very accessible language.

01:05:08.020 --> 01:05:08.980
And it's phenomenal.

01:05:08.980 --> 01:05:12.340
I feel like it's the first philosophy of science book I've read where I was like,

01:05:12.340 --> 01:05:16.660
this guy really is explaining what science is, what the practice of science is.

01:05:16.660 --> 01:05:20.900
And it has a very, I don't know, I thought it was a really interesting

01:05:21.540 --> 01:05:26.180
way to appreciate what we do, especially in the applied sciences.

01:05:26.180 --> 01:05:28.020
So I thought that was a great book.

01:05:28.020 --> 01:05:29.140
The Knowledge Machine.

01:05:29.140 --> 01:05:29.380
Yeah.

01:05:30.180 --> 01:05:30.500
All right.

01:05:30.500 --> 01:05:36.820
So maybe the last question again from the audience is about redesigning machine learning.

01:05:36.820 --> 01:05:43.780
And in particular, they ask, if you could redesign ML from scratch, how would you redesign it?

01:05:45.140 --> 01:05:47.700
Oh, do you think they're asking about the discipline or the course?

01:05:47.700 --> 01:05:50.580
So remove all the hype that surrounds it now.

01:05:50.580 --> 01:05:50.820
Yeah.

01:05:50.820 --> 01:05:52.260
What would you, what would you change?

01:05:52.820 --> 01:05:54.660
Well, so I could sell my book now, right?

01:05:54.660 --> 01:05:56.260
And that's what we do at the end of the podcast.

01:05:56.260 --> 01:05:56.820
Absolutely.

01:05:57.780 --> 01:05:59.540
So there we go.

01:05:59.540 --> 01:06:02.660
So, and even that's incomplete.

01:06:02.820 --> 01:06:08.340
But Moritz Hart and I wrote a book called Patterns, Predictions, and Actions, which

01:06:08.340 --> 01:06:12.340
was our attempt to figure out what would we want to teach a graduate course in machine learning.

01:06:12.900 --> 01:06:16.420
And it's, I really like it.

01:06:16.420 --> 01:06:21.700
I mean, I do think that this is kind of, would be a book I teach from for a long time.

01:06:22.420 --> 01:06:26.340
Of course, as soon as the book went to the publisher, I told Moritz, oh, no, we have

01:06:26.340 --> 01:06:27.300
to rewrite the whole thing.

01:06:27.300 --> 01:06:30.500
And then he got, he didn't want to hear that because he's right.

01:06:30.500 --> 01:06:31.220
I shouldn't do that.

01:06:31.220 --> 01:06:34.740
But I still have ideas about how to iterate and how we might, how we might change it down

01:06:34.740 --> 01:06:35.140
the line.

01:06:35.140 --> 01:06:42.020
But, you know, walking you through that book, we start with the elements of detection theory,

01:06:42.020 --> 01:06:46.820
which is, you know, I do think where it's the reasonable starting point for classification.

01:06:49.700 --> 01:06:53.700
From detection theory alone, just making decisions and prediction and making decisions,

01:06:54.500 --> 01:06:59.060
you already start to run into all of these tricky issues about the best you could possibly

01:06:59.060 --> 01:07:03.060
do and what happens when there's like hidden heterogeneity.

01:07:03.060 --> 01:07:05.220
And you just, there's all sorts of things that come out of that.

01:07:06.420 --> 01:07:09.620
From there though, once we say, okay, well, okay, we're going to do this anyway.

01:07:09.620 --> 01:07:11.860
We're going to do this kind of decision-making.

01:07:11.860 --> 01:07:15.700
The thing we move to is, you know, supervised learning.

01:07:15.700 --> 01:07:20.100
And it's kind of a nice jump, you know, supervised learning is, you know, how you would do prediction

01:07:20.100 --> 01:07:24.580
when I don't necessarily know the way that the data is coming to me.

01:07:24.580 --> 01:07:27.380
And so then we dive into supervised learning.

01:07:28.340 --> 01:07:32.020
I think when we teach the course, about half the course is on supervised learning.

01:07:32.020 --> 01:07:35.460
And we talk about, you know, feature representations and where those might come

01:07:35.460 --> 01:07:37.460
from and how you can build new ones.

01:07:37.460 --> 01:07:41.060
And then we talk about optimization, which is really key.

01:07:41.060 --> 01:07:44.260
We have a simplified view there, but, you know, I think there's, I honestly think that

01:07:44.260 --> 01:07:47.780
most people who do a first course in machine learning really should take a second course

01:07:47.780 --> 01:07:48.500
in optimization.

01:07:49.860 --> 01:07:53.860
We also talk about generalization and generalizability, which to me, we just kind of

01:07:53.860 --> 01:07:55.700
go through a lot of different ideas that people have.

01:07:56.820 --> 01:07:58.980
And some of them are satisfying and some of them are not.

01:07:58.980 --> 01:08:03.380
But we connect back to these ideas from the 70s and 60s about like, we knew a lot of these

01:08:05.300 --> 01:08:06.500
concepts are old.

01:08:06.500 --> 01:08:08.260
And like the way we approach them are old.

01:08:08.260 --> 01:08:10.660
And most of them are sensitivity, just like you said.

01:08:10.660 --> 01:08:16.420
If you have a prediction that's very insensitive to changes in the data, it's probably going

01:08:16.420 --> 01:08:18.500
to be similar on new data.

01:08:18.500 --> 01:08:21.540
So that's an interesting thing there.

01:08:21.540 --> 01:08:26.660
Then we have this whole chapter on data and datasets, which I think I've never, nobody

01:08:26.660 --> 01:08:30.980
else has done in a book, which like that's kind of like most of the field now is about

01:08:30.980 --> 01:08:31.300
data.

01:08:31.300 --> 01:08:36.260
So we really dive into datasets, why they exist, why they persist.

01:08:36.260 --> 01:08:37.220
What do we do after that?

01:08:37.220 --> 01:08:38.100
I mean, I could keep going.

01:08:38.100 --> 01:08:38.420
It's great.

01:08:38.420 --> 01:08:41.380
So after that, we have like five pages on deep learning.

01:08:41.380 --> 01:08:44.260
And then we move to, I think this is the thing that's interesting.

01:08:44.260 --> 01:08:48.100
After the five pages on deep learning, we move to, you know, what happens when you're

01:08:48.100 --> 01:08:51.300
trying to make predictions and you're not in a static world anymore?

01:08:52.020 --> 01:08:56.580
Touching on notions of causality, we talk about optimal control, we talk about reinforcement

01:08:56.580 --> 01:08:57.060
learning.

01:08:57.060 --> 01:08:59.620
So I think that that pivot in the middle is very interesting.

01:08:59.620 --> 01:09:03.220
And I think that that's the stuff that we have to, I mean, it's still, I'm not sure

01:09:03.220 --> 01:09:05.220
I'm happy with any of the answers that we have there.

01:09:05.220 --> 01:09:08.180
I don't think, I think they're just things that we should know as we move forward.

01:09:08.180 --> 01:09:12.980
But I think those are the places where I'm most interested still to look at machine learning

01:09:12.980 --> 01:09:17.700
is like what happens when you move away from the static frame to this dynamic frame?

01:09:17.700 --> 01:09:20.660
It's just, we're still figuring it out.

01:09:20.660 --> 01:09:20.900
Yeah.

01:09:20.900 --> 01:09:27.140
I guess a book always can be seen as a picture of the zeitgeist, if you want, of the time.

01:09:27.140 --> 01:09:31.220
And I'm just particularly interested about the chapter on datasets.

01:09:31.780 --> 01:09:38.260
I think that's a great idea because it doesn't really get talked about, I guess, in any book

01:09:38.260 --> 01:09:39.220
that I've seen.

01:09:39.220 --> 01:09:42.420
Do you touch on the standardization issues?

01:09:43.700 --> 01:09:44.100
Yeah.

01:09:44.100 --> 01:09:46.260
So we touched on a variety of aspects there.

01:09:47.060 --> 01:09:52.500
Also, just for the people listening, the book is available for free, early version of the

01:09:52.500 --> 01:09:55.220
book is available for free at mlstory.org.

01:09:56.180 --> 01:09:58.100
And I think that's right.

01:09:58.820 --> 01:10:04.020
And we'll correct that after the fact or add it to the ladder notes, the correct link there.

01:10:04.020 --> 01:10:11.700
But the dataset chapter talks about some of the existing datasets and how they were created.

01:10:11.700 --> 01:10:16.420
And actually, we go through this whole study of just the history of the dataset.

01:10:16.420 --> 01:10:18.820
Like, why did we even standardize these things to begin with?

01:10:19.540 --> 01:10:20.820
It's really fascinating.

01:10:20.820 --> 01:10:29.220
I mean, there's this amazing story that Moritz and I found about the first dataset for handwritten

01:10:29.220 --> 01:10:33.460
character recognition, which is everybody's bread and butter favorite application of machine

01:10:33.460 --> 01:10:34.340
learning.

01:10:34.340 --> 01:10:36.340
And the first dataset is from 1959.

01:10:36.900 --> 01:10:38.020
It's crazy.

01:10:38.020 --> 01:10:38.580
Wow.

01:10:38.580 --> 01:10:39.860
That's really crazy.

01:10:40.500 --> 01:10:41.220
It's crazy.

01:10:41.300 --> 01:10:43.300
The story behind that dataset is just incredible.

01:10:44.100 --> 01:10:46.420
There was this fellow named Bill Heilemann.

01:10:46.420 --> 01:10:47.860
He was working at Bell Labs.

01:10:48.660 --> 01:10:54.740
He was tasked with making, everybody was trying to make OCR readers at the time, which is

01:10:54.740 --> 01:10:58.020
kind of crazy to think that you're trying to do that in the 50s.

01:10:58.020 --> 01:10:58.980
But they were trying.

01:10:59.620 --> 01:11:03.780
And let me just tell a couple things about this.

01:11:03.780 --> 01:11:05.060
This is one of my favorite stories.

01:11:06.340 --> 01:11:10.340
The first thing was that they didn't realize, I mean, Bill's thought,

01:11:11.700 --> 01:11:14.580
was most people were trying to build end-to-end devices.

01:11:14.580 --> 01:11:18.260
Because that was, you know, personal computers were not really that big a thing.

01:11:18.260 --> 01:11:22.740
And Bill was like, I can't design it unless I could kind of abstract this somehow.

01:11:22.740 --> 01:11:24.820
So his idea was to have two abstractions.

01:11:24.820 --> 01:11:27.540
You would have a scanner that would produce data.

01:11:27.540 --> 01:11:31.220
And then once you had the data stored, you would have the second step of the, you would

01:11:31.220 --> 01:11:34.180
simulate the rest of the process on one of their big computers.

01:11:34.180 --> 01:11:34.740
Wow.

01:11:34.740 --> 01:11:37.940
Like now this sounds obvious, but it was not at all obvious.

01:11:37.940 --> 01:11:40.340
At the time it was quite ambitious, I guess.

01:11:40.580 --> 01:11:41.460
Quite ambitious.

01:11:41.460 --> 01:11:46.100
And so what he did was he built a scanner that was supposed to be a general purpose

01:11:46.100 --> 01:11:46.580
scanner.

01:11:46.580 --> 01:11:53.380
And he collected 50 different alphabets from 50 different writers and put them on punch

01:11:53.380 --> 01:11:56.100
cards and then let people, and then published a paper.

01:11:56.820 --> 01:12:01.460
It was kind of this weird thing where he published a paper on these and someone else grabbed

01:12:01.460 --> 01:12:01.700
them.

01:12:02.500 --> 01:12:04.660
Well, actually, I remember this is a good story.

01:12:04.660 --> 01:12:07.940
He published a paper saying somebody else's method wasn't good.

01:12:08.820 --> 01:12:11.140
That's always the start of a paper, I guess.

01:12:11.140 --> 01:12:11.700
Of course.

01:12:11.700 --> 01:12:12.980
That's always how it goes.

01:12:12.980 --> 01:12:18.820
And that guy, his name was Bledsoe, who was at Stanford at the time or in Palo Alto at

01:12:18.820 --> 01:12:21.700
the time, you know, asked, well, I think you're doing this wrong.

01:12:21.700 --> 01:12:22.820
Can you send me your cards?

01:12:23.540 --> 01:12:28.020
And so Heilemann sent the cards to Bledsoe and then Bledsoe published some other paper.

01:12:28.020 --> 01:12:33.460
And then this fellow Chow was also really famous for a lot of things in machine learning.

01:12:33.620 --> 01:12:34.340
Machine learning.

01:12:35.140 --> 01:12:38.900
Chow got a copy of them, did his own analysis.

01:12:39.620 --> 01:12:41.540
Duda and Hart at SRI got a copy.

01:12:41.540 --> 01:12:47.060
And so, you know, Bill has this amazing short note in one of the IEEE journals that said

01:12:47.060 --> 01:12:50.100
that, like, I made these this data if you want it.

01:12:50.100 --> 01:12:51.380
Here's my mailing address.

01:12:51.380 --> 01:12:52.420
I'll print them for you.

01:12:53.620 --> 01:12:57.380
And he's sending a shoebox of 1800 punch cards around the country, I guess.

01:12:57.380 --> 01:12:59.780
That was what that was the way the data was disseminated.

01:12:59.780 --> 01:13:00.340
Fantastic.

01:13:01.060 --> 01:13:01.560
Physically.

01:13:02.580 --> 01:13:03.940
Physically, physically.

01:13:03.940 --> 01:13:09.940
And I think one of the another, like, just fun quirk about that story is that he well,

01:13:09.940 --> 01:13:10.740
there are two fun quirks.

01:13:10.740 --> 01:13:16.020
First of all, Bill wrote the first paper I've ever seen on training versus test split.

01:13:16.900 --> 01:13:21.860
So I think and everybody just naturally came to this conclusion that you use the first

01:13:21.860 --> 01:13:25.140
40 alphabets for training and last 10 for testing.

01:13:25.140 --> 01:13:26.100
It was very natural.

01:13:26.100 --> 01:13:29.940
And Bill actually wrote a paper trying to justify it, which is great.

01:13:29.940 --> 01:13:35.700
And the second thing that was fascinating is in 1962, he published a PhD thesis on this

01:13:35.700 --> 01:13:38.660
work, and then left machine learning for the rest of his career.

01:13:39.460 --> 01:13:42.500
Because because he just considered the project a failure.

01:13:42.500 --> 01:13:43.460
No way.

01:13:43.460 --> 01:13:44.500
He considered it a failure.

01:13:44.500 --> 01:13:45.220
And it's amazing.

01:13:45.220 --> 01:13:48.500
It led to everything since no one's heard of this man, Bill, Bill Hyman.

01:13:48.500 --> 01:13:53.220
What he went to work on was computer networks, because he's like, I obviously doesn't want

01:13:53.220 --> 01:13:55.700
to send around shoeboxes of punch cards anymore.

01:13:56.340 --> 01:14:01.860
He did a lot of interesting work in high performance computing after after this, but he just considered

01:14:01.860 --> 01:14:03.460
that, you know, the machines weren't there.

01:14:04.260 --> 01:14:10.500
And it's kind of wild to just watch over the course of, you know, several decades, everything

01:14:10.500 --> 01:14:11.300
really came together.

01:14:11.300 --> 01:14:16.740
And now OCR is effectively solved, but not using anything dramatically more sophisticated

01:14:16.740 --> 01:14:17.780
than what he proposed.

01:14:17.780 --> 01:14:20.180
You just needed the technology to catch up.

01:14:20.180 --> 01:14:24.980
I guess this is also a fantastic message for all those PhD students out there hitting their

01:14:24.980 --> 01:14:25.860
PhD blues.

01:14:28.420 --> 01:14:33.620
You may not know whether, you know, your work is going to end up unfolding as pioneering

01:14:33.620 --> 01:14:34.500
in 50 years.

01:14:34.500 --> 01:14:35.140
Amazing.

01:14:35.140 --> 01:14:40.420
And this is also a fantastic assist maybe for my next and possibly last question before

01:14:40.420 --> 01:14:41.540
we move on to music.

01:14:42.820 --> 01:14:48.340
This is something that I would actually probably I'm going to ask to everybody else on the

01:14:48.340 --> 01:14:51.700
podcast, and it's about future students.

01:14:52.660 --> 01:14:54.820
If you were a student today, what would you do?

01:14:55.380 --> 01:15:00.420
Or what would you invest on, rather, in the sense that what would you need to do to live

01:15:00.420 --> 01:15:02.100
a life that you would be proud of?

01:15:02.100 --> 01:15:03.700
What's your best advice?

01:15:03.700 --> 01:15:05.860
I mean, this comes back to what we talked about at the beginning.

01:15:05.860 --> 01:15:12.500
I mean, to me, it's just like if you're there's ups and downs in a research career, and it's

01:15:12.500 --> 01:15:15.220
not necessarily for everyone.

01:15:15.220 --> 01:15:19.860
And if you could think of something else to do, I would just say you should do that.

01:15:19.860 --> 01:15:22.580
Something else that inspires and fulfills you, you should do that.

01:15:22.580 --> 01:15:26.740
But if this is what keeps you up at night, just make sure that you're always reminding

01:15:26.740 --> 01:15:27.780
yourself of why.

01:15:27.780 --> 01:15:31.860
Like, yeah, this is like reminding yourself that, yeah, this case, I'm passionate about

01:15:31.860 --> 01:15:32.100
this.

01:15:32.980 --> 01:15:34.500
This is why I'm passionate about this.

01:15:34.500 --> 01:15:36.180
And this is why I'm going to keep doing it.

01:15:36.180 --> 01:15:39.780
I think that's like, you know, being in touch with that is super important.

01:15:40.420 --> 01:15:44.180
And the other thing I would say to everybody is like the topic never really matters.

01:15:44.180 --> 01:15:47.860
I think it's this making sure you're enjoying the work.

01:15:47.860 --> 01:15:55.060
And then for me, this is not true for everyone, but for me, it was also the interactions with

01:15:55.060 --> 01:15:58.820
my fellow graduate students that really just kept me going.

01:15:58.820 --> 01:16:03.460
I think, you know, chatting with you're in this weird opportunity to be around a bunch

01:16:03.460 --> 01:16:07.540
of brilliant other people who know a lot of different things in you and taking advantage

01:16:07.540 --> 01:16:12.660
of that resource and, you know, stepping out of your department, even, you know, so I again,

01:16:12.660 --> 01:16:16.420
I said, I went to an interesting program at MIT called the Media Lab.

01:16:16.420 --> 01:16:20.980
But there I was surrounded with people who, you know, worked on nanotechnology, who worked

01:16:20.980 --> 01:16:26.340
on machine learning, who worked on education of kids, who worked on electronic art, who

01:16:26.340 --> 01:16:30.500
worked on computer net, which is such a diverse, weird group of people.

01:16:30.500 --> 01:16:37.540
And it's just that kind of getting a bunch of people like that under one roof and letting

01:16:37.540 --> 01:16:38.500
them do whatever they want.

01:16:38.500 --> 01:16:43.220
It's just kind of a powerful, amazing thing to experience.

01:16:43.220 --> 01:16:45.860
And this in general, I think is great advice.

01:16:45.860 --> 01:16:48.580
And I'm pretty sure we will resonate with many out there.

01:16:49.860 --> 01:16:53.060
I guess this brings me to the last topic of today.

01:16:53.060 --> 01:16:59.060
And the next question or question, rather topic of discussion, if you want, will be

01:16:59.060 --> 01:17:00.020
about music.

01:17:00.020 --> 01:17:06.580
So while doing my homework for this podcast episode, I found out that you're a brilliant

01:17:06.580 --> 01:17:08.820
and very accomplished musician.

01:17:08.820 --> 01:17:10.580
I found that super interesting.

01:17:10.580 --> 01:17:18.020
I personally like many of your tracks and I just thought it would be interesting to

01:17:18.020 --> 01:17:23.220
dig into this a little bit and maybe ask you, what's your relationship with music?

01:17:23.220 --> 01:17:24.260
How did it all start?

01:17:24.260 --> 01:17:31.220
And how do you manage to have such a successful career at the academic level and also keep

01:17:31.220 --> 01:17:34.740
such a passion alive in the way you do it?

01:17:34.740 --> 01:17:36.340
Yeah, I've always loved music.

01:17:36.420 --> 01:17:41.620
I bought my first guitar when I was, I think, 13, 12 or 13, started playing there.

01:17:41.620 --> 01:17:42.820
And then I was just kind of hooked.

01:17:42.820 --> 01:17:43.860
Yeah, I was just hooked.

01:17:43.860 --> 01:17:45.460
I was just hooked after that.

01:17:45.460 --> 01:17:51.300
And I think that there was always, so it's always been something that I've been passionate

01:17:51.300 --> 01:17:51.540
about.

01:17:51.540 --> 01:17:56.820
So it's something, of course, also when you're a teenager, feel like you could do that for

01:17:56.820 --> 01:18:01.220
a living, which is not for, I mean, to those who can, more power to you.

01:18:01.220 --> 01:18:03.060
And I love it and just pursue it.

01:18:03.060 --> 01:18:04.260
But not everybody can.

01:18:04.260 --> 01:18:07.940
It's definitely, I think, a little bit easier to become a professor than a professional

01:18:07.940 --> 01:18:09.780
musician, especially when you could live.

01:18:09.780 --> 01:18:14.260
So you're stating that becoming a professor at Berlgrie is easier than becoming a professional

01:18:14.260 --> 01:18:14.740
musician.

01:18:15.380 --> 01:18:16.660
I worry that that might be.

01:18:18.420 --> 01:18:19.220
I don't know, man.

01:18:19.780 --> 01:18:21.060
It's a hard hustle being.

01:18:21.060 --> 01:18:23.940
Sorry, sorry to intervene, but it was just so funny.

01:18:23.940 --> 01:18:27.700
I do worry that it's a hard hustle for my musician friends.

01:18:27.700 --> 01:18:32.900
I mean, I think some of them have been, you know, I think the ones who've been successful

01:18:32.900 --> 01:18:37.940
have been very entrepreneurial and really thought hard about what exactly they wanted

01:18:37.940 --> 01:18:42.900
from that career path that they were choosing, but it's been hard for a lot of them.

01:18:42.900 --> 01:18:45.700
And I think it's, you know, there's a lot of passion that goes into it.

01:18:48.420 --> 01:18:52.820
But I'll say that, like, up until my late 20s, I still kind of had a dream.

01:18:52.820 --> 01:18:59.860
So it took a long time for me to to shake that idea of being a professional musician.

01:18:59.860 --> 01:19:02.340
And you're still an active musician in the sense.

01:19:02.340 --> 01:19:04.660
I mean, we chatted about this a couple of weeks ago.

01:19:05.620 --> 01:19:08.660
Have you been active also during the pandemic?

01:19:08.660 --> 01:19:09.220
Yeah, no.

01:19:09.220 --> 01:19:16.420
And I think the funny thing now is that so I still have one very active recording project

01:19:16.420 --> 01:19:17.700
that's called The Fun Years.

01:19:18.500 --> 01:19:23.460
My bandmate and I, I don't know, we're both, you know, in our 40s and we haven't really

01:19:23.460 --> 01:19:27.780
figured out this, how to release our back catalog, but we're thinking about it.

01:19:27.780 --> 01:19:30.820
It's going to come out and we definitely have some new stuff that we want to put out

01:19:30.820 --> 01:19:31.460
soon.

01:19:31.460 --> 01:19:32.580
We just haven't figured out how.

01:19:32.580 --> 01:19:32.740
Yeah.

01:19:32.740 --> 01:19:37.860
So to the audience out there that are listening to this podcast, please hit a like on Spotify

01:19:37.860 --> 01:19:39.460
or wherever you listen to your music.

01:19:39.460 --> 01:19:40.020
That would be great.

01:19:40.740 --> 01:19:44.340
Shameless advertisement also in this respect.

01:19:44.340 --> 01:19:49.220
Actually, with your permission, I'm going to ask you to close this episode with my favorite

01:19:49.220 --> 01:19:54.020
track, which whose name, if I'm correct, is Breach on the Bowstring.

01:19:54.020 --> 01:19:54.740
Is that right?

01:19:54.740 --> 01:19:55.300
Absolutely.

01:19:55.300 --> 01:19:56.500
Yeah, absolutely.

01:19:56.500 --> 01:19:58.820
So with this, we're going to close this episode.

01:19:58.820 --> 01:20:01.380
And Ben, thanks for being with us.

01:20:01.380 --> 01:20:02.100
Thank you so much.

01:20:02.100 --> 01:20:03.220
It was fun.

01:20:57.380 --> 01:20:58.820
Thank you for listening.

01:20:58.820 --> 01:21:00.260
I hope you liked the show today.

01:21:01.060 --> 01:21:06.420
If you enjoyed the podcast, please consider giving us five stars on Apple Podcasts, follow

01:21:06.420 --> 01:21:12.260
us on Spotify, support on Patreon or PayPal, and connect with us on social media platforms.

01:21:13.460 --> 01:21:14.260
See you next time.

